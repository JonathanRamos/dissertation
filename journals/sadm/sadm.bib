@Article{hastie&stuetzle:1989,
  author = "T. Hastie and W. Stuetzle",
  year = "1989",
  title = "Principal curves",
  journal = "Journal of the American Statistical Association",
  volume = "84",
  pages = "502--516"}


@Unpublished{aldous99:_rever_markov,
  author =		 {D. Aldous and J.A. Fill},
  title =		 {Reversible {M}arkov chains and random walks on
                  graphs},
  note =		 {Manuscript available at
                  \url{"http://www.stat.berkeley.edu/~aldous/RWG/book.html"}},
  year =		 1999
}

@Article{anderson03:_gener,
  author =		 {M.-J. Anderson and J. Robinson},
  title =		 {Generalized discriminant analysis based on
                  distances},
  journal =		 {Australian \& New Zealand Journal of Statistics},
  year =		 2003,
  volume =		 45,
  pages =		 {301-318}
}

@Article{johnson75:_power_matric_posit_defin_real_part,
  author =		 {C. R. Johnson},
  title =		 {Powers of Matrics with Positive Definite Real Part},
  journal =		 {Proceedings of the American Mathematical Society},
  year =		 1975,
  volume =		 50,
  pages =		 {85-91}
}

@Article{yan07:_graph_embed_exten,
  author =		 {S. Yan and D. Xu and B. Zhang and H. Zhang and
                  Q. Yang and S. Lin},
  title =		 {Graph Embedding and Extensions: A General Framework
                  for Dimensionality Reduction},
  journal =		 {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  year =		 2007,
  volume =		 29,
  pages =		 {40-51}
}

@InProceedings{lecun98:_gradien,
  author = 		 {Y. Le{C}un and others},
  title = 		 {Gradient-based learning applied to document recognition},
  booktitle = {Proceedings of the {IEEE}},
  pages = 	 {2278-2324},
  year = 	 1998,
  volume = 	 86}

@Book{bapat97:_nonneg_matric_applic,
  author =		 {R. B. Bapat and T. E. S. Raghavan},
  title =		 {Nonnegative Matrices and Applications},
  publisher =	 {Cambridge University Press},
  year =		 1997}

@Article{bapat99:_resis_distan_in_graph,
  author =		 {R.B. Bapat},
  title =		 {Resistance Distance in Graphs},
  journal =		 {The Mathematics Student},
  year =		 1999,
  volume =		 68,
  pages =		 {87-98}
}

@Article{belkin03:_laplac,
  author =		 {M. Belkin and P. Niyogi},
  title =		 {Laplacian eigenmaps for dimensionality reduction and
                  data representation},
  journal =		 {Neural Computation},
  year =		 2003,
  volume =		 15,
  pages =		 {1373-1396},
}

@Article{belkin06:_conver_laplac_eigen,
  author =		 {M. Belkin and P. Niyogi},
  title =		 {Convergence of {L}aplacian Eigenmaps},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2006,
  volume =		 19
}

@Article{belkin06:_manif_regul,
  author =		 {M. Belkin and P. Niyogi and S. Sindhwani},
  title =		 {Manifold Regularization: A geometric framework for
                  learning from labeled and unlabeled examples},
  journal =		 {Journal of Machine Learning Research},
  year =		 2006,
  volume =		 7,
  pages =		 {2399-2434}
}

@Article{belkin08:_towar_theor_found_laplac,
  author =		 {M. Belkin and P. Niyogi},
  title =		 {Towards a Theoretical Foundation for
                  {L}aplacian-based manifold methods},
  journal =		 {Journal of Computer and System Sciences},
  year =		 2008,
  volume =		 74,
  pages =		 {1289-1308}
}


@Article{bengio04:_out_lle_isomap_mds_eigen,
  author =		 {Y. Bengio and J. Paiement and P. Vincent and
                  O. Delalleau and N. Le Roux and M. Ouimet},
  title =		 {Out-of-sample extensions for LLE, {I}somap, MDS,
                  Eigenmaps and spectral clustering},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2004,
  volume =		 16
}

@InBook{bengio06:_featur_extrac,
  author =		 {Y. Bengio and O. Delalleau and N. Le Roux and
                  J.-F. Paiement and P. Vincent and M. Ouimet},
  title =		 {Feature Extraction: Foundations and Applications},
  chapter =		 {Spectral dimensionality reduction},
  publisher =	 {Springer},
  year =		 2006
}

@TechReport{bernstein00:_graph,
  author =		 {M. Bernstein and V. de Silva and J. C. Langford and
                  J. B. Tenenbaum},
  title =		 {Graph approximations to geodesic on embedded
                  manifolds},
  institution =	 {Stanford University},
  year =		 2000,
  annote =	 { The technical report discussed the convergence of
                  the shortest path distances as used in Isomap to the
                  geodesic distance on a manifold. }
}

@Article{bishop98:_gtm,
  author =		 {C. Bishop and M. Svens\'en and C. K. I. Williams},
  title =		 {GTM: The Generative Topographic Mapping},
  journal =		 {Neural Computation},
  year =		 1998,
  volume =		 10,
  pages =		 {215-234}
}

@TechReport{boley09:_gener_laplac,
  author =		 {D. Boley},
  title =		 {Generalized {L}aplacian and first transit times for
                  directed graphs},
  institution =	 {University of Minnesota},
  year =		 2009,
  number =		 {09-009},
  annote =	 { The technical report proved that expected commute
                  time for directed graphs is also a Euclidean
                  distance measure. }
}
  


@Book{borg05:_moder,
  author =		 {I. Borg and P. J. F. Groenen},
  title =		 {Modern multidimensional scaling},
  publisher =	 {Springer},
  year =		 2005,
  edition =		 2 
}

@Article{brand05:_chart,
  author = 		 {M. Brand},
  title = 		 {Charting a manifold},
  journal = 	 {Advances in Neural Information Processing Systems},
  year = 		 2005}

@InProceedings{brand05:_from,
  author =		 {M. Brand},
  title =		 {From subspaces to submanifolds},
  booktitle =	 {Proceedings of the British Machine Vision
                  Conference},
  year =		 2005
}

@Article{bregler94:_surfac,
  author =		 {C. Bregler and S. M. Omohundro},
  title =		 {Surface learning with applications to lip-reading},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 1994,
  volume =		 6
}

@Article{bregler95:_nonlin,
  author =		 {C. Bregler and S.M. Omohundro},
  title =		 {Nonlinear image interpolation using manifold
                  learning},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 1995,
  volume =		 7
}

@InBook{burges05:_data,
  author =		 {C. J. C. Burges},
  title =		 {Data mining and knowledge discovery handbook: A
                  complete guide for researches and practitioners},
  chapter =		 {Geometric methods for feature extraction and
                  dimensional reduction},
  publisher =	 {Kluwer},
  year =		 2005
}

@Article{c.75:_role_group_gener_inver_markov,
  author =		 {C. D. Meyer, Jr.},
  title =		 {The Role of the Group Generalized Inverse in the
                  theory of finite {M}arkov chain},
  journal =		 {SIAM Review},
  year =		 1975,
  volume =		 17,
  pages =		 {443-464}
}

@Article{chaiken82,
  author = 		 {S. Chaiken},
  title = 		 {A combinatorial proof of the all minors matrix tree theorem},
  journal = 	 {SIAM Journal on Algebraic and Discrete Methods},
  year = 		 1982,
  volume = 	 3,
  pages = 	 {319-329}}

@Article{chapelle02:_sclus,
  author =		 {O. Chapelle and J. Weston and B. Sch\"{o}lkopf},
  title =		 {Scluster kernels for semi-supervised learning},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2002,
  volume =		 15
}

@Article{chapelle03:_clust_kernel_semi_super_learn,
  author =		 {O. Chapelle and J. Weston and B. Sch\"{o}lkopf},
  title =		 {Cluster Kernels for Semi-Supervised Learning},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2003,
  volume =		 15,
  pages =		 {585-592}
}

@Unpublished{chebotarev,
  author =		 {P. Chebotarev},
  title =		 {A new family of graph distances},
  year =		 {2008},
  note =		 {\url{http://arxiv.org/abs/0810.2717}}
}

@Article{chebotarev02:_fores_laplac,
  author = 		 {P. Chebotarev and R. Agaev},
  title = 		 {Forest matrices around the {L}aplacian matrix},
  journal = 	 {Linear ALgebra and its Applications},
  year = 		 2002,
  volume = 	 356,
  pages = 	 {253-274}}


@Article{chebotarev02:_fores_metric_for_graph_vertic,
  author =		 {P. Chebotarev and E. Shamis},
  title =		 {The Forest Metrics for Graph Vertices},
  journal =		 {Electronic Notes in Discrete Mathematics},
  year =		 2002,
  volume =		 11,
  pages =		 {98-107},
  annote =		 {The paper introduced the notion of forest metrics
                  for weighted graphs. An interpretation of forest
                  metrics as the $\kappa$ transform of a stochastic
                  matrix whose entries are proportional to the number
                  of spanning $2$-forests on the graph is provided. }
}

@Unpublished{chebotarev08:_new_famil_of_graph_distan,
  author =		 {P. Chebotarev},
  title =		 {A new family of graph distances},
  note =		 {arXiv:0810.2717v2},
  year =		 2008,
  annote =		 {The paper discussed a family of graph distances
                  parameterized by $\alpha$. This family approaches
                  the shortest path distances and expected commute
                  time as $\alpha \rightarrow 0$, $\alpha \rightarrow
                  \infty$, respectively. However, experimental
                  evidence seems to suggest that the convergence is
                  unstable. }
}

@Article{chebotarev97:_matrix_fores_theor_and_measur,
  author =		 {P. Chebotarev and E. Shamis},
  title =		 {The Matrix-Forest Theorem and Measuring Relations in
                  Small Social Groups},
  journal =		 {Automation and Remote Control},
  year =		 1997,
  volume =		 58,
  pages =		 {1505-1514}
}


@Article{chen07:_resis_laplac,
  author =		 {H. Chen and F. Zhang},
  title =		 {Resistance distance and the normalized Laplacian
                  spectrum},
  journal =		 {Discrete applied mathematics},
  year =		 2007,
  volume =		 155,
  pages =		 {654-661},
  annote =	 { The paper study expected commute time in terms of
                  the normalized Laplacian. Specifically, the paper
                  introduced the notion of a modified Kirchoff index
                  $K'$ based on the normalized Laplacian and showed
                  that one can bound $K'$ from below by using the
                  Kirchoff index that's based on the combinatorial
                  Laplacian.  }
}

@Article{chen09:_simil_class,
  author =		 {Y. Chen and E. K. Garcia and M. R. Gupta and
                  A. Rahimi and L. Cazzanti},
  title =		 {Similarity-based Classification: Concepts and
                  Algorithms },
  journal =		 {Journal of Machine Learning Research},
  year =		 2009,
  volume =		 10,
  pages =		 {747-776}
}

@Article{chung05:_laplac_cheeg,
  author =		 {F. Chung},
  title =		 {Laplacians and the {C}heeger inequality for directed
                  graphs},
  journal =		 {Annals of Combinatorics},
  year =		 2005,
  volume =		 9,
  pages =		 {1-19}
}

@Book{chung97:_spect_graph_theor,
  author =		 {F. Chung},
  title =		 {Spectral Graph Theory},
  publisher =	 {Conference Board of the Matehmatical Sciences},
  year =		 1997,
  volume =		 92,
  series =		 {CBMS Regional Conference Series in Mathematics}
}

@Article{coifman06:_diffus_maps,
  author =		 {R. Coifman and S. Lafon},
  title =		 {Diffusion maps},
  journal =		 {Applied and Computational Harmonic Analysis},
  year =		 2006,
  volume =		 21,
  pages =		 {5-30},
  annote =		 {The paper introduced diffusion maps and diffusion
                  distances.  A family of probability transition
                  function parameterized by $\alpha \in (0,1)$ is also
                  introduced. It is shown that as the number of
                  sampling points in creased and the time between
                  transition decreased, the probability transition
                  function define a semigroup operator $T$ whose
                  generator $\mathcal{A}$ corresponds to various
                  operators such as the Laplace-Beltrami operator and
                  the Fokker-Plank operator.  }
}

@Book{cox01:_multid,
  author =		 {T. F. Cox and M. A. A. Cox},
  title =		 {Multidimensional scaling},
  publisher =	 {Chapman and Hall},
  year =		 2001
}

@Article{critchley88:_certain_linear_mappin,
  author =		 {F. Critchley},
  title =		 {On Certain Linear Mappings between inner-product and
                  squared-distance matrices},
  journal =		 {Linear Algebra and Its Applications},
  year =		 1988,
  volume =		 105,
  pages =		 {91-107}
}
@Book{cvetkovic80:_spect_graph_theor_applic,
  author =		 {D. M. Cvetkovi\'{c} and M. Doob and H. Sachs},
  title =		 {Spectra of Graphs, Theory and Applications},
  publisher =	 {Academic Press},
  year =		 1980
}
@InProceedings{dhillon01:_co,
  author =		 {I. Dhillon},
  title =		 {Co-clustering documents and words using bipartite
                  spectral graph partitioning},
  booktitle =	 {Proceedings of the seventh ACM SIGKDD international
                  conference on Knowledge discovery and data mining},
  year =		 2001
}

@Article{donoho03:_hesian,
  author =		 {D. L. Donoho and C. Grimes},
  title =		 {Hesian eigenmaps: Locally linear embedding
                  techniques for high-dimensional data},
  journal =		 {Proceedings of the National Academy of Sciences},
  year =		 2003,
  volume =		 100,
  pages =		 {5591-5596}
}
@Book{doyle84:_random_walks_elect_networ,
  author =		 {P.G. Doyle and J.L. Snell},
  title =		 {Random Walks and Electrical Networks},
  publisher =	 {Mathematical Association of America},
  year =		 1984
}

@Article{eckart36:_approx,
  author =		 {C. Eckart and G. Young},
  title =		 {Approximation of one matrix by another of lower
                  rank},
  journal =		 {Psychometrika},
  year =		 1936,
  volume =		 1,
  pages =		 {211-218}
}

@Article{fiedler73:_algeb,
  author = 		 {M. Fiedler},
  title = 		 {Algebraic connectivity of graphs},
  journal = 	 {Czechoslovak Mathematical Journal},
  year = 		 1973,
  volume = 	 23,
  pages = 	 {298-305}}

@Book{gamelin01:_compl_analy,
  author = 	 {T. W. Gamelin},
  title = 		 {Complex Analysis},
  publisher = 	 {Springer},
  year = 		 2001}



@Article{gersgorin31:_uber_abgren_eigen_matrix,
  author = 		 {S. Ger\u{s}gorin},
  title = 		 {\"{U}ber die Abgrenzung der Eigenwerte einer Matrix},
  journal = 	 {Izv. Akad. Nauk. USSR Otd. Fiz.-Mat},
  year = 		 1931,
  volume = 	 7,
  pages = 	 {749-754}}

@InProceedings{gomes02,
  author =		 {J. Gomes and A. Mojsilovic},
  title =		 {A variational approach to recovering a manifold},
  booktitle =	 {Proceeding of the seventh european conference on
                  computer vision},
  pages =		 {3-17},
  year =		 2002
}

@Book{gorban08:_princ_manif,
  editor = 	 {A. N. Gorban and B. K\'egl and D. C. Wunsch and A. Zinovyev},
  title = 		 {Principal Manifolds for data visualization and dimension reduction},
  publisher = 	 {Springer Verlag},
  year = 		 2008,
  volume = 	 58,
  series = 	 {Lecture Notes in Computation Science and Engineering}}

@Article{gower66:_some,
  author =		 {J. C. Gower},
  title =		 {Some distance properties of latent root and vector
                  methods used in multivariate analysis},
  journal =		 {Biometrika},
  year =		 1966,
  volume =		 53,
  pages =		 {325-338}
}

@Article{hagen92:_new,
  author =		 {L. Hagen and A. Kahng},
  title =		 {New spectral methods for ratio cut partitioning and
                  clustering},
  journal =		 {IEEE Transactions on Computer Aided Design},
  year =		 1992,
  volume =		 11,
  pages =		 {1074-1085}
}

@InProceedings{ham04,
  author =		 {J. Ham and D. D. Lee and S. Mika and
                  B. Sch\"{o}lkopf},
  title =		 {A kernel view of the dimensionality reduction of
                  manifolds},
  booktitle =	 {Proceedings of the 21$^{\textrm{st}}$ International
                  Conference on Machine Learning},
  year =		 2004,
  annote =		 { The paper showed that several dimensionality
                  reduction algorithms, namely Isomap, Laplacian
                  eigenmaps, and LLE, can be viewed as kernel methods.
                  }
}

@PhdThesis{hastie84:_princ,
  author = 		 {T. Hastie},
  title = 		 {Principal curves and surfaces},
  school = 		 {Stanford University},
  year = 		 1984}

@Article{hastie89:_princ,
  author = 		 {T. Hastie and W. Stuetzle},
  title = 		 {Principal curves},
  journal = 	 {Journal of the American Statistical Association},
  year = 		 1989,
  volume = 	 84,
  pages = 	 {502-516}}

@InProceedings{hein05:_from,
  author =		 {M. Hein and J. Y. Audibert and U. von Luxburg},
  title =		 {From graphs to manifolds: weak and strong pointwise
                  consistency of graph {L}aplacians},
  booktitle =	 {Proceedinngs of the 18$^{\textrm{th}}$ conference on
                  learning theory,},
  year =		 2005,
  annote =		 { }
}

@Article{hein07:_conver_laplac,
  author =		 {M. Hein and J.-Y. Audibert and U. von Luxburg},
  title =		 {Convergence of graph Laplacians on random
                  neighbourhood graphs},
  journal =		 {Journal of Machine Learning Research},
  year =		 2007,
  volume =		 8,
  pages =		 {1325-1370}
}

@Book{horn90:_matrix_analy,
  author =		 {R.~Horn and C.~Johnson},
  title =		 {Matrix Analysis},
  publisher =	 {Cambridge University Press},
  year =		 1990
}

@Article{schoenberg35:_remar_mauric_frech_sur_hilber,
  author =		 {I.~Schoenberg},
  title =		 {Remarks to {M}aurice {F}rechet article "Sur la
                  d\'{e}finition axiomatique d'une class d'espaces
                  distanci\'{e}s vectoreillement applicable sur
                  l'espace de Hilbert"},
  journal =		 {Annals of Mathematics},
  year =		 1935,
  volume =		 38,
  pages =		 {724--738}
}

@Article{hendrickson95,
  author =		 {B. Hendrickson and R. Leland},
  title =		 {An improved spectral graph partitioning algorithm
                  for mapping parallel computations},
  journal =		 {SIAM Journal on Scientific Computing},
  year =		 1995,
  volume =		 16,
  pages =		 {452-469}
}

@Book{horn94:_topic_in_matrix_analy,
  author =		 {R.A. Horn and C.R. Johnson},
  title =		 {Topics in Matrix Analysis},
  publisher =	 {Cambridge University Press},
  year =		 1994
}

@Article{hotelling33:_analy,
  author =		 {H. Hotelling},
  title =		 {Analysis of a complex of statistical variables into
                  principle components},
  journal =		 {Journal of Educational Psychology},
  year =		 1933,
  volume =		 24,
  pages =		 {417-441}
}

@Article{j.69:_nonlin,
  author =		 {Sammon, Jr, J. W.},
  title =		 {A Nonlinear mapping for data structure analysis},
  journal =		 {IEEE Transactions on Computers},
  year =		 1969,
  volume =		 18,
  pages =		 {401-409}
}

@Book{jolliffe02:_princ_compon_analy,
  author = 	 {I. T. Jolliffe},
  title = 		 {Principal Component Analysis},
  publisher = 	 {Springer},
  year = 		 2002,
  edition = 	 {2nd}}
  


@InBook{joly94:_class_dissim_analy,
  author =		 {S. Joly and G. Le {C}alve},
  title =		 {Classification and Dissimilarity Analysis},
  chapter =		 {Similarity functions},
  publisher =	 {Springer},
  year =		 1994,
  volume =		 93,
  series =		 {Lecture Notes in Statistics},
}

@InProceedings{kannan00,
  author =		 {R. Kannan and S. Vempala and A. Vetta},
  title =		 {On clusterings -- good, bad and spectral},
  booktitle =	 {Proceedings of the 41st Annual Symposium on
                  Foundations of computer Science},
  year =		 2000
}
@Book{kemeny83:_finit_markov_chain,
  author =		 {J. G. Kemeny and J. L. Snell},
  title =		 {Finite Markov Chains},
  publisher =	 {Springer},
  year =		 1983,
}

@Article{kim09:_semi_regres_hessian,
  author =		 {K. I. Kim and F. Steinke and M. Hein},
  title =		 {Semi-supervised Regression using {H}essian energy
                  with an application to semi-supervised
                  dimensionality reduction},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2009,
  volume =		 22
}

@Article{kirchhoff47:_uber_aufl_gleic_str,
  author =		 {G. Kirchhoff},
  title =		 {\"{U}ber die Aufl\"{o}sung der Gleichunge, auf
                  welche man bei der untersuchung der linearen
                  verteilung galvanischer Str\"{o}mer gef\"{u}hrt
                  wird},
  journal =		 {Ann. Phys. Chem},
  year =		 1847,
  volume =	 72,
  pages =	 {497-508}}

@Article{kirkland97:_distan_weigh_trees_group_inver_laplac_matric,
  author =		 {S. J. Kirkland and M. Neumann and B. L. Shader},
  title =		 {Distances in Weighted Trees and Group Inverse of
                  Laplacian Matrices},
  journal =		 {{SIAM} Journal on Matrix Analysis and Applications},
  year =		 1997,
  volume =		 18,
  pages =		 {827-841}
}

@Article{klein93:_resis_distan,
  author =		 {D.J. Klein and M. Randic},
  title =		 {Resistance distance},
  journal =		 {Journal of Mathematical Chemistry},
  year =		 1993,
  volume =		 12,
  pages =		 {81-95}
}

@InProceedings{kondor02:_diffus,
  author =		 {R. I. Kondor and J. Lafferty},
  title =		 {Diffusion kernels on graphs and other discrete input
                  spaces},
  booktitle =	 {Proceedings of the nineteenth International
                  Conference on Machine Learning},
  year =		 2002,
  annote =	 { The paper proposed a method of constructing natural
                  families of kernels over discrete structures. The
                  kernels are constructed by using the matrix exponent
                  of some matrix. The paper focus on a particular kind
                  of kernels for graphs, those that arise by taking
                  the matrix exponential of the combiatorial Laplacian
                  of a graph.}
}

@Article{lafon06:_data_fusion_multic_data_match_diffus_maps,
  author =		 {S. Lafon and Y. Keller and R. R. Coifman},
  title =		 {Data Fusion and Multicue Data Matching by Diffusion
                  Maps},
  journal =		 {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  year =		 2006,
  volume =		 28,
  pages =		 {1784-1797}
}

@Article{lafon06:_diffus,
  author =		 {S. Lafon and A. B. Lee},
  title =		 {Diffusion maps and coarse-graining: {A} unified
                  framework for dimensionality reduction, graph
                  partitioning and data set parameterization},
  journal =		 {{IEEE} transactions on pattern analysis and machine
                  intelligence},
  year =		 2006,
  volume =		 28,
  pages =		 {1393-1203},
  annote =	 { The paper attempt to illustrate the notion that
                  nonlinear dimensionality reduction, clustering, and
                  data set parameterization can be solved within the
                  same framework. What the paper meant by data set
                  parameterization and nonlinear dimensionality
                  reduction is the notion of nonlinear dimensionality
                  reduction being performed so that the pairwise
                  Euclidean distances of the embedding is an
                  approximation of a predefined notion of distances,
                  namely diffusion distances. The paper also
                  introduced a clustering algorithm on graphs similar
                  to $k$-means, with the centroid of each clusters
                  corresponding to a "diffusion center" on the
                  graph. }
}


@Book{levin09:_markov,
  author = 	 {D. A. Levin and Y. Peres and E. L. Wilmer},
  title = 		 {Markov chains and mixing times},
  publisher = 	 {American Mathematical Society},
  year = 		 2009}

@Book{levin09:_markov_chain_mixin_times,
  author =		 {D.A. Levin and Y. Peres and E.L. Wilmer},
  title =		 {Markov Chains and Mixing Times},
  publisher =	 {American Mathematical Society},
  year =		 2009
}

@Article{lovasz96:_random_graph,
  author =		 {L. Lov\'{a}sz},
  title =		 {Random walks on Graphs: A Survey},
  journal =		 {Bolyai Society Mathematical Studies},
  year =		 1996,
  volume =		 2,
  pages =		 {353-397},
  annote =		 {The paper discussed random walks on unweighted,
                  undirected graphs. Various inequalities for expected
                  commute time are given. Connections between random
                  walks and sampling are also illustrated. The paper
                  gave an expression for the expected commute time in
                  terms of the eigenvalues and eigenvectors of the
                  normalized Laplacian matrix. }
}


@TechReport{luxburg07:_tutor_spect_clust,
  author =		 {U. Von Luxburg},
  title =		 {A Tutorial on Spectral Clustering},
  institution =	 {Max Planck Institute for Biological Cybernetics},
  year =		 2007,
  annote =	 { The technical report presented several variants of
                  spectral clustering algorithms from the view point
                  of different type of graph Laplacians. The technical
                  report showed the various motivation behind the
                  spectral clustering algorithms, such as graph cuts,
                  random walks, and pertubation of matrices. }
}

@Article{luxburg08:_consis,
  author =		 {U. Von Luxburg and M. Belkin and O. Bousquet},
  title =		 {Consistency of spectral clustering},
  journal =		 {Annals of Statistics},
  year =		 2008,
  volume =		 36,
  pages =		 {555-586},
  annote =	 { The paper discussed the consistency of spectral
                  clustering. Two forms of spectral clustering are
                  considered: (1) Those that employ the combinatorial
                  Laplacian $L$, (2) Those that employ the normalized
                  Laplacian $\mathcal{L}$. The paper show that for
                  both forms of spectral clustering, under certain
                  criteria, the eigenvectors converges to the
                  eigenfunctions of the corresponding operators. The
                  criteria for the convergence in the case of the
                  normalized Laplacian is less restrictive than the
                  criteria in the case of the combinatorial Laplacian.
                  }
}


@Unpublished{lyons:_probab_trees_networ,
  author = 		 {R. Lyons and Y. Peres},
  title = 		 {Probability on Trees and Networks},
  year =         2009, 
  note = 		 {\url{http://mypage.iu.edu/~rdlyons/prbtree/prbtree.html}}}

@InProceedings{m.01:_random_walks_view_spect_segmen,
  author =		 {M. Melia and J. Shi},
  title =		 {A Random Walks View of Spectral Segmentation},
  booktitle =	 {8th International Workshop on Artificial
                  Intelligence and Statistics},
  year =		 2001,
  annote =		 { The paper discussed spectral clustering using the
                  probability transition matrix $P$. The paper show
                  that the resulting clusters satisfied a relaxed
                  version of the normalized cut criterion.  }
}

@Article{maier08:_influen,
  author =		 {M. Maier and U. von {L}uxburg and M. Hein},
  title =		 {Influence of graph construction on graph-based
                  clustering measures},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2008,
  volume =		 21
}

@Article{maier09:_optim_k,
  author =		 {M. Maier and M. Hein and U. von {L}uxburg},
  title =		 {Optimal construction of $K$-nearest neighbour graphs
                  for identifying noisy clusters},
  journal =		 {Theoretical computer Science},
  year =		 2009,
  volume =		 410,
  pages =		 {1749-1764}
}

@InProceedings{shi97:_normal,
  author =		 {J. Shi and J. Malik},
  title =		 {Normalized cuts and image segmentation},
  booktitle =	 {Proceedings of the {IEEE} conference on computer
                  vision and pattern recognition},
  pages =		 {731-737},
  year =		 1997
}

@Article{merris94:_laplac,
  author =		 {R. Merris},
  title =		 {Laplacian matrices of graphs: A survey},
  journal =		 {Linear Algebra and Its Applications},
  year =		 1994,
  volume =		 197,
  pages =		 {143-176}
}





@InBook{mohar91:_graph,
  author =		 {B. Mohar},
  title =		 {Graph theory, combinatorics, and
                  applications. VOl. 2},
  chapter =		 {The {L}aplacian spectrum of graphs},
  publisher =	 {Addison Wiley},
  year =		 1991
}



@Article{moon94:_some_deter_expan_and_matrix_tree_theor,
  author =		 {J.W. Moon},
  title =		 {Some determinant expansions and the matrix-tree
                  theorem},
  journal =		 {Discrete Mathematics},
  year =		 1994,
  volume =		 124,
  pages =		 {163-171}
}

@Article{mosbah99:_non_unifor_random_spann_trees_weigh_graph,
  author =		 {M. Mosbah and N. Saheb},
  title =		 {Non-uniform random spanning trees on weighted
                  graphs},
  journal =		 {Theoretical Computer Science},
  year =		 1999,
  volume =		 218,
  pages =		 {263-271},
  annote =		 {The paper show that a random spanning tree, with
                  probabilities proportional to its weights, on
                  weighted graphs can be constructed by using the
                  edges corresponding to the first hitting time as
                  given by a random walk on the graph itself. }
}

@Article{nadler05:_diffus_eigen_fokker_planc_operat,
  author =		 {B. Nadler and S. Lafon and R. R. Coifman and
                  I. G. Kevrekidis},
  title =		 {Diffusion maps, spectral clustering and
                  Eigenfunctions of {F}okker-{P}lanck Operators},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2005,
  volume =		 18
}

@Article{nadler06:_diffus,
  author =		 {B. Nadler and S. Lafon and R. R. Coifman and
                  I. G. Kevrekidis},
  title =		 {Diffusion maps, spectral clustering and reaction
                  coordinates of dynamical systems},
  journal =		 {Applied and Computational Harmonic Analysis},
  year =		 2006,
  volume =		 21,
  pages =		 {113-127}
}

@Article{ng02,
  author =		 {A. Ng and M. Jordan and Y. Weiss},
  title =		 {On spectral clustering: analysis and an algorithm},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2002,
  volume =		 14
}

@Article{oja02:_unsup,
  author = 		 {E. Oja},
  title = 		 {Unsupervised learning in neural computation},
  journal = 	 {Theoretical Computer Science},
  year = 		 2002,
  volume = 	 287,
  pages = 	 {187-207}}

@Article{oja92:_princ,
  author =		 {E. Oja},
  title =		 {Principal components, minor components, and linear
                  neural networks},
  journal =		 {Neural Networks},
  year =		 1992,
  volume =		 5,
  pages =		 {927-935}
}

@InProceedings{park07:_hippoc,
  author =		 {Y. Park and C. E. Priebe and K. Botteron and
                  M. Miller and N. R. Mohan},
  title =		 {Hippocampus shape-space analysis of clinically
                  depressed, high-risk, and control populations},
  booktitle =	 {Proceedings of the 2007 Fronties in the convergence
                  of bioscience and information technologies},
  pages =		 {465-469},
  year =		 2007
}

@Article{pearson01:_on,
  author =		 {K. Pearson},
  title =		 {On lines and planes of closest fit to a system of
                  points in space},
  journal =		 {Philosophical Magazine},
  year =		 1901,
  volume =		 2,
  pages =		 {557-572}
}

@InProceedings{platt05:_fastm_metric_mds_nyst,
  author =		 {J. C. Platt},
  title =		 {FastMap, {M}etric{M}ap, and landmark {MDS} are all
                  {N}yst\"{o}m algorithms},
  booktitle =	 {Proceedings of the International Workshop on
                  Artificial Intelligence and Statistics},
  pages =		 {261-268},
  year =		 2005
}

@Article{propp98:_how_markov,
  author =		 {J. Propp and D. Wilson},
  title =		 {How to get a perfectly random sample from a generic
                  {M}arkov chain and generate a random spanning tree
                  of a directed graph},
  journal =		 {Journal of Algorithms},
  year =		 1998,
  volume =		 27,
  pages =		 {170-217}
}



@Article{qui07:_clust,
  author =		 {H. Qui and E.R. Hancock},
  title =		 {Clustering and embedding using commute times},
  journal =		 {IEEE transactions in Pattern Analysis and Machine
                  Intelligence},
  year =		 2007,
  volume =		 29,
  pages =		 {1873-1890},
  annote =	 { The paper exploits the properties of expected
                  commute time for the purposes of embedding and
                  clustering. The authors noted the connections
                  between expected commute time embedding and
                  Laplacian eigenmaps and diffusion maps. The paper
                  also argued that there are scenarios where the
                  clustering using expected commute time is superior
                  to normalized cuts. }
}

@Unpublished{radl09,
  author = 		 {A. Radl and U. von Luxburg and M. Hein},
  title = 		 {The resistance distance is meaningless on large random graphs},
  note = 		 {Presented at the NIPS Workshop on analyzing network and learning with graphs},
  year =         2009
  }

@Article{roweis00:_nonlin,
  author =		 {S.T. Roweis and L.K. Saul},
  title =		 {Nonlinear dimensionality reduction by locally linear
                  embedding},
  journal =		 {Science},
  year =		 2000,
  volume =		 290,
  pages =		 {2323-2326},
}



@Article{rubner89,
  author =		 {J. Rubner and P. Tavan},
  title =		 {A self-organizing network for principal component
                  analysis},
  journal =		 {Europhysics Letters},
  year =		 1989,
  volume =		 10,
  pages =		 {693-698}
}

@Article{saerens04,
  author =		 {M. Saerens and F. Fouss and L. Yen and P. Dupont},
  title =		 {The principal components analysis of a graph and its
                  relationships to spectral clustering},
  journal =		 {Proceedings of the fifteenth {E}uropean conference
                  on machine learning},
  year =		 2004
}



@InBook{saul06:_semis,
  author =		 {L. K. Saul and K. Q. Weinberger and J. H. Ham and
                  F. Sha and D. D. Lee},
  title =		 {Semisupervised learning},
  chapter =		 {Spectral methods for dimensionality reduction},
  publisher =	 {MIT Press},
  year =		 2006
}



@Article{schoenberg38:_metric,
  author =		 {I. J. Schoenberg},
  title =		 {Metric spaces and positive definite functions},
  journal =		 {Transaction of the American Mathematical Society},
  year =		 1938,
  volume =		 44,
  pages =		 {522-553},
}

@InBook{scholkopf97:_lectur_notes_comput_scien,
  author =		 {B. Sch\"{o}lkopf and A. J. Smola and K. R. Mueller},
  title =		 {Lecture Notes in Computer Science},
  chapter =		 {Kernel Principal Component Analysis},
  publisher =	 {Springer Verlag},
  year =		 1997,
  volume =		 1327,
  pages =		 {583-588}
}

@InProceedings{scott90:_featur,
  author =		 {G. Scott and H. Longuet-Higgins},
  title =		 {Feature grouping by relocalisation of eigenvectors
                  of the proximity matrix},
  booktitle =	 {Proceedings of the British Machine Vision
                  Conference},
  year =		 1990
}

@Article{shi00:_normal,
  author =		 {J. Shi and J. Malik},
  title =		 {Normalized cuts and image segmentation},
  journal =		 {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  year =		 2000,
  volume =		 22,
  pages =		 {888-905}
}

@Article{silva02:_global,
  author =		 {V. de Silva and J. B. Tenenbaum},
  title =		 {Global versus local methods in nonlinear
                  dimensionality reduction},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2002,
  volume =		 15
}

@Article{smola01:_regul_princ_manif,
  author =		 {A. J. Smola and S. Mika and B. Sch\"{o}lkopf and
                  R. C. Williamson},
  title =		 {Regularized Principal Manifolds},
  journal =		 {Journal of Machine Learning Research},
  year =		 2001,
  volume =		 1,
  pages =		 {179-209}
}

@InProceedings{smola03:_kernel,
  author =		 {A. Smola and R. Kondor},
  title =		 {Kernels and regularization on graphs},
  booktitle =	 {Conference on Learning Theory},
  year =		 2003
}

@InProceedings{spielmand08:_graph,
  author =		 {D. A. Spielman and N. Srivastava},
  title =		 {Graph sparsification by effective resistances},
  booktitle =	 {Proceedings of the annual {ACM} symposium on Theory
                  of Computing},
  year =		 2008
}

@Article{szummer01:_partial_markov,
  author = 		 {M. Szummer and T. Jaakkola},
  title = 		 {Partially labeled classification with Markov random walks},
  journal = 	 {Advances in Neural Information Processing System},
  year = 		 2001}

@Article{tenebaum00:_global_geomet_framew_nonlin_dimen_reduc,
  author =		 {J. B. Tenenbaum and others},
  title =		 {A Global Geometric Framework for Nonlinear
                  Dimensionality Reduction},
  journal =		 {Science},
  year =		 2000,
  volume =		 290,
  pages =		 {2319-2323},
  month =		 {December},
}

@Misc{donoho00:_high,
  author =		 {D. Donoho},
  title =		 {High-dimensional data analysis: {T}he curses and
                  blessings of dimensionality},
  howpublished = {Lecture at AMS conference on Math Challenges of the
                  21$^{\textrm{st}}$ Century },
  year =		 2000
}
@Article{tenenbaum98:_mappin,
  author =		 {J. B. Tenenbaum},
  title =		 {Mapping a manifold of perceptual observations},
  journal =		 {Advances in Neural Infomration Processing Systems},
  year =		 1998,
  volume =		 10
}

@Article{tipping99:_mixtur,
  author =		 {M. E. Tipping and C. M. Bishop},
  title =		 {Mixtures of probabilistic principal components
                  analysis},
  journal =		 {Neural computation},
  year =		 1999,
  pages =		 {443-482}
}

@Article{torgesen52:_multid,
  author =		 {W. S. Torgesen},
  title =		 {Multidimensional scaling: {I}. {T}heory and method},
  journal =		 {Psychometrika},
  year =		 1952,
  volume =		 17,
  pages =		 {401-419},
}

@Article{trosset08,
  author =		 {M. W. Trosset and C. E. Priebe},
  title =		 {The out-of-sample problem for classical
                  multidimensional scaling},
  journal =		 {Computational Statistics and Data Analysis},
  year =		 2008,
  volume =		 52,
  pages =		 {4635-4642}
}

@Article{venna10:_infor,
  author =		 {J. Venna and J. Peltonen and K. Nybo and H. Aidos
                  and S. Kaski},
  title =		 {Information retrieval perspective to nonlinear
                  dimensionality reduction for data visualization},
  journal =		 {Journal of Machine Learning Research},
  year =		 2010,
  volume =		 11,
  pages =		 {451-490}
}

@InProceedings{weiss9:_segmen,
  author =		 {Y. Weiss},
  title =		 {Segmentation using eigenvectors: A unifiying view},
  booktitle = {Proceedings of the International Conference on
                  Computer Vision},
  year =		 {1999 }}


@Article{williams01:_using_nystr,
  author =		 {C. K. I. Williams and M. Seeger},
  title =		 {Using the {N}ystr\"{o}m method to speed up kernel
                  machines},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2001
}

@Article{wouters03:_graph,
  author =		 {L. Wouters and H. W. G\"{o}hlmann and L. Bijnens and
                  S. U. Kass and G. Molenberghs and P. J. Lewi},
  title =		 {Graphical exploration of gene expression data: a
                  comparative study of three multivariate methods},
  journal =		 {Biometrics},
  year =		 2003,
  volume =		 59,
  pages =		 {1131-1139}
}



@InProceedings{yen07:_graph,
  author =		 {L. Yen and F. Fouss and C. Decaestecker and
                  P. Francq and M. Saerens},
  title =		 {Graph nodes clustering based on the commute-time
                  kernel},
  booktitle =	 {Proceedings of the {P}acific-{A}sia conference on
                  knowledge discovery and data mining},
  year =		 2007
}


@Article{yen08:_famil_of_dissim_measur_between,
  author =		 {L. Yen and M. Saerens and A. Mantrach and M. Shimbo},
  title =		 {A family of dissimilarity measures between nodes
                  generalizing both the shortest-path and the
                  commute-time distances},
  journal =	 {Proceeding of the 14th ACM SIGKDD international
                  conference on Knowledge discover},
  pages =		 {785-793},
  year =		 2008,
  annote =	 { The paper presented a family of dissimilarity
                  measures parameterized by $\theta$. The family
                  approaches expected commute time for $\theta
                  \rightarrow 0$ and shortest path distances for
                  $\theta \rightarrow \infty$. The dissimilarity
                  measures for $0 < \theta < \infty$ is constructed so
                  as to minimize a criterion based on
                  KL-divergences. },
}


@Article{zhang03:_intel_data_engin_autom_learn,
  author =		 {Z. Zhang and H. Zha},
  title =		 {Nonlinear Dimension Reduction via Local Tangent
                  Space Alignment},
  year =		 {2003},
  volume =		 {2690},
  journal =		 {Lecture Notes in Computer Science},
  pages =		 {477-481},
  annote =		 { The book chapter presented an algorithm for
                  manifold learning that attempts to represent the
                  local geometry of the manifold using tangent
                  spaces. The tangent spaces are learned by fitting an
                  affine subspace to the neighborhood of every data
                  point. Each of the tangent spaces are then aligned
                  so that data points that lie in the overlap of
                  multiple neighborhoods have consistent coordinates.
                  }
}


@Article{zhou04:_learn,
  author = 		 {D. Zhou and  T. N. Lal and J. Weston and B. Sch\"{o}lkopf},
  title = 		 {Learning with local and global consistency},
  journal = 	 {Advances in Neural Information Processing System},
  year = 		 2004}


@Article{zhou04:_learn_label_unlab,
  author =		 {D. Zhou and B. Sch\"{o}lkopf},
  title =		 {Learning from Labeled and Unlabeled data using
                  random walks},
  journal =		 {Proceedings of the twenty sixth DAGM Sympsium},
  year =		 2004,
  pages =		 {237-244},
  annote =		 {The paper analyzed an algorithm that was previously
                  introduced by the authors. The algorithm is a
                  semi-supervised learning algorithm that attempts to
                  classify unlabeled points by assigning it the sign
                  of $(I - \alpha \mathcal{L})^{-1}y$ where
                  $\mathcal{L}$ is the normalized Laplacian, $\alpha
                  \in (0,1)$ and $y$ is the vector of labels, with
                  $y_i = 0$ if $x_i$ has no labels. The resulting
                  algorithm is related to the concept of normalized
                  commute time.  }
}



@Article{zhu03:_semi_super_learn_using_gauss,
  author =		 {X. Zhu and Z. Ghahramani and J. Lafferty},
  title =		 {Semi-Supervised Learning Using {G}aussian Fields and
                  Harmonic Functions},
  journal =		 {International Conference on Machine Learning},
  year =		 2003,
  annote =		 { The paper proposed an approach to semi-supervised
                  learning that's based on a Gaussian random field
                  model. The labeled and unlabeled data are
                  represented as a weighted graph. Some
                  interpretations and connections of the learning
                  algorithms with harmonic functions on graphs, graphs
                  kernels, and spectral clustering is noted. }
}

@InBook{zhu05:_semi,
  author =		 {X. Zhu and J. Kandola and J. Lafferty and
                  Z. Ghahramani},
  title =		 {Semi-supervised learning},
  chapter =		 {Graph Kernels by Spectral Transforms},
  publisher =	 {MIT Press},
  year =		 2005
}


@TechReport{zhu05:_semi_super,
  author =		 {X. Zhu},
  title =		 {Semi-Supervised learning literature survey},
  institution =	 {University of Wisconsin-Madison},
  year =		 2005,
  number =		 1530
}

@Article{berge93:_comput_indsc,
  author =		 {J. M. F. {Ten Berge} and H. A. L. Kiers and
                  W. P. Krijnen},
  title =		 {Computational solutions for the problem of negative
                  saliences and nonsymmetry in INDSCAL},
  journal =		 {Journal of Classification},
  year =		 1993,
  volume =	 10,
  pages =	 {115--124}}




@Article{coope09:_trace,
  author =		 {I. D. Coope},
  title =		 {Trace inequalities with applications to orthogonal
                  regression and matrix nearness problems},
  journal =		 {Journal of Inequlities in Pure and Applied
                  Mathematics},
  year =		 2009,
  volume =		 10,
  pages =		 {1--7}
}

@Article{kiers90:_major,
  author =		 {H. A. L. Kiers},
  title =		 {Majorization as a tool for optimizing a class of
                  matrix functions.},
  journal =		 {Psychometrika},
  year =		 1990,
  volume =		 55,
  pages =		 {417-428}
}


@InBook{harshman78:_model_n,
  author =		 {R. A. Harshman},
  title =		 {Proceedings of the first joint meeting of the
                  psychometric society and the society of mathematical
                  psychology},
  chapter =		 {Models for analysis of asymmetrical relationships
                  among N objects},
  publisher =	 {Verlag},
  year =		 1978
}

@Article{husson06:_indsc,
  author =		 {F. Husson and J. Pag\`{e}s},
  title =		 {{INDSCAL} model: geometrical interpretation and
                  methodology},
  journal =		 {Computational Statistics and Data Analysis},
  year =		 2006,
  volume =		 50,
  pages =		 {358-378}
}

@Article{kiers92:_minim,
  author =		 {H. A. L. Kiers and J. M. F. Ten Berge},
  title =		 {Minimization of a class of matrix trace functions by
                  means of refined majorization},
  journal =		 {Psychometrika},
  year =		 1992,
  volume =		 57,
  pages =		 {371-382}
}

@Article{chino78:_n,
  author =		 {N. Chino},
  title =		 {A graphical technique for representing the
                  asymmetric relationships between $N$ objects},
  journal =		 {Behaviormetrika},
  year =		 1978,
  volume =		 5,
  pages =		 {23--40}
}

@Article{okada87:_geomet,
  author =		 {A. Okada and T. Imaizumi},
  title =		 {Geometric models for asymmetric data},
  journal =		 {Behaviormetrika},
  year =		 1987,
  volume =		 21,
  pages =		 {81--96}
}

@InBook{leeuw82:_handb,
  author =		 {J. {de Leeuw} and J. W. Heiser},
  title =		 {Handbook of statistics},
  chapter =		 {Theory of multidimensional scaling},
  publisher =	 {North Holland Publishing Company},
  year =		 1982,
  volume =		 2,
  pages =		 {283--316}
}

@InProceedings{bove91:_metric,
  author =		 {G. Bove and F. Critchley},
  title =		 {Metric multidimensional scaling for asymmetric
                  proximities when the asymmetry is one-dimensional},
  booktitle =	 {Psychometric methodology: Proceedings of the 7th
                  European Meeting of the Psychometric Society},
  year =		 1991
}

@Article{zielmand93:_analy,
  author =		 {B. Zielman and J. W. Heiser},
  title =		 {Analysis of asymmetry by slide-vector},
  journal =		 {Psychometrika},
  year =		 1993,
  volume =		 58,
  pages =		 {101--114}
}

@Article{chino90,
  author =		 {N. Chino},
  title =		 {A generalized inner product model for the analysis
                  of asymmetry},
  journal =		 {Behaviormetrika},
  year =		 1990,
  volume =		 27,
  pages =		 {25--46}
}

@Article{carroll70:_analy_n_eckar_young,
  author =		 {J. D. Carroll and J.-J. Chang},
  title =		 {Analysis of individual differences in
                  multidimensional scaling via an {N}-way
                  generalization of "{E}ckart-{Y}oung" decomposition},
  journal =		 {Psychometrika},
  year =		 1970,
  volume =		 35,
  pages =		 {283-319}
}

@Article{takane77:_nonmet,
  author =		 {Y. Takane and F. W. Young and J. {de Leeuw}},
  title =		 {Nonmetric individual differences multidimensional
                  scaling: An alternative least squares method with
                  optimal scaling features},
  journal =		 {Psychometrika},
  year =		 1977,
  volume =		 42,
  pages =		 {7-67}
}



@Article{desarbo84,
  author =		 {W. S. DeSarbo and G. De Soete},
  title =		 {On the use of hierarchical clustering for the
                  analysis of nonsymmmetric proximities},
  journal =		 {Journal of Consumer Research},
  year =		 1984,
  volume =		 11,
  pages =		 {601-610}
}

@Article{rothkopf57,
  author =		 {E. Z. Rothkopf},
  title =		 {A measure of stimulus similarity and errors in some
                  paired associate learning},
  journal =		 {Journal of Experimental Psychology},
  year =		 1957,
  volume =		 53,
  pages =		 {94-101}
}

@Book{saito05:_data_analy_asymm_struc,
  author =		 {T. Saito and H. Yadohisa},
  title =		 {Data Analysis of Asymmetric Structures: Advanced
                  Approaches in Computational Statistics},
  publisher =	 {Marcel Dekker},
  year =		 2005
}


@Article{kruskal64:_nonmet,
  author =		 {J. B. Kruskal},
  title =		 {Nonmetric multidimensional scaling: A numerical
                  method},
  journal =		 {Psychometrika},
  year =		 1964,
  volume =		 29,
  pages =		 {115-129}
}

@PhdThesis{tang10:_graph,
  author =		 {M. Tang},
  title =		 {Graph metrics and dimensionality reduction},
  school =		 {Indiana University, Bloomington},
  year =		 2010
}

@InBook{leeuw80:_multiv,
  author =		 {J. {de Leeuw} and J. W. Heiser},
  title =		 {Multivariate analysis},
  chapter =		 {Multidimensional scaling with restrictions on the
                  configuration},
  publisher =	 {North Holland Publishing Company},
  year =		 1980
}

@Article{leeuw09:_multid_scalin_using_major,
  title =		 {Multidimensional Scaling Using Majorization:
                  {SMACOF} in {R}},
  author =		 {J. {de Leeuw} and P. Mair},
  journal =		 {Journal of Statistical Software},
  year =		 2009,
  volume =		 31,
  pages =		 {1--30}
}


@Article{berge91:_some_candec_indsc,
  author = 		 {J. M. F. {Ten Berge} and H. A. L. Kiers},
  title = 		 {Some clarifications of the {CANDECOMP} algorithm applied to {INDSCAL} },
  journal = 	 {Pschometrika},
  year = 		 1991,
  volume = 	 56,
  pages = 	 {317--326}}

@Article{schonemann72,
  author = 		 {P. H. Sch\"{o}nemann },
  title = 		 {An algebraic solution for a class of subjective metrices models},
  journal = 	 {Psychometrika},
  year = 		 1972,
  volume = 	 37,
  pages = 	 {441-451}}

@InBook{carroll74:_contem,
  author =		 {J. D. Carroll and M. Wish},
  title =		 {Contemporary developments in mathematical
                  psychology},
  chapter =		 {Models and methods for three-way multidimensional
                  scaling},
  publisher =	 {W. H. Freeman},
  year =		 1974
}

@InBook{gower77:_recen,
  author =		 {J. C. Gower},
  title =		 {Recent developments in statistics.},
  chapter =		 {The analysis of asymmetry and orthogonality},
  publisher =	 {NOrth Holland Publishing Company},
  year =		 1977
}


@Article{schoenberg38:_metric_spaces_compl_monot_funct,
  author =		 {I. J. Schoenberg},
  title =		 {Metric Spaces and Completely Monotone Functions},
  journal =		 {Annals of Mathematics},
  year =		 1938,
  volume =		 39,
  pages =		 {811--841}
}



