 \documentclass[professionalfonts,hyperref={colorlinks=true,linkcolor=red}]{beamer}
%\documentclass[handout]{beamer}
\mode<presentation>
{
  \usetheme{Boadilla}
  \useinnertheme{rectangles}
}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{subfigure}
\usepackage{bm}
\usepackage[all]{xy}
% \usepackage{pgfpages}
%\pgfpagesuselayout{4 on 1}[letterpaper,landscape,border shrink=5mm]

\newtheorem{question}[theorem]{Question}
\newtheorem{openquestion}[theorem]{Open Question}
\setbeamercolor{question title}{bg=red}
\setbeamercolor{block body question}{bg=blue!60}

\title[Graph Metrics and Dimensionality Reduction]{Graph Metrics and
  Dimensionality Reduction}

\author[Minh Tang]{ Minh Tang}

\institute[Indiana University]{
  Department of Computer Science \\
  Indiana University, Bloomington
}

\AtBeginSection[]
{
  \begin{frame}<beamer>
    \frametitle{Outline}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}

\bibliographystyle{plain}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

% \begin{frame}
%   \frametitle{Outline}
%   \tableofcontents[pausesections]
% \end{frame}

%\section{Introduction}

\begin{frame}
  \frametitle{Curse of dimensionality and dimensionality reduction}
  \begin{itemize}[<+->]
  \item Data in some domains now contains a large number of variables,
    e.g., microarray data, image data. It's often computationally
    intractable to search through these large, high dimensional data sets to
    look for patterns.
  \item Some traditional techniques for statistics and machine
    learning works best when there are a small number of variables
    compared to the number of instances. 
  \item It's usually easier to interpret a simple model. A complicated
    model might over-fit and is harder to interpret.
  \item All of the above points suggest that reducing the
    dimensions of the data is useful for eliminating noise, finding
    patterns, and creating models. 
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Dimensionality reduction}
  \begin{itemize}[<+->]
  \item There are varying degrees of dimensionality reduction that can
    be performed. For example, one could 
    \begin{itemize}
    \item Reduce the dimension without loss of information. 
    \item Reduce the complexity and noise of the data with at most a
      slight loss of information
    \item Significantly reduce the complexity of the data with large
      loss of information (or noise). 
    \end{itemize}
  \item The degree to which dimensionality reduction should/can be
    performed depend on applications, e.g., visualization, regression,
    classification.  
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Manifold learning algorithms}
  \begin{itemize}[<+->]
  \item Manifold learning algorithms assume that the data lies on or
    near a smooth manifold $\mathcal{M}$ with lower intrinsic
    dimension. 
  \item Example: a sphere in three dimension, images of a human face.  
  \item Neighborhood of a point is homeomorphic to Euclidean space
    $\mathbb{R}^{k}$.  
  \item Geodesic distances on a manifold can be approximated by
    line segments in Euclidean space. 
  \item Examples of manifold learning algorithms are Isomap, LLE,
    Laplacian eigenmaps, diffusion maps.
  \item Reducing the dimension of the data past the intrinsic
    dimension might be more helpful in discerning patterns. 
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Isomap}
  \onslide<1->{
    Isomap is one of the best known manifold learning
    algorithm. Tenenbaum et
    al. \cite{tenebaum00:_global_geomet_framew_nonlin_dimen_reduc}
    introduced Isomap using the following recipe: }
  \begin{enumerate}
  \item<2-> Represent the data as a graph. 
    \begin{itemize}
    \item<3-> Vertices correspond to feature vectors
    \item<3-> Edges convey information about relationship between
      feature vectors. In the case of Isomap, two vertices are
      adjacent if the Euclidean distance between them in the feature
      space are small. 
    \item<3-> Edges weights represent pairwise proximities in the
      ambient feature space, either similarities or
      dissimilarities. In the case of Isomap, the proximities are the
      Euclidean distances between the points, provided that the
      distances are small enough.
    \end{itemize}
  \item<4-> Compute new dissimilarities between pairs of
    vertices. Isomap computes the shortest path distances between the
    vertices of the graph. 
  \item<5-> Embed the dissimilarity matrix $\Delta$ into Euclidean
    space using classical multidimensional scaling. 
  \end{enumerate}
  % \onslide<6->{
  %   We will see later on that several other manifold learning algorithms
  %   and embedding techniques can also be discussed using the above
  %   recipe, with the main difference between them being the computation
  %   of the new dissimilarities in Step 2. 
  % }
\end{frame}
\begin{frame}[label=isomap_example]
  \frametitle{Isomap example}
  \subfiglabelskip=0pt
  \begin{figure}[htbp]
    \label{fig:swissroll}
    \centering
    \subfigure[][]{
      \includegraphics[width=50mm]{graphics/swissroll.pdf}
    }
    \hspace{3pt}
    \subfigure[][]{
      \includegraphics[width=50mm]{graphics/isomap_swissroll.pdf}
    }
    \caption{Isomap embedding of a swiss roll}
  \end{figure}
\end{frame}
\section{Mathematical preliminaries}
\begin{frame}[label=graph_terminology]
  \frametitle{Some graph terminologies}
  Let $G = (V,E,\omega)$ be an undirected graph, where
  $\omega$ is a similarity measure.
  \begin{itemize}
  \item $u \sim v$ if $\{u,v\} \in E$ 
  \item The degree of $v \in V$ is $\deg(v) = \sum_{u \sim
      v}{\omega(u,v)}$.
  \item The volume of $G$ is $\mathrm{Vol}(G) = \sum_{v \in
      V}{\deg(v)}$.
  \item The diagonal matrix $\bm{D}$ with entries $\bm{D}(v,v) =
    \deg(v)$ is the degree matrix. 
  \end{itemize}
  % If $G$ is directed, then there is some slight changes in the
  % terminologies. 
  % \begin{itemize}
  % \item Let $e = (u,v) \in E$. Then the two endpoints $u$ and $v$ of
  %   $e$ are denoted by $e^{-} = u$ and $e^{+} = v$.
  % \item The in-degree of $v \in V$ is $\deg^{\mathrm{in}}(v) =
  %   \sum_{e \colon e^{+} = v}{\omega(e)}$. Similarly, the out-degree
  %   of $v \in V$ is $\deg^{\mathrm{out}}(v) = \sum_{e \colon e^{-} =
  %     v}{\omega(e)}$
  % \end{itemize}
\end{frame}
\begin{frame}[label=laplacians]
  \frametitle{Graph Laplacians}
  \begin{itemize}
  \item The \alert{combinatorial Laplacian} $\bm{L}$ of an undirected graph
    $G$ is the symmetric, positive semidefinite matrix with entries
    \begin{equation}
      \label{eq:33}
      \bm{L}(u,v) = \begin{cases}
        - \omega(u,v) & \text{if $u \not = v$ and $u \sim v$} \\
        \deg(u) & \text{if $u = v$} \\
        0 & \text{otherwise}
      \end{cases}
    \end{equation}
  \item $\bm{L} = \bm{D} - \bm{W}$ where $\bm{D}$ is the degree matrix
    and $\bm{W}$ is the similarity matrix, i.e., $\bm{W}(i,j) =
    \omega_{ij}$. 
  \item The \alert{normalized Laplacian} $\bm{\mathcal{L}}$ of an
    undirected graph $G$ is the symmetric, positive semidefinite
    matrix with entries
    \begin{equation}
      \bm{\mathcal{L}}(u,v) = \begin{cases}
        - \tfrac{\omega(u,v)}{\sqrt{\deg(u)}\sqrt{\deg(v)}} & 
        \text{if $u \not = v$ and $u \sim v$} \\
        1 & \text{if $u = v$} \\
        0 & \text{otherwise}
      \end{cases}
    \end{equation}
  \item $\bm{\mathcal{L}} = \bm{D}^{-1/2} \bm{L} \bm{D}^{-1/2} = \bm{I} -
    \bm{D}^{-1/2}\bm{W}\bm{D}^{-1/2}$.  
  % \item The second smallest eigenvalues of $\bm{L}$ or
  %   $\bm{\mathcal{L}}$ is a measure of algebraic connectivity (Fiedler) and is
  %   used in spectral clustering. 
  \end{itemize}
\end{frame}

\begin{frame}[label=random-walks]
  \frametitle{Random walks on undirected graphs}
  \onslide<1->{
    Let $G = (V,E,\omega)$ be an undirected graph. We define the transition matrix
    $\bm{P}_G = (p_{uv})$ of a Markov chain with state space $V$ as
  }
  \onslide<2->{
    \begin{equation}
      \label{eq:2}
      p_{uv} = \begin{cases}
        \tfrac{\omega(\{u,v\})}{\deg(u)} & \text{if $u \sim v$} \\
        0 & \text{otherwise}
      \end{cases}
    \end{equation}
  }
  \onslide<3->{
    Suppose that $G$ is connected. Then the Markov chain $\mathbf{X}$ associated with
    $\bm{P}_G$ satisfy the following properties.}
  \onslide<4->{
    \begin{itemize}
    \item $\mathbf{X}$ is \alert{irreducible}.
    \item $\pi(v) = \tfrac{\deg(v)}{\mathrm{Vol}(G)}$.
    \item $\bm{P}_G$ is \alert{time-reversible}.
    \end{itemize}
  }
\end{frame}

\begin{frame}[label=distance_geometry]
  \frametitle{Distance geometry}
  \onslide<1->{
    \begin{definition}[$\tau$ transform]
      \label{def:1}
      Let $\bm{P} = \bm{I} - \tfrac{1}{N}\bm{J}$ be a projection matrix
      where $\bm{I}$ is the identity matrix and $\bm{J}$ is the $N
      \times N$ matrix of all ones. Suppose that $\bm{A}$ is a
      dissimilarity matrix. The \alert{$\tau$ transform} of $\bm{A}$ is
      the matrix $\tau(\bm{A})$ given by
      \begin{equation}
        \label{eq:1}
        \tau(\bm{A}) = -\tfrac{1}{2} \bm{P} \bm{A} \bm{P}
      \end{equation}
    \end{definition}
  }
  \onslide<2->{
    \begin{definition}[$\kappa$ transform]
      Let $\bm{B}$ be a matrix . The
      $\kappa$ transform of $\bm{B}$ is the symmetric matrix
      $\kappa(\bm{B})$ with entries
      \begin{equation}
        \label{eq:34}
        \kappa(\bm{B})(i,j) = \bm{B}(i,i) - \bm{B}(i,j) - \bm{B}(j,i) +
        \bm{B}(j,j)
      \end{equation}
    \end{definition}
  }
\end{frame}
\begin{frame}
  \frametitle{Distance geometry (cont')}
  \begin{definition}[EDM-1 and EDM-2 matrices]
    \label{def:2}
    Let $\Delta^{(1)} = (\delta_{ij})$ and $\Delta^{(2)} =
    (\delta_{ij}^{2}) $ be $N \times N$ matrices of dissimilarity
    measure. $\Delta^{(1)}$ is an \alert{EDM-1 matrix} if there exists a $k
    \in \mathbb{N}$ and $N$ points $x_1, x_2, \dots, x_N \in
    \mathbb{R}^{k}$ such that $\delta_{ij} = \| x_i - x_j
    \|$. Similarly, $\Delta^{(2)}$ is an \alert{EDM-2 matrix} if
    $\delta_{ij}^{2} = \| x_i - x_j \|^2$. The \alert{embedding
      dimension} of $\Delta^{(1)}$ and $\Delta^{(2)}$ is the
    dimension $k$ of the Euclidean space $\mathbb{R}^{k}$. 
  \end{definition}
  \onslide<2->{
  \begin{fact}[Characterization of EDM-2 matrices (Schoenberg)]
    \label{thm:1}
    $\Delta^{(2)}$ is an EDM-2 matrix if and only if $\bm{B} =
    \tau(\Delta^{(2)})$ is \alert{positive semidefinite}. If $\bm{B}$
    is of rank $p$, then the embedding dimension of $\Delta^{(2)}$ is
    $p$. Similarly, if $\bm{B}$ is positive semidefinite of rank $p$,
    then $\Delta^{(2)} = \kappa(\bm{B})$ is an EDM-2 matrix with
    embedding dimension $p$. If $\bm{B}$ is also doubly centered,
    i.e., the sums of any rows and columns of $\bm{B}$ is $0$, then
    $\tau(\kappa(\bm{B})) = \bm{B}$.
  \end{fact}
  }
\end{frame}
\begin{frame}
  \frametitle{Classical multidimensional scaling}
  \onslide<1->{ If $\Delta^{(2)}$ is EDM-2, then an embedding can be
    found by factoring $\bm{B} = \tau(\Delta^{(2)})$ into $\bm{B} =
    \bm{X}\bm{X}^{T}$.  Classical multidimensional scaling (Torgerson
    \cite{torgesen52:_multid}) embeds an arbitrary dissimilarity
    matrix $\Delta^{(2)}$ by replacing $\bm{B} = \kappa(\Delta^{(2)})$
    with the nearest positive semidefinite matrix $\bm{B}^{*}$.  }
  \begin{enumerate}
  \item<2-> Set $\bm{B} =  \tau(\Delta^{(2)})$.  
  \item<3-> Compute the eigenvalues and eigenvectors of $\bm{B}$. Let
    $\lambda_0 \geq \lambda_1 \geq \dots \geq \lambda_{N-1}$ be the
    eigenvalues of $\bm{B}$ and $\bm{f}_0, \bm{f}_1, \dots,
    \bm{f}_{N-1}$ the corresponding eigenvectors. 
  \item<4-> Let $\lambda'_i = \max(\lambda_i, 0)$ for $i =
    0,1,2,\dots$.
  \item<5-> Set $\bm{B}^{*} = \sum_{i=0}^{N-1}{\lambda'_i
      \bm{f}_i \bm{f}_i^{T}}$.   
  \item<6-> Embed by factoring $\bm{B}^{*}$. 
\end{enumerate}
\end{frame}
\begin{frame}
  \frametitle{Distances and embeddings}
  \begin{itemize}[<+->]
  \item In the discussion that follows, there will be an interplay
    between mappings into Euclidean space and distances associated
    with the mappings.
  \item<2-> If $\Delta^{(2)}$ is an EDM-2, matrix, then $\bm{B}
    = \tau(\Delta^{(2)})$ is positive definite.  
  \item<3-> If $\bm{B}$ is positive semidefinite, then there is a
    matrix $\bm{X}$ such that $\bm{X} \bm{X}^{T} =
    \bm{B}$. Furthermore, if $\bm{X}^{(i)}$ and $\bm{X}^{(j)}$ is the
    $i$-th and $j$-th row of $\bm{X}$, then $\Delta^{(2)}(i,j) = \|
    \bm{X}^{(i)} - \bm{X}^{(j)} \|^2$.
  \end{itemize}
  \onslide<4->{
  \begin{exampleblock}{}
    \begin{equation*}
      \xymatrix{
        & & \bm{B} \ar@{<->}[ddll]^{\kappa(\bm{B})}_{\tau(\Delta^{(2)})}
        \ar@{<->}[ddrr]^{\bm{B}^{1/2}}_{\bm{X}\bm{X}^{T}} & &\\ 
        \\
        \Delta^{(2)} \ar@{<~}[rrrr]^{\|\bm{X}^{(i)} -
          \bm{X}^{(j)}\|^{2}} & & & & \bm{X} 
      }
    \end{equation*}
  \end{exampleblock}
  }
\end{frame}
\section{Manifold learning algorithms}
\begin{frame}
  \frametitle{Laplacian eigenmaps}
  Laplacian eigenmaps refers to a class of techniques
  that embeds using the eigenvalues and eigenvectors of the graph
  Laplacian. There are various perspectives regarding
  Laplacian eigenmaps. 
  \begin{itemize}
  \item<2-> Laplacian eigenmaps can be interpreted in the framework of
    spectral clustering, e.g., Shi \& Malik
    \cite{shi00:_normal_cuts_image_segmen}
  \item<3-> Laplacian eigenmaps can also be viewed in the framework of
    regularization and graph kernels, Smola \& Kondor
    \cite{smola03:_kernel_regul_graph}
  \item<4-> Belkin \& Niyogi \cite{belkin03:_laplac} justified Laplacian
    eigenmaps by viewing the graph Laplacians as a discrete
    approximation of the Laplace-Beltrami operator on a Riemannian
    manifold.  
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Laplacian eigenmaps (cont')}
  Let $\mathcal{X} = \{x_1,x_2,\dots,x_N\}$ be $N$ data points in some
  high-dimensional space. Laplacian eigenmaps can be described as
  follows:
  \begin{itemize}
  \item<2-> Construct a graph $G = (V,E,\omega)$ with $V = \mathcal{X}$
    and $\omega$ is a similarity measure.
    % \begin{itemize}
    % \item $\epsilon$ neighborhoods vs $K$-NN.
    % \item heat-kernels vs unweighted.
    % \end{itemize}
  \item<3-> Compute the eigenvalues $\lambda$ and eigenvectors
    $\bm{f}$ of the generalized eigenvalue problem
    \begin{equation}
      \label{eq:3}
      \bm{Lf} = \lambda \bm{Df}
    \end{equation}
    Note that if $\bm{f}$ is an eigenvector of Eq.~(\ref{eq:3}), then
    $\bm{D}^{-1/2}\bm{f}$ is an eigenvector of $\mathcal{L}$.
  \item<4-> Let $\lambda_0 \leq \lambda_1 \leq \dots \leq \lambda_{N-1}$ be
    the eigenvalues of Eq.~(\ref{eq:3}) and $\bm{f}_0, \bm{f}_1,
    \dots, \bm{f}_{N-1}$ be the corresponding eigenvectors. 
  \item<5-> Embed into $\mathbb{R}^{m}$ by
    \begin{equation}
      \label{eq:4}
      x_i \mapsto (\sigma(\lambda_1) \bm{f}_{1}(i), \sigma(\lambda_2),
      \bm{f}_{2}(i), \dots, \sigma(\lambda_m) \bm{f}_{m}(i))
    \end{equation}
    where $\sigma$ is a decreasing function on $[0,+\infty)$.  Note
    that $\bm{f}_0$ is constant and is thus ignored. One choice is
    $\sigma(\lambda) \equiv 1$. Another popular choice is
    $\sigma(\lambda_k) = 1/\sqrt{\lambda_k}$.
  \end{itemize}
\end{frame}

% \begin{frame}
%   \frametitle{Laplacian eigenmaps (cont')}
%   \onslide<1->{
%     Let $z_i \in \mathbb{R}^{m}$ be the embedding of $x_i$ into
%     $\mathbb{R}^{m}$. Laplacian eigenmaps minimize
%     \begin{equation}
%       \label{eq:5}
%       \sum_{x_i, x_j \in \mathcal{X}}{\|z_i - z_j \|^{2} \omega_{ij}}
%     \end{equation}
%   }
%   \onslide<2->{Thus Laplacian eigenmaps try to maps data points $x_i, x_j \in
%     \mathcal{X}$ that are similar ( large $\omega_{ij}$ ) to points
%     $z_i, z_j$ that are close in Euclidean space. }
% \end{frame}

\begin{frame}[label=laplacian_example]
  \frametitle{Laplacian eigenmaps example}
  \begin{figure}[htbp]
    \label{fig:helix}
    \centering
    \subfigure[][]{
      \includegraphics[width=45mm]{graphics/helix.pdf}
    }
    \hspace{3pt}
    \subfigure[][]{
      \includegraphics[width=45mm]{graphics/laplacian_helix.pdf}
    }
    \caption{Laplacian eigenmaps embedding of a torsoidal helix}
  \end{figure}
\end{frame}

\begin{frame}[label=diffusion_maps]
  \frametitle{Diffusion maps (Coifman \& Lafon
    \cite{coifman06:_diffus_maps})}
  \onslide<1->{
    Let $\mathcal{X} = \{x_1,x_2,\dots,x_N\}$ be $N$ data points in some
    high-dimensional space. Diffusion maps can be described as follows:}
  \begin{itemize}
  \item<2-> Construct a graph $G = (V,E,\omega)$ with $V =
    \mathcal{X}$.
  \item<3-> Generate the transition matrix $\bm{P}_G$ of $G$.
  \item<4-> Find the eigenvalues $\lambda_0 \geq \lambda_1 \geq \dots
    \geq \lambda_{N-1}$ and corresponding eigenvectors $\bm{f}_0, \bm{f}_1,
    \dots, \bm{f}_{N-1}$ of $\bm{P}_G$.
  \item<5-> Choose a time scale $t$ and embedding dimension $m$. Embed
    into $\mathbb{R}^{m}$ by
    \begin{equation}
      \label{eq:6}
      x_i \mapsto \bigl( \lambda_{1}^{t} \bm{f}_{1}(i),
      \lambda_{2}^{t} \bm{f}_{2}(i), \dots, \lambda_{m}^{t}
      \bm{f}_{m}(i))
    \end{equation}
    Again, $\bm{f}_{0}(i)$ is constant and thus is ignored. 
  \end{itemize}
\end{frame}

\begin{frame}[label=diffusion_example]
  \frametitle{Diffusion maps example}
  \begin{figure}[htbp]
    \label{fig:gaussian}
    \centering
    \subfigure[][]{
      \includegraphics[width=45mm]{graphics/gaussian.pdf}
    }
    \hspace{3pt}
    \subfigure[][]{
      \includegraphics[width=45mm]{graphics/gaussian_diffusion.pdf}
    }
    \caption{Diffusion maps embedding of a Gaussian in three dimension}
  \end{figure}
\end{frame}

% \begin{frame}[label=diffusion_multiscale]
%   \frametitle{Multiscale geometries of diffusion maps}
%   \subfiglabelskip = 0pt
%   \begin{figure}[htbp]
%     \centering
%     \subfigure[][]{
%       \label{fig:diff_map1-a}
%       \includegraphics[width=30mm]{graphics/wellsdata.pdf}
%     }
%     \hspace{8pt}
%     \subfigure[][]{
%       \label{fig:diff_map1-b}
%       \includegraphics[width=30mm]{graphics/wells1.pdf}
%     }
%     \\
%     \subfigure[][]{
%       \label{fig:diff_map1-c}
%       \includegraphics[width=30mm]{graphics/wells5.pdf}
%     }
%     \hspace{8pt}
%     \subfigure[][]{
%       \label{fig:diff_map1-d}
%       \includegraphics[width=30mm]{graphics/wells10.pdf}
%     }
%   \end{figure}
% \end{frame}

\section{Metrics on graphs}
\begin{frame}
  \frametitle{Graph metrics}
  \onslide<1->{
    Graph metrics measure the dissimilarity between pairs of vertices
    of the graphs. Examples of graph metrics are ... }
  \begin{itemize}
  \item<2-> Shortest path distances (Isomap).
  \item<3-> Expected commute times
  \item<4-> Diffusion distances (diffusion maps)
  \item<5-> Forest metrics.
  \end{itemize}
  \onslide<6->{ Expected commute time, diffusion distances, and forest
    metrics can be explained in terms of random walks on the
    underlying graph. }
\end{frame}
\begin{frame}[label=diffusion_distances]
  \frametitle{Diffusion distances}
  \begin{itemize}
  \item<1-> Let $\rho_{t}^{2}(x_i,x_j)$ be defined by
    \begin{equation}
      \label{eq:7}
      \rho_{t}^{2}(x_i,x_j) = \sum_{x_k \in \mathcal{X}}{
        \frac{\bigl(\bm{P}^{t}(x_i,x_k) -
          \bm{P}^{t}(x_j,x_k)\bigr)^2}{\pi(x_k)}}
    \end{equation}
  \item<2-> $\rho_{t}^2(x_i,x_j)$ is called the \alert{diffusion
      distance at time scale $t$} between $x_i$ and $x_j$. Diffusion
    distances are induced by diffusion maps. Specifically
    \begin{equation}
      \label{eq:8}
      \rho_{t}^{2}(x_i,x_j) = \sum_{k =
        1}^{N-1}{\lambda_k^{2t}(f_k(i) - f_k(j))^2} 
    \end{equation}
    % \item<3-> The time scale $t$ is interpreted as
    %   follows. $\bm{P}^{t}$ is the transition matrix of the random walk
    %   after $t$ steps. $\rho_{t}^{2}(x_i,x_j)$ thus measures the
    %   discrepancy at time $t$ between a random walk starting at
    %   $x_i$ against a random walk starting at $x_j$.
    % \item<4-> Thus at different time scale $t$, we have a different
    %   picture regarding the geometry of the data sets. \hyperlink{diffusion_multiscale}{\beamergotobutton{Example}}
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Expected commute time}
  \onslide<1->{
    Suppose that $G = (V,E,\omega)$ is an undirected graph with $\omega$
    being the similarity measure. Let $\bm{P}$ be the transition matrix of
    the \hyperlink{random-walks}{\beamergotobutton{random walk}} on
    $G$. } 
  \onslide<2->{
    \begin{definition}[First passage time]
      \label{def:3}
      The first passage time $\mathbb{E}_{u}[\tau_v]$ from $u
      \in V$ to $v \in V$ is 
      \begin{equation}
        \label{eq:10}
        \mathbb{E}_{u}[\tau_v] = \sum_{t = 0}^{\infty}{t \,
          \mathbb{P}(\tau_v = t \, | \, X_0 = u)} 
      \end{equation}
    \end{definition}  
  }
  \onslide<3->{
    \begin{definition}[Expected commute time]
      \label{def:4}
      The expected commute time $\delta(u,v)$ between $u \in V$ and $v \in V$
      \begin{equation}
        \label{eq:11}
        \delta(u,v) = \mathbb{E}_{u}[\tau_v] +
          \mathbb{E}_{v}[\tau_u]
      \end{equation}
    \end{definition}
  }
\end{frame}
\begin{frame}
  \frametitle{Expected commute time (cont')}
  \begin{itemize}[<+->]
  \item $\delta(u,v)$ is also known in
    the literature as \alert{resistance distance}
    \cite{klein93:_resis_distan} because of its relation to the notion
    of effective resistance if $G$ is viewed as an electrical network
    with $\omega$ being the conductance between nodes.
  \item Classical Multidimensional Scaling applied to $\Delta$
    thus defines an embedding of the vertices of $V$ into some
    Euclidean space $\mathbb{R}^{m}$.
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Embedding using expected commute time}
  \begin{fact}[Spectral form of $\delta(u,v)$ \cite{lovasz96:_random_graph}]
    \label{prop:1}
    Let $\bm{\mathcal{L}}$ be the normalized Laplacian of $G$. If
    $\lambda_0 \leq \lambda_1 \leq \dots \leq \lambda_{N-1}$ are the
    eigenvalues of $\mathcal{L}$ and $\bm{f}_0, \bm{f}_1, \dots,
    \bm{f}_{N-1}$ the corresponding eigenvectors, then
    \begin{equation}
      \label{eq:13}
      \delta(u,v) = \sum_{k =
        1}^{N-1}{\frac{1}{\lambda_k}\Bigl(\frac{\bm{f}_k(u)}{\sqrt{\deg(u)}}
        - \frac{\bm{f}_k(v)}{\sqrt{\deg(v)}}\Bigr)^2} 
    \end{equation}
    Thus $u \in V$ can be embed into $\mathbb{R}^{m}$ by
    \begin{equation}
      \label{eq:14}
      u \mapsto \frac{1}{\sqrt{\deg(u)}}\Bigl(
      \frac{\bm{f}_{1}(u)}{\sqrt{\lambda_1}},
      \frac{\bm{f}_{2}(u)}{\sqrt{\lambda_2}}, \dots,
      \frac{\bm{f}_{m}(u)}{\sqrt{\lambda_m}}\Bigr)
    \end{equation}
  \end{fact}
  \onslide<2->{
    Note that the embedding using expected commute time is equivalent
    to Laplacian eigenmaps with $\sigma(\lambda_k) =
    \tfrac{1}{\sqrt{\lambda_k}}$. 
  }
\end{frame}
\begin{frame}
  \frametitle{Diffusion maps and expected commute time}
  \onslide<1->{
    \begin{alertblock}{Question}
      Both diffusion distances and expected commute time are
      defined in terms of random walks. What are their connections, if
      any ?
    \end{alertblock}
  }
  \onslide<2->{
    \begin{alertblock}{Question}
      Diffusion maps
      \hyperlink{diffusion_maps<5>}{\beamergotobutton{embeds}} using the
      eigenvalues and eigenvectors of $\bm{P}$ while expected commute
      time embeds using the eigenvalues and eigenvectors of
      $\mathcal{L}$. Can we say anything about the embedding ?
    \end{alertblock}
  }
\end{frame}
\begin{frame}
  \frametitle{Diffusion maps and expected commute time (cont')}
  \onslide<1->{
    \begin{alertblock}{Question}
      Diffusion map embeddings vs expected commute times embeddings ?
    \end{alertblock}
  }
  \onslide<2->{
    \begin{exampleblock}{Answer: Anisotropic scaling matrix}
      Let $\lambda_k$ and $f_k$ be the $k$-th eigenvalue and
      eigenvector of $\mathcal{L}$ in non-increasing order. Let
      $\lambda'_{k}$ and $f'_k$ be the $k$-th eigenvalue and
      eigenvector of $\bm{P}$ in non-decreasing order. Then
      \begin{equation}
        \label{eq:16}
        \lambda'_k = 1 - \lambda_{k}, \qquad 
        f'_k(u) = \frac{f_k(u)}{\sqrt{\deg(u)}}
      \end{equation}
      Thus if $z_u$ and $z'_u$ are the embeddings of $u \in V$ into
      $\mathbb{R}^{m}$ using diffusion maps and expected commute
      time, respectively, then $z_u = \bm{T} z'_u$
      where 
      \begin{equation}
        \label{eq:18}
        \bm{T}(i,i) = \frac{\sqrt{1 - \lambda_k}}{\lambda_k^{t}},
        \qquad \bm{T}(i,j) = 0 \,\, \text{for $i \not= j$}
      \end{equation}
    \end{exampleblock}
  }
\end{frame}
\begin{frame}
  \frametitle{Diffusion maps and expected commute time (cont')}
  \onslide<1->{
    \begin{alertblock}{Question}
      Both diffusion distances and expected commute time are
      defined in terms of random walks. What are their connections, if
      any ?
    \end{alertblock}
  }
  \onslide<2->{
    \begin{exampleblock}{Answer: Expected commute time is sum of diffusion
        distances}
      Let $\delta_{\bm{P}^{2}}(u,v)$ be the expected commute time using
      the two-step random walk, i.e., we sampled the random walk on $G$
      only at even time. Then we have
      \begin{equation}
        \label{eq:15}
        \delta_{\bm{P}^{2}}(u,v) = \sum_{t =
          0}^{\infty}{\rho_{t}^{2}(u,v)}
      \end{equation}
    \end{exampleblock}
  }
\end{frame}
% \begin{frame}[label=forest_metrics]
%   \frametitle{Forest metrics
%     \cite{chebotarev02:_fores_metric_for_graph_vertic}}
%   \onslide<1->{
%     Let $G = (V,E,\omega)$ be an undirected graph, and $\alpha > 0$ a
%     constant. Denote by $\bm{Q}_{\alpha}$ the matrix $(\bm{I} + \alpha
%     \bm{L})^{-1}$. 
%   }
%   \onslide<2->{
%     \begin{definition}[Forest metrics]
%       \label{def:5}
%       The forest metric $\eta_\alpha(u,v)$ between $u \in V$ and
%       $v \in V$ is
%       \begin{equation}
%         \label{eq:19}
%         \eta_\alpha(u,v) = \bm{Q}_\alpha(u,u) - \bm{Q}_\alpha(u,v) -
%         \bm{Q}_\alpha(v,u) + \bm{Q}_\alpha(v,v)
%       \end{equation}
%     \end{definition}
%   }
%   \begin{itemize}
%   \item<3-> $\bm{Q}_{\alpha}$ is a symmetric, stochastic matrix.
%   \item<4-> $\bm{Q}_{\alpha}(u,v)$ is the probability that a rooted
%     spanning forest on $V$ has a tree rooted at $u$ containing $v$
%     (matrix-tree theorem
%     \cite{chebotarev97:_matrix_fores_theor_and_measur,moon94:_some_deter_expan_and_matrix_tree_theor}. 
%   \item<5-> $\tfrac{\alpha}{2}\eta_\alpha(u,v)$ approaches $\delta(u,v)$ as $\alpha
%     \rightarrow \infty$. 
%   \end{itemize}
% \end{frame}
% \begin{frame}
%   \frametitle{Forest metrics and random walks}
%   \onslide<1->{
%     \begin{alertblock}{Question}
%       Expected commute time can be defined in term of the pseudo-inverse
%       of $\bm{L}$ while forest metrics can be defined in terms of the
%       inverse of $\bm{I} + \alpha \bm{L}$. Also, $\tfrac{\alpha}{2}
%       \eta_{\alpha}(u,v) \rightarrow \delta(u,v)$. Is there a random walk
%       interpretation for forest metrics ?
%     \end{alertblock}
%   }
%   \onslide<2->{
%     \begin{exampleblock}{A not too satisfactory answer}
%       Let $G = (V,E,\omega)$ and $G_{*} = (V \cup \{v_*\}, E \cup \{
%       \{u, v_* \} \colon u \in V \}, \omega_*)$ be
%       the graph formed by augmenting $G$ with a vertex $v_* \notin
%       V$ as follows
%       \begin{equation}
%         \label{eq:20}
%         \omega_*(e) = \alpha \omega(e)\,\, \text{if $e \in E$}, \quad
%         \omega_{*}(e) = 1\,\, \text{otherwise}
%       \end{equation}
%       Given a random walk on $G_*$, starting at $v_*$, that visits
%       all $v \in V_*$, we can construct a spanning tree $T$ as
%       follows. Add to $T$ the edge $e = \{u,v\}$ if $e$ correspond to
%       the first visit of the random walk to either $u$ or $v$.
%       $\bm{Q}_{\alpha}(u,v)$ is then the probability that the tree $T$
%       contains the edge $\{v_*,u\}$ on the path from $v_*$ to $v$. 
%     \end{exampleblock}
%   }
% \end{frame}
% \begin{frame}[label=embedding_forest_metric]
%   \frametitle{Embedding using forest metrics}
%   \begin{fact}[Spectral form of $\eta_\alpha(u,v)$.] 
%     Let $\lambda_0 \leq \lambda_1 \leq \dots \leq \lambda_{N-1}$ be
%     the eigenvalues of $\bm{L}$ and $\bm{f}_0, \bm{f}_1, \dots,
%     \bm{f}_{N-1}$ be the corresponding eigenvectors. One has
%     \begin{equation}
%       \label{eq:21}
%       \eta_\alpha(u,v) = \sum_{k = 1}^{N-1}{\frac{1}{1 + \alpha \lambda_k}
%         \Bigl(f_k(u) - f_k(v)\Bigr)^2}
%     \end{equation}
%     Thus $u \in V$ can be embed into $\mathbb{R}^{m}$ by
%     \begin{equation}
%       \label{eq:22}
%       u \mapsto \Bigl( \frac{f_1(u)}{\sqrt{1 + \alpha \lambda_1}},
%       \frac{f_2(u)}{\sqrt{1 + \alpha \lambda_2}}, \dots,
%       \frac{f_m(u)}{\sqrt{1 + \alpha \lambda_m}} \Bigr)
%     \end{equation}
%   \end{fact}
% \end{frame}
\section{Techniques for directed graphs}
\begin{frame}
  \frametitle{Directed graphs in manifold learning algorithms}
  \begin{itemize}
  \item<1-> Directed graphs arise naturally when one tries to construct
    a graphical representation of data points by $K$-NN.
  \item<2-> There are various scenarios where proximity measures are
    asymmetric. For example, driving time between points $A$ and $B$,
    or hippocampus images. 
  \item<3-> However, many of the well known manifold learning
    algorithms such as Laplacian eigenmaps and diffusion maps can only
    handle undirected graphs. This is because these methods perform
    spectral decomposition on matrices that are symmetric only
    when the graphs are undirected.
  \item<4-> Expected commute times and forest metrics on a graph $G$
    are Euclidean distances when $G$ is undirected. It will be nice if
    these graph metrics are also Euclidean distances when $G$ is
    directed. 
  \end{itemize}
\end{frame}
\begin{frame}[label=directed_diffusion_distances]
  \frametitle{Diffusion distances for directed graphs}
  \onslide<1->{
    The starting point is the definition of
    $\rho_{t}^{2}(u,v)$ as
    \begin{equation}
      \label{eq:23}
      \rho_{t}^{2}(u,v) = \sum_{w \in V}{
        \frac{\bigl(\bm{P}^{t}(u,w) -
          \bm{P}^{t}(v,w)\bigr)^2}{\pi(w)}}     
    \end{equation}
  }
  \onslide<2->{
    \begin{exampleblock}{$\Delta_{\rho}^{(2)}$ is EDM-2}
      Let $G$ be a directed graph and $\bm{P}$ be its transition
      matrix. Denote by $\bm{P}_{*}$ the time-reversal of $\bm{P}$. One has
      \begin{equation}
        \label{eq:24}
        \rho_{t}^{2}(u,v) = \frac{\bm{R}^{(t)}(u,u) - \bm{R}^{(t)}(v,u)}{\pi(u)} +
        \frac{\bm{R}^{(t)}(v,v) - \bm{R}^{(t)}(u,v)}{\pi(v)}
      \end{equation}
      where $\bm{R}^{(t)} = \bm{P}^{t} \bm{P}_{*}^{t}$. $\bm{R}^{(t)}
      \bm{\Pi}^{-1}$ is \alert{positive
        semidefinite}. $\Delta_{\rho}^{(2)} = \kappa(\bm{R}^{(t)}
      \bm{\Pi}^{-1})$ is thus EDM-2. 
    \end{exampleblock}
  }
\end{frame}
% \begin{frame}
%   \frametitle{Diffusion distances for directed graphs (cont')}
%   \begin{itemize}
%   \item<1-> Diffusion maps for directed graphs is then the embedding
%     of $u \in V$ into $\mathbb{R}^{m}$ by using the eigenvalues and
%     eigenvectors of $\bm{R}^{(t)}$. Let $\lambda_0 \geq
%     \lambda_1 \geq \dots \geq \lambda_{N-1}$ be the eigenvalues of
%     $\bm{R}^{(t)}$ and $f_0, f_1, \dots, f_{N-1}$ be the corresponding
%     eigenvectors. The embedding into $\mathbb{R}^{m}$ is
%     \onslide<2->{
%       \begin{equation}
%         \label{eq:25}
%         u \mapsto \Bigl( \lambda_1 f_1(u), \lambda_2 f_2(u), \dots,
%         \lambda_{m} f_m(u) \Bigr)
%       \end{equation}
%     }
%   \item<3-> The embeddding of Eq.(\ref{eq:25}) reduces to the normal
%     diffusion maps when $G$ is undirected. 
%   \end{itemize}
% \end{frame}
\begin{frame}
  \frametitle{Expected commute time for directed graphs}
  \begin{itemize}
  \item<1-> Expected commute time, i.e., $\delta(u,v) = \mathbb{E}_{u}[\tau_v] +
    \mathbb{E}_{v}[\tau_u]$, is well defined for directed graphs.
  \item<2-> If the transition matrix $\bm{P}$ is \alert{irreducible}
    and \alert{aperiodic}, then $\lim_{k \rightarrow \infty}
    \bm{P}^{k} = \bm{W}$ exists. In fact, $\bm{W} = \bm{e}^{T}
    \bm{\pi}$.
  \item<3-> Let $\bm{Z}$ be the \alert{fundamental matrix} of
    Kemeny-Snell \cite{kemeny83:_finit_markov_chain}
    \begin{equation}
      \label{eq:26}
      \bm{Z} = (\bm{I} - \bm{P} + \bm{W})^{-1}
    \end{equation}
  \item<4-> The matrix $\Delta_{\delta}$ of expected commute time is
    then \cite{kemeny83:_finit_markov_chain}
    \begin{equation}
      \label{eq:27}
      \Delta_{\delta} = \tfrac{1}{2} \kappa(\bm{Z}\bm{\Pi}^{-1} +
      \bm{\Pi}^{-1} \bm{Z}^{T}) 
    \end{equation}
  \end{itemize}
  \onslide<5->{
    \begin{exampleblock}{$\Delta_\delta$ is EDM-2
        (Boley \cite{boley09:_gener_laplac})}
      $\bm{Z}\bm{\Pi}^{-1} + \bm{\Pi}^{-1} \bm{Z}^{T}$ is
      positive semidefinite. $\Delta_{\delta}$ is thus EDM-2.
    }
  \end{exampleblock}
\end{frame}
% \begin{frame}
%   \frametitle{Forest metrics for directed graphs}
%   \begin{itemize}
%   \item<1-> If $G$ is an undirected, 
%     \begin{equation}
%       \label{eq:28}
%       \bm{Q}_{\alpha} = (\bm{I} +
%       \alpha \bm{L})^{-1} = \beta(\bm{I} - \bm{P} +
%       \beta \bm{\Pi}^{-1})^{-1} \bm{\Pi}^{-1} 
%     \end{equation}
%     for some constant $\beta$ depending on $\alpha$.
%   \item<2-> If we use Eq.~(\ref{eq:28}) as the starting point for the
%     notion of forest metrics for directed graphs, then
%     \begin{equation}
%       \label{eq:29}
%       \Delta_{\eta} = \tfrac{1}{2}\kappa( \bm{Q}_\alpha +
%       \bm{Q}_{\alpha}^{T} )
%     \end{equation}
%   \end{itemize}
%   \onslide<3->{
%     \begin{exampleblock}{$\Delta_{\eta}$ is EDM-2}
%       $\bm{Q}_{\alpha} + \bm{Q}_{\alpha}^{T}$ is positive
%       semidefinite. $\Delta_{\eta}$ is thus EDM-2.
%     }
%   \end{exampleblock}
% \end{frame}
% \begin{frame}
%   \frametitle{Laplacian eigenmaps for directed graphs}
%   \onslide<1->{
%     Extending Laplacian eigenmaps to directed graphs is a
%     little more ambiguous since Laplacian eigenmaps (in the sense of
%     Belkin \& Niyogi \cite{belkin03:_laplac}) ignored the
%     eigenvalues.
%   }
%   \onslide<2->{
%     \begin{exampleblock}{Distance measure induced by Laplacian eigenmaps}
%       Let $G$ be an undirected graph. Let $z_u$ and $z_v$ be the embedding
%       coordinates of $u$ and $v$ into $\mathbb{R}^{N-1}$ using Laplacian
%       eigenmaps. Then
%       \begin{equation}
%         \label{eq:30}
%         \| z_u - z_v \|^{2} = \frac{1}{\deg(u)} + \frac{1}{\deg(v)} = \frac{1}{\mathrm{Vol}(G)} \Bigl( \frac{1}{\pi(u)} + \frac{1}{\pi(v)} \Bigr)
%       \end{equation}
%     \end{exampleblock}
%   } \onslide<3->{ Thus if we use Eq.~(\ref{eq:30}) as the starting point
%     for the notion of the distance associated with Laplacian eigenmaps
%     for directed graphs, then the matrix $\Delta^{(2)}$ of pairwise
%     squared Euclidean distance between $z_u$ and $z_v$ in
%     $\mathbb{R}^{N-1}$ is an EDM-2 matrix.  }
% \end{frame}
% \begin{frame}
%   \frametitle{Laplacian eigenmaps (cont')}
%   \onslide<1->{ Laplacian eigenmaps for undirected graphs was defined
%     as the embedding using eigenvectors of $\bm{P}$. However, if the
%     graph is directed, $\bm{P}$ might have non-real eigenvectors and
%     thus embedding using eigenvectors of $\bm{P}$ might not be possible. To
%     extend Laplacian eigenmaps to directed graphs, one might require
%     that the extensions satisfy the following requirements }
%   \begin{enumerate}
%   \item<2-> The embedding is into the eigenvectors of some transition
%     matrix $\bm{P}'$ related to $\bm{P}$.
%   \item<3-> The matrix $\bm{P}'$ has real eigenvalues and eigenvectors.
%   \item<4-> The matrix $\Delta^{(2)}$ of pairwise squared Euclidean
%     distances in $\mathbb{R}^{N-1}$ satisfy $\Delta^{(2)} =
%     \kappa(\bm{\Pi}^{-1})$.
%   \item<5-> The embedding reduces to Laplacian eigenmaps when $G$ is undirected.
%   \end{enumerate}
%   \onslide<6->{
%     \begin{exampleblock}{Examples of transition matrices}
%       $\bm{P}'$ can be, for example, either $\tfrac{1}{2}(\bm{P} + \bm{P}_*),
%       \bm{P}\bm{P}_*$, or $\bm{P}_*\bm{P}$.
%     \end{exampleblock}
%   }
%   \onslide<7->{
%     \begin{alertblock}{Open Question}
%       Which transition matrix is most suitable ?
%     \end{alertblock}
%   }
% \end{frame}
\section{Canonical representation}
\begin{frame}[label=canonical_representation]
  \frametitle{Canonical representation}
  % \begin{itemize}
  % \item<1-> 
  %   In the preceding discussion of graphs metrics and manifold
  %   learning algorithms, there's an interplay between mappings into
  %   Euclidean space and distances associated with the mappings.
  % \item<2-> If $\Delta^{(2)}$ is an EDM-2, matrix, then $\bm{B}
  %   = \tau(\Delta^{(2)})$ is positive definite. If $\bm{B}$ is positive
  %   semidefinite, then $\kappa(\bm{B})$ is an EDM-2 matrix.   
  % \item<3-> If $\bm{B}$ is positive semidefinite, then there is a
  %   matrix $\bm{X}$ such that $\bm{X} \bm{X}^{T} =
  %   \bm{B}$. Furthermore, if $\bm{X}^{(i)}$ and $\bm{X}^{(j)}$ is the
  %   $i$-th and $j$-th row of $\bm{X}$, then $\Delta^{(2)}(i,j) = \|
  %   X^{(i)} - X^{(j)} \|^2$.
  % \end{itemize}
  % \onslide<4->{
  \begin{exampleblock}{}
    \begin{equation*}
      \xymatrix{
        & & \bm{B} \ar@{<->}[ddll]^{\kappa(\bm{B})}_{\tau(\Delta^{(2)})}
        \ar@{<->}[ddrr]^{\bm{B}^{1/2}}_{\bm{X}\bm{X}^{T}} & &\\ 
        \\
        \Delta^{(2)} \ar@{<~}[rrrr]^{\|\bm{X}^{(i)} -
          \bm{X}^{(j)}\|^{2}} & & & & \bm{X} 
      }
    \end{equation*}
  \end{exampleblock}
\end{frame}
\begin{frame}
  \frametitle{Canonical representation (cont')}
  \begin{itemize}
  \item<1-> The matrix $\bm{B}$ is our notion of the canonical
    representation for manifold learning algorithms. 
  \item<2-> The canonical representation for several of the previously
    discussed methods are
    \begin{enumerate}
    \item<3-> Belkin \& Niyogi's Laplacian eigenmaps: $\bm{B} = \bm{\Pi}^{-1}$.
    \item<4-> Diffusion maps: $\bm{B} = \bm{P}^{t} \bm{P}_{*}^{t}
      \bm{\Pi}^{-1}$.
    \item<5-> Commute time: $\bm{B} = \Bigl((\bm{I} - \bm{P} +
      \bm{W})^{-1} + (\bm{I} - \bm{P}_{*} + \bm{W})^{-1}\Bigr)
      \bm{\Pi}^{-1}$.
    \item<6-> Forest metrics: $\bm{B} = \Bigl((\bm{I} - \bm{P} + \beta
      \bm{\Pi}^{-1})^{-1} + (\bm{I} - \bm{P}_{*} + \beta
      \bm{\Pi}^{-1})^{-1}\Bigr) \bm{\Pi}^{-1}$.
    \end{enumerate}
  \item<7->
    All of the above canonical representations are expressible
    in terms of the transition matrix $\bm{P}$ and thus are agnostic to
    whether the graph is directed or undirected. 
  \end{itemize}
  % \onslide<8->{
  %   \begin{alertblock}{Open Question}
  %     Investigate other measures of distances on graphs and their
  %     canonical representation. 
  %   \end{alertblock}
  % }
\end{frame}
\begin{frame}
  \frametitle{Future work and open questions}
  \begin{itemize}[<+->]
  \item Analyze \alert{real} data.
  \item Construct random walks using dissimilarity measures.
  \item What's the role of the matrix $\bm{\Pi}^{-1}$ and why does it
    appear in all of the $\bm{B}$ matrices in our list of canonical
    representations ? 
  \item A more thorough understanding of forest metrics and random walks.
  \item A more thorough understanding of Laplacian eigenmaps for
    directed graphs.
  \item Investigate other measures of distances on graphs.
    \begin{itemize}
      \item Not all positive semidefinite matrices $\bm{B}$ will have
        an easily interpretable $\Delta^{(2)} = \kappa(\bm{B})$.
      \item Learning the graph metrics based on the data.   
    \end{itemize}
  \item Investigate the convergence properties of the eigenvalues and
    eigenvectors of $\bm{P}$ as number of data points increases.
  \end{itemize}
\end{frame}
\bibliography{proposal}
% \appendix
% \begin{frame}[label=graph_construction]
%   \frametitle{Graph construction}
%   \begin{itemize}
%   \item $\epsilon$-neighborhood vs $K$-NN. In
%     $\epsilon$-neighborhood graph construction, $u \in V$ is
%     adjacent to $v \in V$ if the distance between $u$ and $v$ is at
%     most $\epsilon$ for some $\epsilon$. In $K$-NN graph construct,
%     $\{u,v\} \in E$ or $(u,v) \in E$ if $v$ is one of the $K$
%     nearest neighbors of $u$.
%   \item Heat-kernel weights. The similarity measure $\omega(u,v)$ is
%     given by
%     \begin{equation}
%       \label{eq:32}
%       \omega(u,v) = \exp \Bigl( - \frac{\|u - v\|^2}{\sigma}\Bigr)
%     \end{equation}
%     for some $\sigma > 0$. 
%   \end{itemize}
% \end{frame}
\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
