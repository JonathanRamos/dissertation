@Article{zhang03:_intel_data_engin_autom_learn,
  author =		 {Z. Zhang and H. Zha},
  title =		 {Nonlinear Dimension Reduction via Local Tangent
                  Space Alignment},
  year =		 {2003},
  volume =		 {2690},
  journal =		 {Lecture Notes in Computer Science},
  pages =		 {477-481},
  annote =		 { The book chapter presented an algorithm for
                  manifold learning that attempts to represent the
                  local geometry of the manifold using tangent
                  spaces. The tangent spaces are learned by fitting an
                  affine subspace to the neighborhood of every data
                  point. Each of the tangent spaces are then aligned
                  so that data points that lie in the overlap of
                  multiple neighborhoods have consistent coordinates.
                  }
}




@Unpublished{radl09,
  author = 		 {A. Radl and U. von Luxburg and M. Hein},
  title = 		 {The resistance distance is meaningless on large random graphs},
  note = 		 {Presented at the NIPS Workshop on analyzing network and learning with graphs},
  year =         2009
  }

@Article{pearson01:_on,
  author =		 {K. Pearson},
  title =		 {On lines and planes of closest fit to a system of
                  points in space},
  journal =		 {Philosophical Magazine},
  year =		 1901,
  volume =		 2,
  pages =		 {557-572}
}

@InBook{burges05:_data,
  author =		 {C. J. C. Burges},
  title =		 {Data mining and knowledge discovery handbook: A
                  complete guide for researches and practitioners},
  chapter =		 {Geometric methods for feature extraction and
                  dimensional reduction},
  publisher =	 {Kluwer},
  year =		 2005
}

@Book{gorban08:_princ_manif,
  editor = 	 {A. N. Gorban and B. K\'egl and D. C. Wunsch and A. Zinovyev},
  title = 		 {Principal Manifolds for data visualization and dimension reduction},
  publisher = 	 {Springer Verlag},
  year = 		 2008,
  volume = 	 58,
  series = 	 {Lecture Notes in Computation Science and Engineering}}

@Article{hotelling33:_analy,
  author =		 {H. Hotelling},
  title =		 {Analysis of a complex of statistical variables into
                  principle components},
  journal =		 {Journal of Educational Psychology},
  year =		 1933,
  volume =		 24,
  pages =		 {417-441}
}

@Article{hastie89:_princ,
  author = 		 {T. Hastie and W. Stuetzle},
  title = 		 {Principal curves},
  journal = 	 {Journal of the American Statistical Association},
  year = 		 1989,
  volume = 	 84,
  pages = 	 {502-516}}
  


@Book{jolliffe02:_princ_compon_analy,
  author = 	 {I. T. Jolliffe},
  title = 		 {Principal Component Analysis},
  publisher = 	 {Springer},
  year = 		 2002,
  edition = 	 {2nd}}

@PhdThesis{hastie84:_princ,
  author = 		 {T. Hastie},
  title = 		 {Principal curves and surfaces},
  school = 		 {Stanford University},
  year = 		 1984}

@Article{tipping99:_mixtur,
  author =		 {M. E. Tipping and C. M. Bishop},
  title =		 {Mixtures of probabilistic principal components
                  analysis},
  journal =		 {Neural computation},
  year =		 1999,
  pages =		 {443-482}
}

@Article{brand05:_chart,
  author = 		 {M. Brand},
  title = 		 {Charting a manifold},
  journal = 	 {Advances in Neural Information Processing Systems},
  year = 		 2005}

@InProceedings{brand05:_from,
  author =		 {M. Brand},
  title =		 {From subspaces to submanifolds},
  booktitle =	 {Proceedings of the British Machine Vision
                  Conference},
  year =		 2005
}

@Article{gower66:_some,
  author =		 {J. C. Gower},
  title =		 {Some distance properties of latent root and vector
                  methods used in multivariate analysis},
  journal =		 {Biometrika},
  year =		 1966,
  volume =		 53,
  pages =		 {325-338}
}

@Article{eckart36:_approx,
  author =		 {C. Eckart and G. Young},
  title =		 {Approximation of one matrix by another of lower
                  rank},
  journal =		 {Psychometrika},
  year =		 1936,
  volume =		 1,
  pages =		 {211-218}
}

@Article{kim09:_semi_regres_hessian,
  author =		 {K. I. Kim and F. Steinke and M. Hein},
  title =		 {Semi-supervised Regression using {H}essian energy
                  with an application to semi-supervised
                  dimensionality reduction},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2009,
  volume =		 22
}

@Article{critchley88:_certain_linear_mappin,
  author =		 {F. Critchley},
  title =		 {On Certain Linear Mappings between inner-product and
                  squared-distance matrices},
  journal =		 {Linear Algebra and Its Applications},
  year =		 1988,
  volume =		 105,
  pages =		 {91-107}
}

@Article{chebotarev02:_fores_laplac,
  author = 		 {P. Chebotarev and R. Agaev},
  title = 		 {Forest matrices around the {L}aplacian matrix},
  journal = 	 {Linear ALgebra and its Applications},
  year = 		 2002,
  volume = 	 356,
  pages = 	 {253-274}}

@Article{chaiken82,
  author = 		 {S. Chaiken},
  title = 		 {A combinatorial proof of the all minors matrix tree theorem},
  journal = 	 {SIAM Journal on Algebraic and Discrete Methods},
  year = 		 1982,
  volume = 	 3,
  pages = 	 {319-329}}

@Book{cvetkovic80:_spect_graph_theor_applic,
  author =		 {D. M. Cvetkovi\'{c} and M. Doob and H. Sachs},
  title =		 {Spectra of Graphs, Theory and Applications},
  publisher =	 {Academic Press},
  year =		 1980
}


@Article{silva02:_global,
  author =		 {V. de Silva and J. B. Tenenbaum},
  title =		 {Global versus local methods in nonlinear
                  dimensionality reduction},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2002,
  volume =		 15
}

@Book{chung97:_spect_graph_theor,
  author =		 {F. Chung},
  title =		 {Spectral Graph Theory},
  publisher =	 {Conference Board of the Matehmatical Sciences},
  year =		 1997,
  volume =		 92,
  series =		 {CBMS Regional Conference Series in Mathematics}
}

@Article{chebotarev02:_fores_metric_for_graph_vertic,
  author =		 {P. Chebotarev and E. Shamis},
  title =		 {The Forest Metrics for Graph Vertices},
  journal =		 {Electronic Notes in Discrete Mathematics},
  year =		 2002,
  volume =		 11,
  pages =		 {98-107},
  annote =		 {The paper introduced the notion of forest metrics
                  for weighted graphs. An interpretation of forest
                  metrics as the $\kappa$ transform of a stochastic
                  matrix whose entries are proportional to the number
                  of spanning $2$-forests on the graph is provided. }
}


@Book{gamelin01:_compl_analy,
  author = 	 {T. W. Gamelin},
  title = 		 {Complex Analysis},
  publisher = 	 {Springer},
  year = 		 2001}

@Article{chebotarev97:_matrix_fores_theor_and_measur,
  author =		 {P. Chebotarev and E. Shamis},
  title =		 {The Matrix-Forest Theorem and Measuring Relations in
                  Small Social Groups},
  journal =		 {Automation and Remote Control},
  year =		 1997,
  volume =		 58,
  pages =		 {1505-1514}
}

@Article{moon94:_some_deter_expan_and_matrix_tree_theor,
  author =		 {J.W. Moon},
  title =		 {Some determinant expansions and the matrix-tree
                  theorem},
  journal =		 {Discrete Mathematics},
  year =		 1994,
  volume =		 124,
  pages =		 {163-171}
}

@Article{propp98:_how_markov,
  author =		 {J. Propp and D. Wilson},
  title =		 {How to get a perfectly random sample from a generic
                  {M}arkov chain and generate a random spanning tree
                  of a directed graph},
  journal =		 {Journal of Algorithms},
  year =		 1998,
  volume =		 27,
  pages =		 {170-217}
}

@Book{horn94:_topic_in_matrix_analy,
  author =		 {R.A. Horn and C.R. Johnson},
  title =		 {Topics in Matrix Analysis},
  publisher =	 {Cambridge University Press},
  year =		 1994
}

@Article{bapat99:_resis_distan_in_graph,
  author =		 {R.B. Bapat},
  title =		 {Resistance Distance in Graphs},
  journal =		 {The Mathematics Student},
  year =		 1999,
  volume =		 68,
  pages =		 {87-98}
}

@Article{mosbah99:_non_unifor_random_spann_trees_weigh_graph,
  author =		 {M. Mosbah and N. Saheb},
  title =		 {Non-uniform random spanning trees on weighted
                  graphs},
  journal =		 {Theoretical Computer Science},
  year =		 1999,
  volume =		 218,
  pages =		 {263-271},
  annote =		 {The paper show that a random spanning tree, with
                  probabilities proportional to its weights, on
                  weighted graphs can be constructed by using the
                  edges corresponding to the first hitting time as
                  given by a random walk on the graph itself. }
}
@Article{yen08:_famil_of_dissim_measur_between,
  author =		 {L. Yen and M. Saerens and A. Mantrach and M. Shimbo},
  title =		 {A family of dissimilarity measures between nodes
                  generalizing both the shortest-path and the
                  commute-time distances},
  journal =	 {Proceeding of the 14th ACM SIGKDD international
                  conference on Knowledge discover},
  pages =		 {785-793},
  year =		 2008,
  annote =	 { The paper presented a family of dissimilarity
                  measures parameterized by $\theta$. The family
                  approaches expected commute time for $\theta
                  \rightarrow 0$ and shortest path distances for
                  $\theta \rightarrow \infty$. The dissimilarity
                  measures for $0 < \theta < \infty$ is constructed so
                  as to minimize a criterion based on
                  KL-divergences. },
}
@Article{klein93:_resis_distan,
  author =		 {D.J. Klein and M. Randic},
  title =		 {Resistance distance},
  journal =		 {Journal of Mathematical Chemistry},
  year =		 1993,
  volume =		 12,
  pages =		 {81-95}
}

@Article{coifman06:_diffus_maps,
  author =		 {R. Coifman and S. Lafon},
  title =		 {Diffusion maps},
  journal =		 {Applied and Computational Harmonic Analysis},
  year =		 2006,
  volume =		 21,
  pages =		 {5-30},
  annote =		 {The paper introduced diffusion maps and diffusion
                  distances.  A family of probability transition
                  function parameterized by $\alpha \in (0,1)$ is also
                  introduced. It is shown that as the number of
                  sampling points in creased and the time between
                  transition decreased, the probability transition
                  function define a semigroup operator $T$ whose
                  generator $\mathcal{A}$ corresponds to various
                  operators such as the Laplace-Beltrami operator and
                  the Fokker-Plank operator.  }
}
@Unpublished{chebotarev08:_new_famil_of_graph_distan,
  author =		 {P. Chebotarev},
  title =		 {A new family of graph distances},
  note =		 {arXiv:0810.2717v2},
  year =		 2008,
  annote =		 {The paper discussed a family of graph distances
                  parameterized by $\alpha$. This family approaches
                  the shortest path distances and expected commute
                  time as $\alpha \rightarrow 0$, $\alpha \rightarrow
                  \infty$, respectively. However, experimental
                  evidence seems to suggest that the convergence is
                  unstable. }
}

@Book{levin09:_markov_chain_mixin_times,
  author =		 {D.A. Levin and Y. Peres and E.L. Wilmer},
  title =		 {Markov Chains and Mixing Times},
  publisher =	 {American Mathematical Society},
  year =		 2009
}

@Unpublished{aldous99:_rever_markov,
  author =		 {D. Aldous and J.A. Fill},
  title =		 {Reversible {M}arkov chains and random walks on
                  graphs},
  note =		 {Manuscript available at
                  \url{"http://www.stat.berkeley.edu/~aldous/RWG/book.html"}},
  year =		 1999
}

@Article{lovasz96:_random_graph,
  author =		 {L. Lov\'{a}sz},
  title =		 {Random walks on Graphs: A Survey},
  journal =		 {Bolyai Society Mathematical Studies},
  year =		 1996,
  volume =		 2,
  pages =		 {353-397},
  annote =		 {The paper discussed random walks on unweighted,
                  undirected graphs. Various inequalities for expected
                  commute time are given. Connections between random
                  walks and sampling are also illustrated. The paper
                  gave an expression for the expected commute time in
                  terms of the eigenvalues and eigenvectors of the
                  normalized Laplacian matrix. }
}



@InBook{scholkopf97:_lectur_notes_comput_scien,
  author =		 {B. Sch\"{o}lkopf and A. J. Smola and K. R. Mueller},
  title =		 {Lecture Notes in Computer Science},
  chapter =		 {Kernel Principal Component Analysis},
  publisher =	 {Springer Verlag},
  year =		 1997,
  volume =		 1327,
  pages =		 {583-588}
}

@Article{roweis00:_nonlin,
  author =		 {S.T. Roweis and L.K. Saul},
  title =		 {Nonlinear dimensionality reduction by locally linear
                  embedding},
  journal =		 {Science},
  year =		 2000,
  volume =		 290,
  pages =		 {2323-2326},
}

@Article{belkin03:_laplac,
  author =		 {M. Belkin and P. Niyogi},
  title =		 {Laplacian eigenmaps for dimensionality reduction and
                  data representation},
  journal =		 {Neural Computation},
  year =		 2003,
  volume =		 15,
  pages =		 {1373-1396},
}

@Book{bapat97:_nonneg_matric_applic,
  author =		 {R. B. Bapat and T. E. S. Raghavan},
  title =		 {Nonnegative Matrices and Applications},
  publisher =	 {Cambridge University Press},
  year =		 1997
}

@Article{torgesen52:_multid,
  author =		 {W. S. Torgesen},
  title =		 {Multidimensional scaling: {I}. {T}heory and method},
  journal =		 {Psychometrika},
  year =		 1952,
  volume =		 17,
  pages =		 {401-419},
}

@Article{tenebaum00:_global_geomet_framew_nonlin_dimen_reduc,
  author =		 {J. B. Tenenbaum and V. de Silva and J. C. Langford},
  title =		 {A Global Geometric Framework for Nonlinear
                  Dimensionality Reduction},
  journal =		 {Science},
  year =		 2000,
  volume =		 290,
  pages =		 {2319-2323},
  month =		 {December},
}

@Article{schoenberg38:_metric,
  author =		 {I. J. Schoenberg},
  title =		 {Metric spaces and positive definite functions},
  journal =		 {Transaction of the American Mathematical Society},
  year =		 1938,
  volume =		 44,
  pages =		 {522-553},
}

@Book{cox01:_multid,
  author =		 {T. F. Cox and M. A. A. Cox},
  title =		 {Multidimensional scaling},
  publisher =	 {Chapman and Hall},
  year =		 2001
}

@Book{borg05:_moder,
  author =		 {I. Borg and P. J. F. Groenen},
  title =		 {Modern multidimensional scaling},
  publisher =	 {Springer},
  year =		 2005,
  edition =		 2 
}

@Book{kemeny83:_finit_markov_chain,
  author =		 {J. G. Kemeny and J. L. Snell},
  title =		 {Finite Markov Chains},
  publisher =	 {Springer},
  year =		 1983,
}

@Article{qui07:_clust,
  author =		 {H. Qui and E.R. Hancock},
  title =		 {Clustering and embedding using commute times},
  journal =		 {IEEE transactions in Pattern Analysis and Machine
                  Intelligence},
  year =		 2007,
  volume =		 29,
  pages =		 {1873-1890},
  annote =	 { The paper exploits the properties of expected
                  commute time for the purposes of embedding and
                  clustering. The authors noted the connections
                  between expected commute time embedding and
                  Laplacian eigenmaps and diffusion maps. The paper
                  also argued that there are scenarios where the
                  clustering using expected commute time is superior
                  to normalized cuts. }
}
@Book{doyle84:_random_walks_elect_networ,
  author =		 {P.G. Doyle and J.L. Snell},
  title =		 {Random Walks and Electrical Networks},
  publisher =	 {Mathematical Association of America},
  year =		 1984
}


@Article{belkin06:_conver_laplac_eigen,
  author =		 {M. Belkin and P. Niyogi},
  title =		 {Convergence of {L}aplacian Eigenmaps},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2006,
  volume =		 19
}

@TechReport{boley09:_gener_laplac,
  author =		 {D. Boley},
  title =		 {Generalized {L}aplacian and first transit times for
                  directed graphs},
  institution =	 {University of Minnesota},
  year =		 2009,
  number =		 {09-009},
  annote =	 { The technical report proved that expected commute
                  time for directed graphs is also a Euclidean
                  distance measure. }
}

@InProceedings{ham04,
  author =		 {J. Ham and D. D. Lee and S. Mika and
                  B. Sch\"{o}lkopf},
  title =		 {A kernel view of the dimensionality reduction of
                  manifolds},
  booktitle =	 {Proceedings of the 21$^{\textrm{st}}$ International
                  Conference on Machine Learning},
  year =		 2004,
  annote =		 { The paper showed that several dimensionality
                  reduction algorithms, namely Isomap, Laplacian
                  eigenmaps, and LLE, can be viewed as kernel methods.
                  }
}

@Article{luxburg08:_consis,
  author =		 {U. Von Luxburg and M. Belkin and O. Bousquet},
  title =		 {Consistency of spectral clustering},
  journal =		 {Annals of Statistics},
  year =		 2008,
  volume =		 36,
  pages =		 {555-586},
  annote =	 { The paper discussed the consistency of spectral
                  clustering. Two forms of spectral clustering are
                  considered: (1) Those that employ the combinatorial
                  Laplacian $L$, (2) Those that employ the normalized
                  Laplacian $\mathcal{L}$. The paper show that for
                  both forms of spectral clustering, under certain
                  criteria, the eigenvectors converges to the
                  eigenfunctions of the corresponding operators. The
                  criteria for the convergence in the case of the
                  normalized Laplacian is less restrictive than the
                  criteria in the case of the combinatorial Laplacian.
                  }
}
  


@Article{belkin08:_towar_theor_found_laplac,
  author =		 {M. Belkin and P. Niyogi},
  title =		 {Towards a Theoretical Foundation for
                  {L}aplacian-based manifold methods},
  journal =		 {Journal of Computer and System Sciences},
  year =		 2008,
  volume =		 74,
  pages =		 {1289-1308}
}

@TechReport{bernstein00:_graph,
  author =		 {M. Bernstein and V. de Silva and J. C. Langford and
                  J. B. Tenenbaum},
  title =		 {Graph approximations to geodescis on embedded
                  manifolds},
  institution =	 {Stanford University},
  year =		 2000,
  annote =	 { The technical report discussed the convergence of
                  the shortest path distances as used in Isomap to the
                  geodesic distance on a manifold. }
}
@InProceedings{hein05:_from,
  author =		 {M. Hein and J. Y. Audibert and U. von Luxburg},
  title =		 {From graphs to manifolds: weak and strong pointwise
                  consistency of graph {L}aplacians},
  booktitle =	 {Proceedinngs of the 18$^{\textrm{th}}$ conference on
                  learning theory,},
  year =		 2005,
  annote =		 { }
}

@Article{fiedler73:_algeb,
  author = 		 {M. Fiedler},
  title = 		 {Algebraic connectivity of graphs},
  journal = 	 {Czechoslovak Mathematical Journal},
  year = 		 1973,
  volume = 	 23,
  pages = 	 {298-305}}

@InBook{joly94:_class_dissim_analy,
  author =		 {S. Joly and G. Le {C}alve},
  title =		 {Classification and Dissimilarity Analysis},
  chapter =		 {Similarity functions},
  publisher =	 {Springer},
  year =		 1994,
  volume =		 93,
  series =		 {Lecture Notes in Statistics},
}

@InBook{mohar91:_graph,
  author =		 {B. Mohar},
  title =		 {Graph theory, combinatorics, and
                  applications. VOl. 2},
  chapter =		 {The {L}aplacian spectrum of graphs},
  publisher =	 {Addison Wiley},
  year =		 1991
}

@InProceedings{m.01:_random_walks_view_spect_segmen,
  author =		 {M. Melia and J. Shi},
  title =		 {A Random Walks View of Spectral Segmentation},
  booktitle =	 {8th International Workshop on Artificial
                  Intelligence and Statistics},
  year =		 2001,
  annote =		 { The paper discussed spectral clustering using the
                  probability transition matrix $P$. The paper show
                  that the resulting clusters satisfied a relaxed
                  version of the normalized cut criterion.  }
}

@Article{lafon06:_diffus,
  author =		 {S. Lafon and A. B. Lee},
  title =		 {Diffusion maps and coarse-graining: {A} unified
                  framework for dimensionality reduction, graph
                  partitioning and data set parameterization},
  journal =		 {{IEEE} transactions on pattern analysis and machine
                  intelligence},
  year =		 2006,
  volume =		 28,
  pages =		 {1393-1203},
  annote =	 { The paper attempt to illustrate the notion that
                  nonlinear dimensionality reduction, clustering, and
                  data set parameterization can be solved within the
                  same framework. What the paper meant by data set
                  parameterization and nonlinear dimensionality
                  reduction is the notion of nonlinear dimensionality
                  reduction being performed so that the pairwise
                  Euclidean distances of the embedding is an
                  approximation of a predefined notion of distances,
                  namely diffusion distances. The paper also
                  introduced a clustering algorithm on graphs similar
                  to $k$-means, with the centroid of each clusters
                  corresponding to a "diffusion center" on the
                  graph. }
}

@InProceedings{kondor02:_diffus,
  author =		 {R. I. Kondor and J. Lafferty},
  title =		 {Diffusion kernels on graphs and other discrete input
                  spaces},
  booktitle =	 {Proceedings of the nineteenth International
                  Conference on Machine Learning},
  year =		 2002,
  annote =	 { The paper proposed a method of constructing natural
                  families of kernels over discrete structures. The
                  kernels are constructed by using the matrix exponent
                  of some matrix. The paper focus on a particular kind
                  of kernels for graphs, those that arise by taking
                  the matrix exponential of the combiatorial Laplacian
                  of a graph.}
}

@Article{zhu03:_semi_super_learn_using_gauss,
  author =		 {X. Zhu and Z. Ghahramani and J. Lafferty},
  title =		 {Semi-Supervised Learning Using {G}aussian Fields and
                  Harmonic Functions},
  journal =		 {International Conference on Machine Learning},
  year =		 2003,
  annote =		 { The paper proposed an approach to semi-supervised
                  learning that's based on a Gaussian random field
                  model. The labeled and unlabeled data are
                  represented as a weighted graph. Some
                  interpretations and connections of the learning
                  algorithms with harmonic functions on graphs, graphs
                  kernels, and spectral clustering is noted. }
}


@Article{chen09:_simil_class,
  author =		 {Y. Chen and E. K. Garcia and M. R. Gupta and
                  A. Rahimi and L. Cazzanti},
  title =		 {Similarity-based Classification: Concepts and
                  Algorithms },
  journal =		 {Journal of Machine Learning Research},
  year =		 2009,
  volume =		 10,
  pages =		 {747-776}
}

@Article{chen07:_resis_laplac,
  author =		 {H. Chen and F. Zhang},
  title =		 {Resistance distance and the normalized Laplacian
                  spectrum},
  journal =		 {Discrete applied mathematics},
  year =		 2007,
  volume =		 155,
  pages =		 {654-661},
  annote =	 { The paper study expected commute time in terms of
                  the normalized Laplacian. Specifically, the paper
                  introduced the notion of a modified Kirchoff index
                  $K'$ based on the normalized Laplacian and showed
                  that one can bound $K'$ from below by using the
                  Kirchoff index that's based on the combinatorial
                  Laplacian.  }
}

@Article{lafon06:_data_fusion_multic_data_match_diffus_maps,
  author =		 {S. Lafon and Y. Keller and R. R. Coifman},
  title =		 {Data Fusion and Multicue Data Matching by Diffusion
                  Maps},
  journal =		 {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  year =		 2006,
  volume =		 28,
  pages =		 {1784-1797}
}


@Article{chapelle03:_clust_kernel_semi_super_learn,
  author =		 {O. Chapelle and J. Weston and B. Sch\"{o}lkopf},
  title =		 {Cluster Kernels for Semi-Supervised Learning},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2003,
  volume =		 15,
  pages =		 {585-592}
}

@Article{donoho03:_hesian,
  author =		 {D. L. Donoho and C. Grimes},
  title =		 {Hesian eigenmaps: Locally linear embedding
                  techniques for high-dimensional data},
  journal =		 {Proceedings of the National Academy of Sciences},
  year =		 2003,
  volume =		 100,
  pages =		 {5591-5596}
}


@Article{kirkland97:_distan_weigh_trees_group_inver_laplac_matric,
  author =		 {S. J. Kirkland and M. Neumann and B. L. Shader},
  title =		 {Distances in Weighted Trees and Group Inverse of
                  Laplacian Matrices},
  journal =		 {{SIAM} Journal on Matrix Analysis and Applications},
  year =		 1997,
  volume =		 18,
  pages =		 {827-841}
}

@Article{chung05:_laplac_cheeg,
  author =		 {F. Chung},
  title =		 {Laplacians and the {C}heeger inequality for directed
                  graphs},
  journal =		 {Annals of Combinatorics},
  year =		 2005,
  volume =		 9,
  pages =		 {1-19}
}

@Article{c.75:_role_group_gener_inver_markov,
  author =		 {C. D. Meyer, Jr.},
  title =		 {The Role of the Group Generalized Inverse in the
                  theory of finite {M}arkov chain},
  journal =		 {SIAM Review},
  year =		 1975,
  volume =		 17,
  pages =		 {443-464}
}

@TechReport{luxburg07:_tutor_spect_clust,
  author =		 {U. Von Luxburg},
  title =		 {A Tutorial on Spectral Clustering},
  institution =	 {Max Planck Institute for Biological Cybernetics},
  year =		 2007,
  annote =	 { The technical report presented several variants of
                  spectral clustering algorithms from the view point
                  of different type of graph Laplacians. The technical
                  report showed the various motivation behind the
                  spectral clustering algorithms, such as graph cuts,
                  random walks, and pertubation of matrices. }
}

@Article{zhou04:_learn_label_unlab,
  author =		 {D. Zhou and B. Sch\"{o}lkopf},
  title =		 {Learning from Labeled and Unlabeled data using
                  random walks},
  journal =		 {Proceedings of the twenty sixth DAGM Sympsium},
  year =		 2004,
  pages =		 {237-244},
  annote =		 {The paper analyzed an algorithm that was previously
                  introduced by the authors. The algorithm is a
                  semi-supervised learning algorithm that attempts to
                  classify unlabeled points by assigning it the sign
                  of $(I - \alpha \mathcal{L})^{-1}y$ where
                  $\mathcal{L}$ is the normalized Laplacian, $\alpha
                  \in (0,1)$ and $y$ is the vector of labels, with
                  $y_i = 0$ if $x_i$ has no labels. The resulting
                  algorithm is related to the concept of normalized
                  commute time.  }
}


@Article{saerens04,
  author =		 {M. Saerens and F. Fouss and L. Yen and P. Dupont},
  title =		 {The principal components analysis of a graph and its
                  relationships to spectral clustering},
  journal =		 {Proceedings of the fifteenth {E}uropean conference
                  on machine learning},
  year =		 2004
}





@Unpublished{lyons:_probab_trees_networ,
  author = 		 {R. Lyons and Y. Peres},
  title = 		 {Probability on Trees and Networks},
  year =         2009, 
  note = 		 {\url{http://mypage.iu.edu/~rdlyons/prbtree/prbtree.html}}}



@Unpublished{chebotarev,
  author =		 {P. Chebotarev},
  title =		 {A new family of graph distances},
  year =		 {2008},
  note =		 {\url{http://arxiv.org/abs/0810.2717}}
}

@Book{levin09:_markov,
  author = 	 {D. A. Levin and Y. Peres and E. L. Wilmer},
  title = 		 {Markov chains and mixing times},
  publisher = 	 {American Mathematical Society},
  year = 		 2009}

@InProceedings{spielmand08:_graph,
  author =		 {D. A. Spielman and N. Srivastava},
  title =		 {Graph sparsification by effective resistances},
  booktitle =	 {Proceedings of the annual {ACM} symposium on Theory
                  of Computing},
  year =		 2008
}

@Article{ng02,
  author =		 {A. Ng and M. Jordan and Y. Weiss},
  title =		 {On spectral clustering: analysis and an algorithm},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2002,
  volume =		 14
}

@InProceedings{dhillon01:_co,
  author =		 {I. Dhillon},
  title =		 {Co-clustering documents and words using bipartite
                  spectral graph partitioning},
  booktitle =	 {Proceedings of the seventh ACM SIGKDD international
                  conference on Knowledge discovery and data mining},
  year =		 2001
}

@Article{hagen92:_new,
  author =		 {L. Hagen and A. Kahng},
  title =		 {New spectral methods for ratio cut partitioning and
                  clustering},
  journal =		 {IEEE Transactions on Computer Aided Design},
  year =		 1992,
  volume =		 11,
  pages =		 {1074-1085}
}



@Article{hendrickson95,
  author =		 {B. Hendrickson and R. Leland},
  title =		 {An improved spectral graph partitioning algorithm
                  for mapping parallel computations},
  journal =		 {SIAM Journal on Scientific Computing},
  year =		 1995,
  volume =		 16,
  pages =		 {452-469}
}


@Article{kirchhoff47:_uber_aufl_gleic_str,
  author =		 {G. Kirchhoff},
  title =		 {\"{U}ber die Aufl\"{o}sung der Gleichunge, auf
                  welche man bei der untersuchung der linearen
                  verteilung galvanischer Str\"{o}mer gef\"{u}hrt
                  wird},
  journal =		 {Ann. Phys. Chem},
  year =		 1847,
  volume =	 72,
  pages =	 {497-508}}

@TechReport{zhu05:_semi_super,
  author =		 {X. Zhu},
  title =		 {Semi-Supervised learning literature survey},
  institution =	 {University of Wisconsin-Madison},
  year =		 2005,
  number =		 1530
}

@Article{belkin06:_manif_regul,
  author =		 {M. Belkin and P. Niyogi and S. Sindhwani},
  title =		 {Manifold Regularization: A geometric framework for
                  learning from labeled and unlabeled examples},
  journal =		 {Journal of Machine Learning Research},
  year =		 2006,
  volume =		 7,
  pages =		 {2399-2434}
}

@Article{hein07:_conver_laplac,
  author =		 {M. Hein and J.-Y. Audibert and U. von Luxburg},
  title =		 {Convergence of graph Laplacians on random
                  neighbourhood graphs},
  journal =		 {Journal of Machine Learning Research},
  year =		 2007,
  volume =		 8,
  pages =		 {1325-1370}
}

@Article{bengio04:_out_lle_isomap_mds_eigen,
  author =		 {Y. Bengio and J. Paiement and P. Vincent},
  title =		 {Out-of-sample extensions for LLE, {I}somap, MDS,
                  Eigenmaps and spectral clustering},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2004,
  volume =		 16
}

@Article{tenenbaum98:_mappin,
  author =		 {J. B. Tenenbaum},
  title =		 {Mapping a manifold of perceptual observations},
  journal =		 {Advances in Neural Infomration Processing Systems},
  year =		 1998,
  volume =		 10
}

@Article{shi00:_normal,
  author =		 {J. Shi and J. Malik},
  title =		 {Normalized cuts and image segmentation},
  journal =		 {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  year =		 2000,
  volume =		 22,
  pages =		 {888-905}
}

@InProceedings{melia01,
  author =		 {M. Melia and J. Shi},
  title =		 {A random walks view of spectral segmentation},
  booktitle =	 {$8$th International Workshop on Artificial
                  Intelligence and Statistics},
  year =		 2001
}

@Article{szummer01:_partial_markov,
  author = 		 {M. Szummer and T. Jaakkola},
  title = 		 {Partially labeled classification with Markov random walks},
  journal = 	 {Advances in Neural Information Processing System},
  year = 		 2001}

@Article{zhou04:_learn,
  author = 		 {D. Zhou and  T. N. Lal and J. Weston and B. Sch\"{o}lkopf},
  title = 		 {Learning with local and global consistency},
  journal = 	 {Advances in Neural Information Processing System},
  year = 		 2004}

@InProceedings{yen07:_graph,
  author =		 {L. Yen and F. Fouss and C. Decaestecker and
                  P. Francq and M. Saerens},
  title =		 {Graph nodes clustering based on the commute-time
                  kernel},
  booktitle =	 {Proceedings of the {P}acific-{A}sia conference on
                  knowledge discovery and data mining},
  year =		 2007
}

@Article{gersgorin31:_uber_abgren_eigen_matrix,
  author = 		 {S. Ger\u{s}gorin},
  title = 		 {\"{U}ber die Abgrenzung der Eigenwerte einer Matrix},
  journal = 	 {Izv. Akad. Nauk. USSR Otd. Fiz.-Mat},
  year = 		 1931,
  volume = 	 7,
  pages = 	 {749-754}}


@Book{horn90:_matrix_analy,
  author = 	 {R. A. Horn and C. R. Johnson},
  title = 		 {Matrix Analysis},
  publisher = 	 {Cambrdige University Press},
  year = 		 1990}

@Article{nadler06:_diffus,
  author =		 {B. Nadler and S. Lafon and R. R. Coifman and
                  I. G. Kevrekidis},
  title =		 {Diffusion maps, spectral clustering and reaction
                  coordinates of dynamical systems},
  journal =		 {Applied and Computational Harmonic Analysis},
  year =		 2006,
  volume =		 21,
  pages =		 {113-127}
}



@Article{nadler05:_diffus_eigen_fokker_planc_operat,
  author =		 {B. Nadler and S. Lafon and R. R. Coifman and
                  I. G. Kevrekidis},
  title =		 {Diffusion maps, spectral clustering and
                  Eigenfunctions of {F}okker-{P}lanck Operators},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2005,
  volume =		 18
}


@Article{maier09:_optim_k,
  author =		 {M. Maier and M. Hein and U. von {L}uxburg},
  title =		 {Optimal construction of $K$-nearest neighbour graphs
                  for identifying noisy clusters},
  journal =		 {Theoretical computer Science},
  year =		 2009,
  volume =		 410,
  pages =		 {1749-1764}
}


@Article{maier08:_influen,
  author =		 {M. Maier and U. von {L}uxburg and M. Hein},
  title =		 {Influence of graph construction on graph-based
                  clustering measures},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2008,
  volume =		 21
}


@InProceedings{smola03:_kernel,
  author =		 {A. Smola and R. Kondor},
  title =		 {Kernels and regularization on graphs},
  booktitle =	 {Conference on Learning Theory},
  year =		 2003
}


@InBook{zhu05:_semi,
  author =		 {X. Zhu and J. Kandola and J. Lafferty and
                  Z. Ghahramani},
  title =		 {Semi-supervised learning},
  chapter =		 {Graph Kernels by Spectral Transforms},
  publisher =	 {MIT Press},
  year =		 2005
}



@Article{wouters03:_graph,
  author =		 {L. Wouters and H. W. G\"{o}hlmann and L. Bijnens and
                  S. U. Kass and G. Molenberghs and P. J. Lewi},
  title =		 {Graphical exploration of gene expression data: a
                  comparative study of three multivariate methods},
  journal =		 {Biometrics},
  year =		 2003,
  volume =		 59,
  pages =		 {1131-1139}
}

@Article{chapelle02:_sclus,
  author =		 {O. Chapelle and J. Weston and B. Sch\"{o}lkopf},
  title =		 {Scluster kernels for semi-supervised learning},
  journal =		 {Advances in Neural Information Processing Systems},
  year =		 2002,
  volume =		 15
}


@InBook{saul06:_semis,
  author =		 {L. K. Saul and K. Q. Weinberger and J. H. Ham and
                  F. Sha and D. D. Lee},
  title =		 {Semisupervised learning},
  chapter =		 {Spectral methods for dimensionality reduction},
  publisher =	 {MIT Press},
  year =		 2006
}

