\chapter{Graph metrics and dimensionality reduction}
\label{cha:graph-metr-dimens}
We have seen in \S \ref{cha:dist-undir-graphs} and \S
\ref{cha:dist-direct-graphs} several notion of graph metrics. As we
have mentioned previously, for example in \S
\ref{sec:some-thoughts-isomap}, several manifold learning algorithms
can be viewed as embedding a graph using some proximities measure on
the graph. The aim of this chapter is to expound on this point of
view. For the case where the graphs are undirected, we will see that
there might exists different plausible embeddings of the same graph
metrics. For example, one can embed expected commute time on
undirected graphs either by classical MDS, or by using the system of
eigenvalues and eigenvectors of the probability transition matrix.
The situation is slightly different for the case of directed graphs,
where classical MDS seems to be the only suitable approach. This lead
us to propose the view that the main difference between Isomap,
Laplacian eigenmaps, and diffusion maps is in the choice of
proximities measure between the vertices of the underlying graphs.

\section{Embedding expected commute time for undirected graphs}
\label{sec:embedd-expect-comm}
Let $G = (V,E,\omega)$ be an undirected graph. Suppose that
$\Delta_{\delta}$ is the matrix of expected commute time between the
vertices of $G$. We recall below the formula for $\Delta_{\delta}$ from \S
\ref{sec:expect-comm-time} 
\begin{equation}
  \label{eq:101}
  \Delta_{\delta} = \kappa(\mathbf{Z}\bm{\Pi}^{-1}) = \mathrm{Vol}(G)
  \kappa(\mathbf{L}^{\dagger})
\end{equation}
where $\mathbf{Z} = (\mathbf{I} - \mathbf{P} + \mathbf{Q})^{-1}$ and
$\mathbf{L}^{\dagger}$ is the Moore-Penrose pseudoinverse of the
combinatorial Laplacian $\mathbf{L}$.  \\ \\
%
%
\noindent 
The matrix $\Delta_{\delta}$ of expected commute time on $G$ can be
used to define embeddings of the vertices $V$ of $G$ into Euclidean
space. The first and most straightforward embedding is by classical
MDS using $\Delta_{\delta}$ as the squared dissimilarity matrix. Since
$\mathbf{L}^{\dagger}$ is double centered, $\tau(\Delta_{\delta}) =
\mathrm{Vol}(G)\tau(\kappa(\mathbf{L}^{\dagger})) = \mathrm{Vol}(G)
\mathbf{L}^{\dagger}$. If $\lambda_1 \geq \lambda_2 \geq \dots \geq
\lambda_N$ are the eigenvalues of $\mathbf{L}^{\dagger}$ and
$\bm{\nu}_1, \bm{\nu}_2, \dots, \bm{\nu}_N$ are the corresponding
eigenvectors, then the embedding of $v_i \in V$ into $\mathbb{R}^{d}$
using classical MDS is identical to 
\begin{equation}
  \label{eq:98}
  v_i \mapsto \sqrt{\mathrm{Vol}(G)} 
\Bigl(\sqrt{\lambda}_1 \bm{\nu}_1(i), \sqrt{\lambda_2}
  \bm{\nu}_2(i), \dots, \sqrt{\lambda}_d \bm{\nu}_d(i) \Bigr)
\end{equation}
Eq.~\eqref{eq:98} can also be written in terms of the eigenvalues of
$\mathbf{L}$. The eigenvectors of $\mathbf{L}^{\dagger}$ and
$\mathbf{L}$ coincide and the eigenvalues of
$\mathbf{L}^{\dagger}$ can be mapped to the eigenvalues of
$\mathbf{L}$ as
\begin{equation}
  \label{eq:99}
  h(\lambda_i) = \begin{cases}
    1/\lambda_i & \text{if $\lambda_i \not = 0$} \\
    0 & \text{if $\lambda_i = 0$}
    \end{cases}
\end{equation} \\ \\
%
%
\noindent Another embedding of $\Delta_\delta$ can be found by using
the eigenvalues and eigenvectors of $\mathbf{P}$. We know that
$\mathbf{P} = \bm{\Pi}^{-1}\mathbf{P}^{T}\bm{\Pi}$.
$\bm{\Pi}^{1/2}\mathbf{P}\bm{\Pi}^{-1/2}$ is thus symmetric.
$\bm{\Pi}^{1/2}(\mathbf{P} - \mathbf{Q})\bm{\Pi}^{-1/2}$ is therefore
also symmetric. Let $\mathbf{U}\bm{\Sigma}\mathbf{U}^{T}$ be the
spectral decomposition of $\bm{\Pi}^{1/2}(\mathbf{P} -
\mathbf{Q})\bm{\Pi}^{-1/2}$. Then $\bm{\Pi}^{1/2}(\mathbf{I} -
\mathbf{P} + \mathbf{Q})^{-1}\bm{\Pi}^{-1/2} = \mathbf{U}(\mathbf{I} -
\bm{\Sigma})^{-1}\mathbf{U}^{T}$. We thus have
\begin{equation}
  \label{eq:105}
  \begin{split}
  \mathbf{Z}\bm{\Pi}^{-1} &=  (\mathbf{I} - \mathbf{P} +
  \mathbf{Q})^{-1}\bm{\Pi}^{-1} \\ 
  &= \bm{\Pi}^{-1/2} \mathbf{U}(\mathbf{I} -
  \bm{\Sigma})^{-1}\mathbf{U}^{T}\bm{\Pi}^{-1/2} 
  \end{split}
\end{equation}
Since $(\mathbf{P} - \mathbf{Q})$ is similar to
$\bm{\Pi}^{1/2}(\mathbf{P} - \mathbf{Q})\bm{\Pi}^{-1/2}$, we have
that $\bm{\Pi}^{-1/2}\mathbf{U}$ is the matrix of (right) eigenvectors of
$\mathbf{P} - \mathbf{Q}$. Furthermore, the eigenvectors of
$\mathbf{P}$ are also the eigenvectors of $\mathbf{P} - \mathbf{Q}$. 
The embedding of $V$ into $\mathbb{R}^{d}$ using the eigenvalues and
eigenvectors of $\mathbf{P}$ is then given as
\begin{equation}
  \label{eq:104}
  v_i \mapsto \Bigl( \frac{1}{\sqrt{1 - \lambda_2}} \mathbf{f}_2(i),
    \frac{1}{\sqrt{1 - \lambda_3}}\mathbf{f}_3(i), \dots, \frac{1}{\sqrt{1 -
          \lambda_{d+1}}} \mathbf{f}_{d+1}(i) \Bigr)
\end{equation}
where $\lambda_1 = 1 \geq \lambda_2 \geq \dots \geq \lambda_N$ are the
eigenvalues of $\mathbf{P}$. The embedding as given by
Eq.~\eqref{eq:104} is therefore an anisotropic scaling of the
Laplacian eigenmaps as given by Eq.~\eqref{eq:92} in \S
\ref{sec:laplacian-eigenmaps}. Now, $\mathbf{f}$ is an eigenvector of
$\mathbf{P}$ with eigenvalue $\lambda$ implies that
$\bm{\Pi}^{1/2}\mathbf{f}$ is an eigenvector of the normalized
Laplacian $\bm{\mathcal{L}}$ with eigenvalue $1 - \lambda$. We can
thus also view Eq.~\eqref{eq:104} as embedding of $\Delta_\delta$
using the eigenvalues and \emph{scaled} eigenvectors of
$\bm{\mathcal{L}}$. Note that this is not equivalent to
embedding using the eigenvalues and eigenvectors of
$\bm{\mathcal{L}}$, i.e., the embedding given by
\begin{equation}
  \label{eq:102}
  v_i \mapsto \Bigl( \frac{1}{\sqrt{\lambda_2}} \mathbf{g}_2(i),
  \frac{1}{\sqrt{\lambda_3}}\mathbf{g}_3(i), \dots, \frac{1}{\sqrt{
      \lambda_{d+1}}} \mathbf{g}_{d+1}(i) \Bigr)
\end{equation}
where $0 = \lambda_1 \leq \lambda_2 \leq \dots \leq \lambda_{n-1}$ and
$\bm{g}_1, \bm{g}_2, \dots, \bm{g}_{n-1}$ are the eigenvalues and
corresponding eigenvectors of $\bm{\mathcal{L}}$, is not an embedding
of $\Delta_{\delta}$ into $\mathbb{R}^{d}$. However, this embedding is
also shown to be useful in the context of spectral clustering. The
spectral clustering algorithm of \citet{ng02} embed the data points
using the eigenvectors of $\bm{\mathcal{L}}$ and the embedded data
points are then clustered using the K-means algorithm. \citet{ng02}
showed that, under some assumptions regarding the data points, such an
algorithm managed to find a meaningful clusters representation of the
data points.
\\ \\
%
%
\noindent
We comment briefly on the embeddings as given by Eq.~\eqref{eq:98} and
Eq.~\eqref{eq:104}. Eq.~\eqref{eq:98} embeds $\Delta_{\delta}$ using
the eigenvalues and eigenvectors of $\mathbf{L}$ while
Eq.~\eqref{eq:104} embeds using the eigenvalues and eigenvectors of
$\mathbf{P}$. The two embeddings as given by Eq.~\eqref{eq:98} and
Eq.~\eqref{eq:104} are not equivalent. The eigenvalues and
eigenvectors of $\mathbf{L}$ is not related to the eigenvalues and
eigenvectors of $\mathbf{P}$. Thus, the claim in \citet{saul06:_semis}
that the Laplacian eigenmaps of Eq.~\eqref{eq:92} is given by MDS
using expected commute times is inaccurate. Note that, in constrast to
the eigenvectors of $\mathbf{L}$, the eigenvectors of $\mathbf{P}$ are
not orthogonal. Furthermore, the $d$-dimensional embedding as given by
Eq.~\eqref{eq:98} is the best $d$-dimensional embedding of
$\Delta_{\delta}$ with respect to the STRAIN criterion
(Eq.~\eqref{eq:87}) and thus it's expected that the embedding as given
by Eq.~\eqref{eq:98} explains the variance of the data points better
than the embedding as given by Eq.~\eqref{eq:104}. However, since the
$d$-dimensional embedding as given by Eq.~\eqref{eq:98} is not
necessarily the best $d$-dimensional embedding of $\Delta_{\delta}$
with respect to the STRESS criterion, it's not guaranteed that the
embedding as given by Eq.~\eqref{eq:98} is a better approximation to
$\Delta_\delta$ than the embedding given by Eq.~\eqref{eq:104}.

\section{Embedding diffusion distances for undirected graphs}
\label{sec:embedd-diff-dist}
Let $G = (V,E,\omega)$ be an undirected graph. Let
$\Delta_{\rho_{t}^{2}}$ be the matrix of squared diffusion distances
between the vertices of $G$. From Proposition \ref{prop:12},
$\Delta_{\rho_{t}^{2}} = \kappa(\mathbf{P}^{2t} \bm{\Pi}^{-1})$. The
matrix $\Delta_{\rho_{t}^{2}}$ of diffusion distances on $G$ can then
be used to define embeddings of the vertices $V$ of $G$ into Euclidean
space. The first embedding is by classical MDS using
$\Delta_{\rho_{t}^{2}}$. However, contrary to the case of expected
commute time, the classical MDS embedding doesn't seem to correspond to
eigenvalues and eigenvectors of either the Laplacian matrices or the
probability transition matrix. \\ \\
%
%
\noindent
We can also embed $\Delta_{\rho_{t}^{2}}$ using the eigenvalues and
eigenvectors of the probability transition matrix. This is the
diffusion maps of \citet{coifman06:_diffus_maps}. Similar to our
discussion of the embedding of expected commute time into Euclidean
space in \S \ref{sec:embedd-expect-comm}, let
$\mathbf{U}\bm{\Sigma}\mathbf{U}^{T}$ be the spectral decomposition of
$\bm{\Pi}^{1/2}\mathbf{P}\bm{\Pi}^{-1/2}$. Then
$\mathbf{P}^{2t}\bm{\Pi}^{-1} =
\bm{\Pi}^{-1/2}\mathbf{U}\bm{\Sigma}^{2t}\mathbf{U}^{T}\bm{\Pi}^{-1/2}$
and the diffusion maps of \citet{coifman06:_diffus_maps} is given by
\begin{equation}
  \label{eq:106}
  v_i \mapsto (\lambda_{2}^{t} \mathbf{f}_{2}(i), \lambda_{3}^{t}
  \mathbf{f}_{3}(i), \dots, \lambda_{d+1}^{t} \mathbf{f}_{d+1}(i))
\end{equation}
where $\lambda_1 = 1 \geq |\lambda_2| \geq |\lambda_3| \geq
|\lambda_{N}|$ are the eigenvalues of $\mathbf{P}$ in non-increasing
order of modulus and $\mathbf{f}_i$ are the corresponding
eigenvectors. Comparing Eq.~\eqref{eq:106}, Eq.~\eqref{eq:104} and
Eq.~\eqref{eq:92} one see that diffusion maps is an anisotropic
scaling of Laplacian eigenmaps. The following proposition is a
restatement of the above observations.
\begin{proposition}
  \label{prop:22}
  Let $G = (V,E,\omega)$ be an undirected graph and $\mathbf{P}$ be
  its transition matrix. $\Delta_{\rho_{t}^{2}}$ defines an embedding
  of the vertices of $G$ into $\mathbb{R}^{d}$ by
  \begin{equation*}
    v_i \mapsto (\lambda_{2}^{t} \mathbf{f}_{2}(i), \lambda_{3}^{t}
    \mathbf{f}_{3}(i), \dots, \lambda_{d+1}^{t} \mathbf{f}_{d+1}(i))
  \end{equation*}
  where $\lambda_1 = 1 \geq |\lambda_2| \geq \dots \geq |\lambda_N|$
  are the eigenvalues of $\mathbf{P}$
  and $\mathbf{f}_{i}$ are the corresponding eigenvectors. The above
  embedding is an anistropic scaling of Laplacian eigenmaps. It's also an
  anisotropic scaling of the embedding using the
  expected commute time $\Delta_{\delta}$ of Eq.~\eqref{eq:104}.
\end{proposition}
\subfiglabelskip = 0pt
\begin{figure}[htbp]
  \centering
  \subfigure[][]{
    \label{fig:embed1-a}
    \includegraphics[width=55mm]{graphics/wellsdata.pdf}
    }
    \hspace{8pt}
    \subfigure[][]{
      \label{fig:embed1-b}
      \includegraphics[width=55mm]{graphics/resistance.pdf}
      }
      \subfigure[][]{
        \label{fig:embed1-c}
        \includegraphics[width=55mm]{graphics/wells1.pdf}
        }
      \subfigure[][]{
        \label{fig:embed1-d}
        \includegraphics[width=55mm]{graphics/wells10.pdf}
        }
        \caption{Embedding and clustering of an artificial data set
          with three clusters \subref{fig:embed1-a} using diffusion
          distances and expected commute time. \subref{fig:embed1-b}
          gave the embedding of the data point using expected
          commute. The data points are also colored according to the
          clusters formed by hierarichal clustering using expected
          commute time. In \subref{fig:embed1-c} and
          \subref{fig:embed1-d}, the data points were embedded using
          diffusion distance at time scale $t = 1$ and $t = 10$,
          respectively, through Eq.~\eqref{eq:106}. Once again, the
          data points are also colored according to the clusters
          formed by hierarichal clustering.}
  \label{fig:embed1}
\end{figure}
As an illustration of the above observation, we consider a data set
like the one in Figure \ref{fig:embed1-a}. Figure \ref{fig:embed1-b}
gives the embedding of the data points into two dimension using
expected commute time. The embedding was done through the system of
eigenvalues and eigenvectors of the probability transition matrix
$\mathbf{P}$ as in Eq.~\eqref{eq:104}. The data points were also
clustered by hierarichal clustering using expected commute time. We
see that the two dimensional embedding is consistent with the
hierarichal clustering in Figure \ref{fig:embed1-b}. Figure
\ref{fig:embed1-c} and Figure \ref{fig:embed1-d} give the embeddings of
the data points using diffusion distances at time $t = 1$ and $t =
10$, respectively. The embeddings were both done through the system of
eigenvalues and eigenvectors of $\mathbf{P}$ as in
Eq.~\eqref{eq:106}. The data points were also clustered by hierarichal
clustering using diffusion distances. The clusters in Figure
\ref{fig:embed1-d} seem more pronounced than those in Figure
\ref{fig:embed1-c}. This is most likely due to the fact that at time
scale $t = 1$, diffusion distances between the data points are more
tightly concentrated. Note also that the two dimensional embeddings
are very similar to each other. We suppose that this is because for a
small number of dimensions, the scaling matrices between the
embeddings are very close to being isotropic scaling matrices. The
embeddings using classical MDS might not have such a
phenomenon. \\ \\
%
%
\begin{figure}[htbp]
  \centering
  \subfigure[][]{
    \label{fig:embed2-a}
    \includegraphics[width=55mm]{graphics/twosteps_data.pdf}
    }
    \hspace{8pt}
    \subfigure[][]{
      \label{fig:embed2-b}
      \includegraphics[width=55mm]{graphics/twosteps_diffusion1.pdf}
      }
      \subfigure[][]{
        \label{fig:embed2-c}
        \includegraphics[width=55mm]{graphics/twosteps_diffusion2.pdf}
        }
        \caption{Embedding of an artificial data set
          \subref{fig:embed2-a} using diffusion distances. The data
          points are colored from left to right along the $x$
          axis. \subref{fig:embed2-b} gave the embedding of the data
          point using diffusion distances with Gaussian similarities
          and $\sigma^{2} = 0.002$. The points in the embedding are
          colored using their original color in
          \subref{fig:embed2-a}. \subref{fig:embed2-c} gave the
          embedding of the data point using diffusion distances and
          Gaussian similarities, this time with $\sigma^{2} = 0.01$. }
  \label{fig:embed2}
\end{figure}
We mentioned previously in \S \ref{sec:diffusion-distances} that
diffusion distances only take into account paths of even length. This
sometime leads to unexpected results. Consider for example the
contrived data set in Figure \ref{fig:embed2-a}. Let $\mathbf{W}_1$ be
the matrix of Gaussian similarities between the data points with
$\sigma^{2} = 0.002$ (see Eq.~\eqref{eq:100}) and $\mathbf{P}_1$ be the
resulting probability transion matrix. $\mathbf{W}_1$ is constructed
so that each row of $\mathbf{P}_1$ have at most two non-diagonal
entries that are significantly different from $0$. Let $\Delta_{1}$ be
the matrix of diffusion distance at time $t = 5$ with $\mathbf{P}_1$
as the transition matrix. Figure \ref{fig:embed2-b} gives the two
dimensional embedding of $\Delta_{1}$. The embedding was done through
the system of eigenvalues and eigenvectors of $\mathbf{P}_1$. Let
$\mathbf{W}_2$ be the matrix of Gaussian similarities between the data
points, this time with $\sigma^{2} = 0.01$, and $\mathbf{P}_2$ be the
resulting probability transition matrix. Each row of $\mathbf{P}_2$
now contains a sizable number of entries that are significantly
different from $0$. Let $\Delta_{2}$ be the matrix of diffusion
distance at time $t = 5$ with $\mathbf{P}_2$ as the transition
matrix. Figure \ref{fig:embed2-c} gives the two dimensional embedding
of $\Delta_{2}$. The embedding was also done using the system of
eigenvalues and eigenvectors of $\mathbf{P}_2$. In Figure
\ref{fig:embed2-b}, we see that the (almost) sparseness of
$\mathbf{P}_1$ leads to data points that are adjacent in the ambient
space being embedded into different sides of the embedding. The
situation is much less severe in Figure \ref{fig:embed2-c}. However,
since the data points are now embedded on a curve, the distances
between some of the cyan and black data points in the embedded space
is now smaller than the distances between some of the cyan and green/red
data points. It's slightly amusing that sometime figures similar to
Figure \ref{fig:embed2-c} are used as an illustration of the
usefulness of non-linear dimensionality reduction. In that sense,
diffusion maps had performed a non-linear transformation
of the linear data in Figure \ref{fig:embed2-a}.
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "dissertation.tex"
%%% End: 
