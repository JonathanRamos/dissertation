\chapter{Graph metrics and dimensionality reduction}
\label{cha:graph-metr-dimens}
We have seen in \S \ref{cha:dist-undir-graphs} and \S
\ref{cha:dist-direct-graphs} several notion of graph metrics. As we
have mentioned previously, for example in \S
\ref{sec:some-thoughts-isomap}, several manifold learning algorithms
can be viewed as embedding a graph using some proximities measure on
the graph. The aim of this chapter is to expound on this point of
view. For the case where the graphs are undirected, we will see that
there might exists different plausible embeddings of the same graph
metrics. For example, one can embed expected commute time on
undirected graphs either by classical MDS, or by using the system of
eigenvalues and eigenvectors of the probability transition matrix.
The situation is slightly different for the case of directed graphs,
where classical MDS seems to be the only suitable approach. This lead
us to propose the view that the main difference between Isomap,
Laplacian eigenmaps, and diffusion maps is in the choice of
proximities measure between the vertices of the underlying graphs.

\section{Embedding expected commute time for undirected graphs}
\label{sec:embedd-expect-comm}
Let $G = (V,E,\omega)$ be an undirected graph. Suppose that
$\Delta_{\delta}$ is the matrix of expected commute time between the
vertices of $G$. We recall below the formula for $\Delta_{\delta}$ from \S
\ref{sec:expect-comm-time} 
\begin{equation}
  \label{eq:101}
  \Delta_{\delta} = \kappa(\mathbf{Z}\bm{\Pi}^{-1}) = \mathrm{Vol}(G)
  \kappa(\mathbf{L}^{\dagger})
\end{equation}
where $\mathbf{Z} = (\mathbf{I} - \mathbf{P} + \mathbf{Q})^{-1}$ and
$\mathbf{L}^{\dagger}$ is the Moore-Penrose pseudoinverse of the
combinatorial Laplacian $\mathbf{L}$.  \\ \\
%
%
\noindent 
The matrix $\Delta_{\delta}$ of expected commute time on $G$ can be
used to define embeddings of the vertices $V$ of $G$ into Euclidean
space. The first and most straightforward embedding is by classical
MDS using $\Delta_{\delta}$ as the squared dissimilarity matrix. Since
$\mathbf{L}^{\dagger}$ is double centered, $\tau(\Delta_{\delta}) =
\mathrm{Vol}(G)\tau(\kappa(\mathbf{L}^{\dagger})) = \mathrm{Vol}(G)
\mathbf{L}^{\dagger}$. If $\lambda_1 \geq \lambda_2 \geq \dots \geq
\lambda_N$ are the eigenvalues of $\mathbf{L}^{\dagger}$ and
$\bm{\nu}_1, \bm{\nu}_2, \dots, \bm{\nu}_N$ are the corresponding
eigenvectors, then the embedding of $v_i \in V$ into $\mathbb{R}^{d}$
using classical MDS is identical to 
\begin{equation}
  \label{eq:98}
  v_i \mapsto \sqrt{\mathrm{Vol}(G)} 
\Bigl(\sqrt{\lambda}_1 \bm{\nu}_1(i), \sqrt{\lambda_2}
  \bm{\nu}_2(i), \dots, \sqrt{\lambda}_d \bm{\nu}_d(i) \Bigr)
\end{equation}
Eq.~\eqref{eq:98} can also be written in terms of the eigenvalues of
$\mathbf{L}$. The eigenvectors of $\mathbf{L}^{\dagger}$ and
$\mathbf{L}$ coincide and the eigenvalues of
$\mathbf{L}^{\dagger}$ can be mapped to the eigenvalues of
$\mathbf{L}$ as
\begin{equation}
  \label{eq:99}
  h(\lambda_i) = \begin{cases}
    1/\lambda_i & \text{if $\lambda_i \not = 0$} \\
    0 & \text{if $\lambda_i = 0$}
    \end{cases}
\end{equation} \\ \\
%
%
\noindent Another embedding of $\Delta_\delta$ can be found by using
the eigenvalues and eigenvectors of $\mathbf{P}$. We know that
$\mathbf{P} = \bm{\Pi}^{-1}\mathbf{P}^{T}\bm{\Pi}$.
$\bm{\Pi}^{1/2}\mathbf{P}\bm{\Pi}^{-1/2}$ is thus symmetric.
$\bm{\Pi}^{1/2}(\mathbf{P} - \mathbf{Q})\bm{\Pi}^{-1/2}$ is therefore
also symmetric. Let $\mathbf{U}\bm{\Sigma}\mathbf{U}^{T}$ be the
spectral decomposition of $\bm{\Pi}^{1/2}(\mathbf{P} -
\mathbf{Q})\bm{\Pi}^{-1/2}$. Then $\bm{\Pi}^{1/2}(\mathbf{I} -
\mathbf{P} + \mathbf{Q})^{-1}\bm{\Pi}^{-1/2} = \mathbf{U}(\mathbf{I} -
\bm{\Sigma})^{-1}\mathbf{U}^{T}$. We thus have
\begin{equation}
  \label{eq:105}
  \begin{split}
  \mathbf{Z}\bm{\Pi}^{-1} &=  (\mathbf{I} - \mathbf{P} +
  \mathbf{Q})^{-1}\bm{\Pi}^{-1} \\ 
  &= \bm{\Pi}^{-1/2} \mathbf{U}(\mathbf{I} -
  \bm{\Sigma})^{-1}\mathbf{U}^{T}\bm{\Pi}^{-1/2} 
  \end{split}
\end{equation}
Since $(\mathbf{P} - \mathbf{Q})$ is similar to
$\bm{\Pi}^{1/2}(\mathbf{P} - \mathbf{Q})\bm{\Pi}^{-1/2}$, we have
that $\bm{\Pi}^{-1/2}\mathbf{U}$ is the matrix of (right) eigenvectors of
$\mathbf{P} - \mathbf{Q}$. Furthermore, the eigenvectors of
$\mathbf{P}$ are also the eigenvectors of $\mathbf{P} - \mathbf{Q}$. 
The embedding of $V$ into $\mathbb{R}^{d}$ using the eigenvalues and
eigenvectors of $\mathbf{P}$ is then given as
\begin{equation}
  \label{eq:104}
  v_i \mapsto \Bigl( \frac{1}{\sqrt{1 - \lambda_2}} \mathbf{f}_2(i),
    \frac{1}{\sqrt{1 - \lambda_3}}\mathbf{f}_3(i), \dots, \frac{1}{\sqrt{1 -
          \lambda_{d+1}}} \mathbf{f}_{d+1}(i) \Bigr)
\end{equation}
where $\lambda_1 = 1 \geq \lambda_2 \geq \dots \geq \lambda_N$ are the
eigenvalues of $\mathbf{P}$. The embedding as given by
Eq.~\eqref{eq:104} is therefore an anisotropic scaling of the
Laplacian eigenmaps as given by Eq.~\eqref{eq:92} in \S
\ref{sec:laplacian-eigenmaps}. Now, $\mathbf{f}$ is an eigenvector of
$\mathbf{P}$ with eigenvalue $\lambda$ implies that
$\bm{\Pi}^{1/2}\mathbf{f}$ is an eigenvector of the normalized
Laplacian $\bm{\mathcal{L}}$ with eigenvalue $1 - \lambda$. We can
thus also view Eq.~\eqref{eq:104} as embedding of $\Delta_\delta$
using the eigenvalues and \emph{scaled} eigenvectors of
$\bm{\mathcal{L}}$. Note that this is not equivalent to
embedding using the eigenvalues and eigenvectors of
$\bm{\mathcal{L}}$, i.e., the embedding given by
\begin{equation}
  \label{eq:102}
  v_i \mapsto \Bigl( \frac{1}{\sqrt{\lambda_2}} \mathbf{g}_2(i),
  \frac{1}{\sqrt{\lambda_3}}\mathbf{g}_3(i), \dots, \frac{1}{\sqrt{
      \lambda_{d+1}}} \mathbf{g}_{d+1}(i) \Bigr)
\end{equation}
where $0 = \lambda_1 \leq \lambda_2 \leq \dots \leq \lambda_{n-1}$ and
$\bm{g}_1, \bm{g}_2, \dots, \bm{g}_{n-1}$ are the eigenvalues and
corresponding eigenvectors of $\bm{\mathcal{L}}$, is not an embedding
of $\Delta_{\delta}$ into $\mathbb{R}^{d}$. However, this embedding is
also shown to be useful in the context of spectral clustering. The
spectral clustering algorithm of \citet{ng02} embed the data points
using the eigenvectors of $\bm{\mathcal{L}}$ and the embedded data
points are then clustered using the K-means algorithm. \citet{ng02}
showed that, under some assumptions regarding the data points, such an
algorithm managed to find a meaningful clusters representation of the
data points.
\\ \\
%
%
\noindent
We comment briefly on the embeddings as given by Eq.~\eqref{eq:98} and
Eq.~\eqref{eq:104}. Eq.~\eqref{eq:98} embeds $\Delta_{\delta}$ using
the eigenvalues and eigenvectors of $\mathbf{L}$ while
Eq.~\eqref{eq:104} embeds using the eigenvalues and eigenvectors of
$\mathbf{P}$. The two embeddings as given by Eq.~\eqref{eq:98} and
Eq.~\eqref{eq:104} are not equivalent. The eigenvalues and
eigenvectors of $\mathbf{L}$ is not related to the eigenvalues and
eigenvectors of $\mathbf{P}$. Thus, the claim in \citet{saul06:_semis}
that the Laplacian eigenmaps of Eq.~\eqref{eq:92} is given by MDS
using expected commute times is inaccurate. Note that, in constrast to
the eigenvectors of $\mathbf{L}$, the eigenvectors of $\mathbf{P}$ are
not orthogonal. Furthermore, the $d$-dimensional embedding as given by
Eq.~\eqref{eq:98} is the best $d$-dimensional embedding of
$\Delta_{\delta}$ with respect to the STRAIN criterion
(Eq.~\eqref{eq:87}) and thus it's expected that the embedding as given
by Eq.~\eqref{eq:98} explains the variance of the data points better
than the embedding as given by Eq.~\eqref{eq:104}. However, since the
$d$-dimensional embedding as given by Eq.~\eqref{eq:98} is not
necessarily the best $d$-dimensional embedding of $\Delta_{\delta}$
with respect to the STRESS criterion, it's not guaranteed that the
embedding as given by Eq.~\eqref{eq:98} is a better approximation to
$\Delta_\delta$ than the embedding given by Eq.~\eqref{eq:104}.

\section{Embedding diffusion distances for undirected graphs}
\label{sec:embedd-diff-dist}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "dissertation.tex"
%%% End: 
