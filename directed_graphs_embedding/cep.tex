\documentclass[professionalfonts,hyperref={pdfpagelabels=false,colorlinks=true,linkcolor=blue}]{beamer}

\mode<presentation>{
  \usetheme{Boadilla}
  \useinnertheme{rectangles}
}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{subfigure}
\usepackage{bm}
\usepackage{mathrsfs}
\bibliographystyle{plainnat}

\title[Directed Graphs and Embeddings]{Directed Graphs and Embeddings}
\author[Tang \& Trosset]{Minh Tang\inst{1} \and Michael
  Trosset\inst{2}}
\institute[Indiana University]{
  \inst{1} School of Informatics and Computing \\
  Indiana University, Bloomington
  \and \inst{2} Department of Statistics \\ Indiana University,
  Bloomington
}
\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \frametitle{Problem Description}
  Let $\bm{\Delta}$ be a (possibly asymmetric) dissimilarity
  matrix. Denote by $\mathbf{U}$ the upper diagonal matrix  of all
  ones, and $\mathbf{L} = \bm{1}\bm{1}^{T} - \mathbf{U}$, the lower
  diagonal matrices of all ones with $\mathrm{diag}(\mathbf{L}) = \bm{0}$. Let
  \begin{equation*}
  \bm{\Delta}_u = \bm{\Delta} \circ \mathbf{U} + \bm{\Delta}^{T}
  \circ \mathbf{L},
  \qquad  \bm{\Delta}_l = \bm{\Delta}^{T} \circ \mathbf{U} + \bm{\Delta} \circ
  \mathbf{L}.   
  \end{equation*}
  We consider the problem of finding a group embedding $\mathbf{G}$
  and projection matrices $\mathbf{P}_u$ and $\mathbf{P}_l$ such that
  \begin{enumerate}
  \item $\mathbf{G}\mathbf{P}_u$ gives a configuration of points that
    approximates $\bm{\Delta}_u$.
  \item $\mathbf{G}\mathbf{P}_l$ gives a configuration of points that
    approximates $\bm{\Delta}_l$. 
  \end{enumerate}
  Our approach is closely related to three-way MDS models and
  asymmetric MDS models.
\end{frame}
\begin{frame}
  \frametitle{Three-way MDS Models} Let $\mathscr{D} =
  \{\bm{\Delta}_{1}, \bm{\Delta}_{2}, \dots, \bm{\Delta}_{m}\}$ be a
  collection of (symmetric) dissimilarity matrices. We review 
  three-way MDS models for embedding $\mathscr{D}$.
  
  \vskip10pt{\bf Model 1:} Dimension weighting
  model (INDSCAL). Find a matrix $\mathbf{G}$ and
  non-negative diagonal matrices $\mathbf{W}_k$ for $1
  \leq k \leq m$ such that
  \begin{equation*}
    \sum_{k=1}^{m}{\|\mathbf{B}_k -
      \mathbf{G}\mathbf{W}_k^{2}\mathbf{G}^{T}\|^{2}}
  \end{equation*}
  is minimum, where $\mathbf{B}_k = \tau(\bm{\Delta}_{k})$.
  \vskip10pt The INDSCAL procedure
  \cite{carroll70:_ananl_n_eckar_young} solves the above optimization
  by iterating until convergence a sequence of updates of the form (1)
  Fix $\mathbf{G}$ and update $\mathbf{W}_k$; (2) Fix $\mathbf{W}_k$
  and update $\mathbf{G}$. Updating $\mathbf{G}$ for fixed
  $\mathbf{W}_k$ can be done in an analytical manner.
\end{frame}

\begin{frame}
  \frametitle{Three-way MDS Models (cont'd)} Let $\mathscr{D} =
  \{\bm{\Delta}_{1}, \bm{\Delta}_{2}, \dots, \bm{\Delta}_{m}\}$ be a
  collection of (symmetric) dissimilarity matrices. We review
  three-way MDS models for embedding $\mathscr{D}$.

  
  \vskip10pt{\bf Model 2:} Generalized Euclidean model (IDIOSCAL). Find a matrix
  $\mathbf{G}$ and general matrices $\mathbf{V}_k$ such that 
  \begin{equation*}
    \sum_{k=1}^{m}{\|\mathbf{B}_k -
      \mathbf{G}\mathbf{C}_k\mathbf{G}^{T}\|^{2}}
  \end{equation*}
  is minimum, where $\mathbf{B}_k = \tau(\bm{\Delta}_{k})$ and
  $\mathbf{C}_k = \mathbf{V}_{k} \mathbf{V}_k^{T}$. 
  \vskip10pt \cite{schonemann72} proposed to solve the above
  optimization problem by constraining that the $\mathbf{C}_k$ average
  to $\mathbf{I}$. Under this constraint, the optimization problem can
  be solved analytically provided that the $\bm{\Delta}_k$ are all
  Euclidean distance matrices. The INDSCAL and IDIOSCAL models
  can also be solved using SMACOF \cite{leeuw09:_multid}
  in R. 
\end{frame}
\begin{frame}
  \frametitle{Projections and Three-way MDS}
  Recall that $\bm{\Delta}_u = \bm{\Delta} \circ \mathbf{U} + \bm{\Delta}^{T}
  \circ \mathbf{L}$ and $\bm{\Delta}_l = \bm{\Delta}^{T} \circ \mathbf{U} + \bm{\Delta} \circ
  \mathbf{L}$ are our two dissimilarity matrices, formed by the upper
  diagonal entries and lower diagonal entries of $\bm{\Delta}$,
  respectively. 
  \vskip10pt We want to solve the following optimization problem.
  \begin{align*}
    \min_{\mathbf{G},\mathbf{P}_u,\mathbf{P}_l} & \quad \| \mathbf{B}_u -
    \mathbf{G}\mathbf{P}_u \mathbf{G}^{T} \|^2 + \| \mathbf{B}_{l} -
    \mathbf{G}\mathbf{P}_l \mathbf{G}^{T} \|^2 \\
    \text{subject to:} & \\
    & \quad \mathbf{P}_u \succeq 0, \quad \mathbf{P}_{u}^2 = \mathbf{P}_u,
    \quad \mathbf{P}_u \preceq \mathbf{I}, \quad
    \mathrm{rank}(\mathbf{P}_u) = k \tag{*} \\
    & \quad \mathbf{P}_l \succeq 0, \, \quad \mathbf{P}_{l}^2 = \mathbf{P}_l,
    \, \quad \mathbf{P}_l \preceq \mathbf{I}, \quad \mathrm{rank}(\mathbf{P}_l) = k\tag{**}
  \end{align*}
  This optimization problem is related to the INDSCAL and IDIOSCAL model but with
  a different set of constraints on the matrices $\mathbf{P}_u$ and $\mathbf{P}_l$.
\end{frame}
\begin{frame}
  \frametitle{Projections and Three-way MDS (cont'd)}
  \begin{align*}
    \min_{\mathbf{G},\mathbf{P}_u,\mathbf{P}_l} & \quad \| \mathbf{B}_u -
    \mathbf{G}\mathbf{P}_u \mathbf{G}^{T} \|^2 + \| \mathbf{B}_{l} -
    \mathbf{G}\mathbf{P}_l \mathbf{G}^{T} \|^2 \\
    \text{subject to:} & \\
    & \quad \mathbf{P}_u \succeq 0, \quad \mathbf{P}_{u}^2 = \mathbf{P}_u,
    \quad \mathbf{P}_u \preceq \mathbf{I}, \quad
    \mathrm{rank}(\mathbf{P}_u) = k\tag{*} \\
    & \quad \mathbf{P}_l \succeq 0, \, \quad \mathbf{P}_{l}^2 = \mathbf{P}_l,
    \, \quad \mathbf{P}_l \preceq \mathbf{I}, \quad
    \mathrm{rank}(\mathbf{P}_l) = k \tag{**}
  \end{align*}
  \begin{enumerate}
  \item If $\bm{\Delta}$ is symmetric, $\bm{\Delta}_l = \bm{\Delta}_u$
    and the solution of the optimization problem is identical to
    the solution for $\bm{\Delta}$ using classical MDS.
  \item The above optimization problem can be solved in a manner
    similar to the INDSCAL procedure, i.e., fix $\mathbf{G}$ and update
    $\mathbf{P}_u$ and $\mathbf{P}_l$ by SDP, fix $\mathbf{P}_{u}$ and
    $\mathbf{P}_l$ and update $\mathbf{G}$ by an analytic formula.
  \item An adhoc solution can also be found by solving for the
    embedding of $\bm{\Delta}_{u}$ using classical MDS and then solve
    for the projection matrix $\mathbf{P}_l$ for $\bm{\Delta}_l$ using SDP.
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{From Similarities to ??? on Directed Graphs}
  Consider a matrix $\mathbf{B}$ of the form
  \begin{equation*}
    \mathbf{B} =  (a_0
      \mathbf{I} + a_1 \mathbf{P} + a_2 \mathbf{P} + \cdots)\bm{\Pi}^{-1}
  \end{equation*}
  where $\mathbf{P}$ is a probability transition matrix.
  \begin{enumerate}
  \item $\mathbf{B}$ is, in some sense, a (asymmetric) similarity
    matrix. However, $\mathbf{B}(u,u) \geq \mathbf{B}(u,v)$ doesn't
    neccessarily holds. If, on the other hand, $\mathbf{P}$ generates
    a lazy Markov chain, i.e., $\mathbf{P}(u,u) \geq 1/2$ for all $u$,
    then $\mathbf{B}$ as defined is a similarity matrix.
  \item $\mathbf{B}$ could possible generate a Euclidean distance
    matrix $\bm{\Delta} = \kappa(\mathbf{B})$, provided that
    $\mathbf{B} + \mathbf{B}^{T} \succeq 0$. 
  \item If $\mathbf{P}$ is lazy, one could use $\mathbf{B}_U = \mathbf{B} \circ U +
    \mathbf{B}^{T} \circ L$ to generate $\bm{\Delta}_u = \kappa(\mathbf{B}_u)$ and similarly,
    use $\mathbf{B}_l = \mathbf{B}^{T} \circ U +
    \mathbf{B} \circ L$ to generate $\bm{\Delta}_l =
    \kappa(\mathbf{B}_l)$. $\bm{\Delta}_u$ and $\bm{\Delta}_l$ are
    then dissimilarity matrices.
  \end{enumerate}
\end{frame}

\bibliography{dissertation}
\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
