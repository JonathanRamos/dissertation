\chapter{Introduction}
\label{cha:introduction}

\section{Dimensionality reduction algorithms}
\label{sec:dimens-reduct-algor}
This section survey some of the existing literatures on dimensionality
reduction and manifold learning algorithms. Dimensionality reduction,
to be intentionally vague, is the reduction of the number of variables
describing the data points. Classic examples of dimensionality
reduction techniques are principal component analysis (PCA)
\citep{pearson01:_on,hotelling33:_analy} and multidimensional scaling
\citep{torgesen52:_multid}. These techniques are viewed as linear
dimensionality reduction because the reduced variables are obtained by
projections/linear transformations of the original set of
variables. If we believe that such linear transformations are not
suitable for the task at hand, then new techniques or generalizations
of existing techniques are called for. These techniques are grouped
under the umbrella of \emph{nonlinear} dimensionality reduction. \\
\\
\noindent
The need for techniques for nonlinear dimensionality reduction had
been recognized at least as early as the Sammon's mapping of
\citet{j.69:_nonlin}. Some of the early approaches to nonlinear
dimensionality reduction was through the extension of PCA. In the
1980's, nonlinear extension of PCA, namely principal curves and
surfaces, was proposed by Hastie
\citet{hastie84:_princ,hastie89:_princ}. Meanwhile extension of PCA
using neural networks was also done in the neural networks community,
e.g., \citet{rubner89,oja92:_princ}.  See also the survey article of
\citet{oja02:_unsup} for more information on work in this
area. Recently, other extensions of PCA were also proposed, such as
regularized principal manifold \citet{smola01:_regul_princ_manif} ,
probabilistic principal component analysis \citet{tipping99:_mixtur}
and generative topographic mapping
\citet{bishop98:_gtm}. \\ \\
%
% 
\noindent Since the introduction of Isomap
\citep{tenebaum00:_global_geomet_framew_nonlin_dimen_reduc} and
Locally Linear Embedding \citep{roweis00:_nonlin}, there has been an
explosion of interest in techniques for manifold learning. Manifold
learning is a class of nonlinear dimensionality reduction algorithms
that are motivated by the assumption that data lies in a
high-dimensional space but is of low intrinsic dimension. Examples of
data sets where this assumption is reasonable includes image data
\citep{tenebaum00:_global_geomet_framew_nonlin_dimen_reduc,bregler95:_nonlin,bregler94:_surfac}
and microarray data \citep{wouters03:_graph}. In image data, the
dimension of an image can be considered to be the number of pixels in
its representation. This makes a small $100$ pixel by $100$ pixel
image to lies in a space of dimension $10^4$. However, in may case of
image data analysis, the subject of the images are fixed and the only
variable between the images are the directions and placement of the
camera. The images are thus assumed to have a much lower intrinsic
dimension, say $3$, the same as the degrees of freedom of the
camera. In microarray DNA data, the number of observed genes for a
particular test subject could be on the order of $10^4$. On the other
hand, the number of test subjects is much less, often of the order of
$10^2$.
\\ \\
%
%
\noindent
There are many reasons for the interest in manifold learning
algorithm. The first and most important reason has to be the intuitive
geometric motivation of the data points lieing on some low-dimensional
manifold in high-dimensional space. Apart from that, the popularity of
manifold learning algorithms might also stem from the popularity and
success of concepts such as graph Laplacians, random
walks on graphs, and spectral decomposition of matrices.  \\ \\
%
\noindent The concept of the combinatorial Laplacian of a graph had
been known at least as early as the matrix-tree theorem
\citep{kirchhoff47:_uber_aufl_gleic_str} which stated that the number
of spanning tree of a graph $G$ is equal to any cofactor of the
Laplacian $\mathbf{L}(G)$ of $G$. \citet{fiedler73:_algeb} then noted
that the second smallest eigenvalue of $\mathbf{L}(G)$ is related to a
notion of connectivity on $G$. This lead to an interest in
understanding the spectral properties of graph Laplacian. See for
example the book \citet{cvetkovic80:_spect_graph_theor_applic} and the
survey articles \citet{merris94:_laplac,mohar91:_graph}. The
normalized Laplacian $\bm{\mathcal{L}}(G)$ rose in popularity through
the work done by Chung and her coauthors. The monograph
\citet{chung97:_spect_graph_theor} contains an overview of the
normalized Laplacian and its connection with graph theory, harmonic
analysis, and other diverse disciplines. Laplacians matrices of graphs
had also appeared in the context of spectral partitioning and
clustering, e.g.,
\citet{scott90:_featur,shi00:_normal,ng02,weiss9:_segmen}. Laplacian matrices
also appeared in manifold learning, where it was observed that they
share connections with the Laplace-Beltrami operator on Riemannian
manifolds. This lead to the study of convergence of discrete
Laplacians
\citep{hein05:_from,luxburg08:_consis,belkin08:_towar_theor_found_laplac,
  hein07:_conver_laplac,coifman06:_diffus_maps}. The graph Laplacians
also appeared in the context of semi-supervised learning where they
were used as regularization terms
\citep{zhu03:_semi_super_learn_using_gauss,belkin06:_manif_regul}. See
also the survey of \citet{zhu05:_semi_super} and the references
within. \\ \\
%
\noindent
Random walks had been a topic of interest to the scientific community
for the last hundred plus years. Karl Pearson asked in
1905, in an issue of
Nature, for help in finding references or solution to the following
problem
\begin{verbatim}
A man starts from a point O and walks l yards in a
straight line; he then turns through any angle whatever 
and walks another l yards in a second straight line. He 
repeats this process n times. I require the probability 
that after these n stretches he is at a distance between
r and r + dr from his starting point, O.
\end{verbatim}
After hearing from Lord Rayleigh that his problem is the same as that
of the composition of $n$ iso-periodic vibrations of unit amplitudes
and random phases, where the probability that the composition deviates
from $0$ is exponentially small, Karl Pearson quipped that ``in open
country the most probable place to find a drunken man who is at all
capable of keeping on his feet is somewhere near his starting point''
\footnote{\url{http://www.nature.com/physics/looking-back/pearson/index.html}}.
Random walks on graphs had appeared in connection with electrical
networks. See for example the book of
\citet{doyle84:_random_walks_elect_networ} and
\citet{lyons:_probab_trees_networ}. Random walks on graphs are
examples of Markov chains and the study of mixing time through such
notions as cover time of graphs was also performed. See for example
the survey article of \citet{lovasz96:_random_graph} and the book
\citet{levin09:_markov_chain_mixin_times}. Recently, random walks on
graphs had been in widespread use in machine learning. For example,
random walks are used in clustering
\citep{saerens04,yen07:_graph,qui07:_clust}, semi-supervised learning
\citep{szummer01:_partial_markov,zhou04:_learn,zhou04:_learn_label_unlab,zhu03:_semi_super_learn_using_gauss},
and graphs sparsification
\citep{spielmand08:_graph}. \\ \\
%
\noindent
Spectral decomposition is at the heart of many techniques in machine
learning. As we have alluded to in our discussion of graph Laplacians,
the area of spectral partitioning and clustering is based on the idea
that the eigenvectors of various matrix representation of data points,
e.g., the combinatorial Laplacian, provide a meaningful way to
partition or cluster the data points. Similarly, a significant portion
of manifold learning algorithm embed data points by using eigenvalues
and eigenvectors of some matrix representation of the data. With the
transition to very large data sets, a lot of work had been done in
approximate spectral decomposition. \footnote{Need reference/citation}
\\ \\
% We
% think that the terminilogy of ``non-linear dimensionality reduction''
% is unfortunate since it's not entirely clear what non-linear means in
% this context. A significant portion of non-linear dimensionality
% reduction algorithm obtained the embedding coordinates by performing
% spectral decomposition of some matrix and then projecting onto the
% first $d$ columns of the orthogonal matrix in the resulting
% decomposition, which is a linear transformation. We think that it's
% more accurate to refer to these techniques as \emph{non-Euclidean}
% dimensionality reduction. The non-Euclidean refers to the aspect
% that, for a lot of these techniques, Euclidean distance or inner
% product in Euclidean space are not appropriate measure of proximities
% between the data points. For example, kernel PCA
% \citep{scholkopf97:_lectur_notes_comput_scien} extends PCA by
% replacing the covariance matrix, which is an Euclidean inner product
% matrix, with other notions of inner product matrices. In the case of
% Isomap \citep{tenebaum00:_global_geomet_framew_nonlin_dimen_reduc},
% the Euclidean distance between points are replaced by shortest path
% distances on some underlying graphs, in the hope that these shortest
% path distances approximate the geodesic distance on some manifold.
%\\ \\
%
%
The structure for this section is as follow. We discuss the classic
dimensionality reduction technique, namely PCA and classical MDS in \S
\ref{sec:classical-mds} and \S \ref{sec:princ-comp-analys},
respectively. We then review several well known manifold learning
algorithms, namely Isomap, LLE, Hessian eigenmaps, Laplacian
eigenmaps, and diffusion maps, in \S \ref{sec:isomap} through \S
\ref{sec:diffusion-maps}. Some of the techniques, such as Isomap,
Laplacian eigenmaps \citep{belkin03:_laplac}, and diffusion maps
\citep{coifman06:_diffus_maps} can be understood as examples of the
following recipe: (i) Represent the data as a weighted (or possibly
unweighted) graph with vertices corresponding to feature vectors and
edge weights representing pairwise proximities in the ambient feature
space; (ii) Compute new proximities between the vertices. (iii) Embed
the graph into a Euclidean space using the new proximities. The
embedding is typically into a space of low dimension. Other
techniques, such as Locally Linear Embedding, Hessian eigenmaps
\citep{donoho03:_hesian}, LTSA
\citep{zhang03:_intel_data_engin_autom_learn} and GNA
\citep{brand05:_from} strive to obtain a global coordinate system that
preserves the neighbourhood structure of the data points. This is done
by computing, for each data point $p$, an approximation of the tangent
space $T_p$ and then aligning the tangent spaces so as to minimize a
loss function. The tangent spaces represent the neighbourhood
structure at each point and their alignment give a global coordinate
system to embed the data points. For additional survey of manifold
learning and dimensionality reduction algorithms, including some that
are not discussed here, refer to the survey articles of
\citet{burges05:_data} and \citet{saul06:_semis}.

% \\
% \\
% %
% %
% \noindent
% The outline of this section is then as follows. We discuss classical
% MDS and PCA in \S \ref{sec:classical-mds}. \S \ref{sec:isomap} through
% \S \ref{sec:diffusion-maps} summarized some of the key ideas of
% well-known manifold learning algorithms, namely Isomap, LLE, Hessian
% eigenmaps, Laplacian eigenmaps, and diffusion maps.
\subsection{Principal Component Analysis}
\label{sec:princ-comp-analys}

\subsection{Classical multidimensional scaling}
\label{sec:classical-mds}
Classical multidimensional scaling \citep{torgesen52:_multid}
constructs an embedding $\mathbf{X}$ of a dissimilarity matrix
$\Delta$ into a $d$ dimensional space by finding a coordinate matrix
$\mathbf{X}$ such that $\mathbf{X}\mathbf{X}^{T}$ is the best rank $d$
approximation to the matrix $\tau(\Delta \circ \Delta)$. The procedure
for classical multidimensional scaling can be summarized by the
following steps.
\begin{enumerate}
\item Compute the matrix of squared dissimilarities $\Delta^{(2)} =
  \Delta \circ \Delta$.
\item Compute $\mathbf{B}(\Delta) = \tau(\Delta^{(2)})$ where the
  $\tau$ transform is as defined in \S
  \ref{sec:distance-geometry}. Note that if $\Delta$ is EDM-1, then
  $\mathbf{B}(\Delta)$ is positive semidefinite. 
\item Compute the eigendecomposition of $\mathbf{B}(\Delta)$ as
  $\mathbf{B}(\Delta) = \mathbf{Q} \bm{\Lambda} \mathbf{Q}^{T}$ where
  $\bm{\Lambda} = \mathrm{diag}(\lambda_1, \lambda_2, \dots,
  \lambda_n)$ and $\lambda_1 \geq \lambda_2 \geq \dots \geq
  \lambda_n$.  
\item Set $\mathbf{Q}_d$ to be the matrix formed by the first $d$
  columns of $\mathbf{Q}$. Set $\bm{\Lambda_{d}} =
  \mathrm{diag}(\bar{\lambda}_1,\bar{\lambda}_2, \dots,
  \bar{\lambda}_d)$ where $\bar{\lambda}_i = \max(\lambda_i, 0)$. The
  matrix $\mathbf{X}_d$ of embedding coordinates is given by
  $\mathbf{Q}_d \mathbf{\Lambda}_{d}^{1/2}$.
\end{enumerate}
By a result in \citet{eckart36:_approx}, $\mathbf{X}_d$ minimizes the
following loss function
\begin{equation}
  \label{eq:87}
 L(\mathbf{Z}) = \| \mathbf{Z} \mathbf{Z}^{T} - \mathbf{B}(\Delta)
 \|_F^{2} 
\end{equation}
subjected to the rank constraint $\mathbf{rank}(\mathbf{Z}) \leq
d$. The solution $\mathbf{X}_d$ is unique up to a rotation, i.e., if
$\mathbf{Y}$ is another minimizer of Eq. \eqref{eq:87} with
$\mathrm{rank}(X_d) = \mathrm{rank}(Y)$, then $\mathbf{Y} =
\mathbf{U}\mathbf{X}_d$ for some unitary matrix $\mathbf{U}$. A nice
property of classical multidimensional scaling is that the dimensions
are nested, i.e., the first $k$ dimensions of an optimal $d$
dimensions embedding, with $k < d$, is also an optimal $k$ dimensions
embedding. For more on classical multidimensional scaling, see
\citet{borg05:_moder} and \citet{gower66:_some}.
%
%


\subsection{Isomap}
\label{sec:isomap}
Isomap \citep{tenebaum00:_global_geomet_framew_nonlin_dimen_reduc} was
introduced as a framework for non-linear dimensionality reduction. One
way of interpreting Isomap is as the application of classical MDS to
the matrix $\bm{\Delta}$ where each entry of $\bm{\Delta}$ corresponds
to the shortest path distance between nodes on some underlying
graph. More specifically, given a set of $N$ data points $\mathcal{X}
= \{x_1, x_2, \dots, x_N\}$ in some high dimensional space $\Omega$,
the Isomap algorithm proceeds as follows
\begin{enumerate}
\item Construct a graph $G$ over $\mathcal{X}$. There are many ways
  for constructing a graph $G$. For example, $G$ can be a
  $\epsilon$-ball graph, i.e., $x_i$ and $x_j$ is connected if $\|x_i
  - x_j\| < \epsilon$ where $\| \cdot \|$ is the Euclidean norm, or it
  can be a $K$-NN graph for some $K$. The dissimilarity associated
  with each edge between $x_i$ and $x_j$ is then $\|x_i - x_j \|$.
\item Compute the matrix $\Delta$ of shortest path distances between
  any pairs $x_i, x_j \in \mathcal{X}$
\item Construct a $m$-dimensional embedding of $\mathcal{X}$ by
  classical multidimensional scaling on $\Delta$.  
\end{enumerate}
Let $\mathcal{M}$ be a compact $d$-dimensional manifold. Suppose that
the points in $\mathcal{X}$ are sampled from $\mathcal{M}$. Isomap is
based on the assumption that, under appropriate constructions of the
graph $G$, the shortest path distance between $x_i$ and $x_j$ on $G$
provides a good approximation to the geodesic distance between $x_i$
and $x_j$ on $\mathcal{M}$, as the number of sampled points increases
to infinity. \citet{bernstein00:_graph} discussed some of the
theoretical properties of Isomap including a proof of the asymptotic
convergence of shortest path distances to geodesic
distances. \citet{silva02:_global} discussed improving the running
time of Isomap by embedding the matrix of shortest path distances
using landmark MDS. As had been observed by \citet{brand05:_chart},
among others, Isomap and other dimensionality reduction algorithms
discussed in this section are embedding methods, i.e., they don't
learn a mapping function from the domain of the data points to the
embedding space. The question of how to handle data points that are
not in the sample become an important question. Out of sample extension for
Isomap, LLE, MDS, and Laplacian eigenmaps can be found in
\citet{bengio04:_out_lle_isomap_mds_eigen,trosset08}. 
%
%
\subsection{Locally Linear Embedding}
\label{sec:locally-line-embedd}
Locally linear embedding \citep{roweis00:_nonlin} is a dimensionality
reduction algorithm that attempts to compute low dimensional,
neighbourhood preserving embeddings through linear reconstructions of a
point from its neighbours. Let $\mathcal{X} = \{x_1,x_2, \dots, x_N\}$
be $N$ data points with each $x_i \in \mathbb{R}^{D}$. Let
$\mathbf{X}$ be the $N \times D$ matrix with each row $i$ being the
coordinates of $x_i$. The locally linear embedding algorithm with
$\mathbf{X}$ as input proceeds as follows
\begin{enumerate}
\item Choose a parameter $K$ representing the size of the
  neighbourhoods of the data points. For each $x_i \in \mathcal{X}$,
  find its $K$ nearest neighbours and denote the resulting set of data
  points as $\mathcal{N}_K(x_i)$.
\item Let $\mathbf{W} = (w_{ij})$ be the $N \times N$ matrix that is
  the solution of the following optimization problem
  \begin{align*}
    \min_{\mathbf{W}} & \quad \| \mathbf{X} - \mathbf{W}\mathbf{X} \|_F^{2} \\
    \text{subject to:} & \quad w_{ij} = 0 \,\, \text{if $x_j \not \in
      \mathcal{N}_K(x_i)$} \\
    & \quad \sum_{j=1}^{N}{w_{ij}} = 1
  \end{align*}
  $\mathbf{W}$ is the matrix of reconstruction weights with $w_{ij}$
  for $w_{ij} \not = 0$ summarizing the contribution of $x_j \in
  \mathcal{N}_K(x_i)$ to the reconstruction of $x_i$.
\item Let $\mathbf{Y}$ be the $N \times d$ matrix of embedding
  coordinates. $\mathbf{Y}$ is then the solution of the following
  optimization problem
  \begin{equation}
    \label{eq:94}
    \min_{\mathbf{Y}^{T}\mathbf{Y} =
      \mathbf{I}}\|(\mathbf{I} - \mathbf{W})\mathbf{Y}\|_{F}^{2}
  \end{equation}
\end{enumerate}
Let $\mathbf{M} = (\mathbf{I} - \mathbf{W})^{T}(\mathbf{I} -
\mathbf{W})$. If $\lambda_1 \leq \lambda_2 \leq \dots \leq \lambda_N$
are the eigenvalues of $\mathbf{M}$, then the solution of the
optimization problem in Eq.~\eqref{eq:94} is the matrix $\mathbf{Y}$
that's composed of the $d$ eigenvectors corresponding to the
eigenvalues $\lambda_1$ through $\lambda_{d}$. This lead \citet{ham04}
to view locally linear embedding as a version of kernel PCA
\citep{scholkopf97:_lectur_notes_comput_scien} with the kernel given
by the matrix $\mathbf{M}$.
%
%
\subsection{Hessian eigenmaps}
\label{sec:hessian-eigenmaps}
Hessian eigenmaps \citep{donoho03:_hesian} was described as a locally
linear embedding technique for high-dimensionality data,
c.f. \citep{roweis00:_nonlin} and \S
\ref{sec:locally-line-embedd}. Hessian eigenmaps is motivated by the
assumption that the data points lies on a $d$-dimensional Riemannian
manifold $\mathcal{M}$. For $f \colon \mathcal{M} \mapsto \mathbb{R}$,
define a quadratic form $\mathcal{H}(f)$ associated with $f$ by
$\mathcal{H}(f) = \int_{M}{ \| H_f(m) \|_{F}^{2} dm }$ where $H_f(m)$
is the Hessian of $f$ at $m$ and $\| \cdot \|_F$ is the Frobenius
norm. $\mathcal{H}(f)$ averages the Frobenius norm of the Hessian of
$f$ over $\mathcal{M}$. The embedding coordinates
corresponds to the basis for the null space of $\mathcal{H}(f)$. \\ \\
\noindent
Let $\{m_i\}_{i=1}^{N}$ be a collection of $N$ points in
$\mathbb{R}^{n}$. Suppose that $K$ is a parameter chosen to be the
size of the nearest neighborhoods and $d$ is a parameter chosen to be
the dimension of the embedding coordinates. The Hessian eigenmaps
procedure proceed as follows \citet{donoho03:_hesian}
\begin{enumerate}
\item Identify neighbors: For each $m_i$, set $\mathcal{N}(m_i)$ to be
  the set of the $K$ nearest neighbours of $m_i$. For each
  neighbourhood $\mathcal{N}(m_i)$, set $\mathbf{M}^{(i)}$ to be the
  $K \times n$ matrix whose rows are the points $m_j \in
  \mathcal{N}(m_i)$. Define $\bar{\mathbf{M}}^{(i)}$ to be
  $(\mathbf{I} - \mathbf{J}/K)\mathbf{M}^{(i)}$,
  i.e. $\bar{\mathbf{M}}^{(i)}$ is the row centering of
  $\mathbf{M}^{(i)}$.
\item Do principal component analysis (PCA) on
  $\bar{\mathbf{M}}^{(i)}$. This is equivalent to finding the singular
  value decomposition (SVD) of $\bar{\mathbf{M}}^{(i)} = \mathbf{U}^{(i)}
  \bm{\Lambda}^{(i)} \mathbf{V}^{(i)}$ and using the first $d$ columns of
  $\mathbf{U}^{(i)}$ as the tangent coordinates of points in
  $\mathcal{N}(m_i)$. 
\item Let $f \colon \mathcal{M} \mapsto \mathbb{R}$ be a smooth
  function. Denote by $\bm{\nu}^{(i)} \in \mathbb{R}^{K}$ the vector
  consisting of $f(m_j)$ for $m_j \in \mathcal{N}(m_i)$. The Hessian
  estimator for $\mathcal{N}(m_i)$ is a matrix $\mathbf{H}^{(i)}$ such
  that $\mathbf{H}^{(i)} \bm{\nu}^{(i)}$ approximates the entries of
  the Hessian matrix of $f$ at $m_i$, i.e.,
  \begin{equation}
    \label{eq:84}
    \mathbf{H}^{(i)} \bm{\nu}^{(i)} \approx {\left[ \begin{array}{ccccc}
          \frac{\partial^2 f}{\partial u_1 \partial u_1} \bigl |_{m_i} &
          \frac{\partial^2 f}{\partial u_1 \partial u_2} \bigl |_{m_i} &
          & \dots & \frac{\partial^2 f}{\partial u_{d} \partial u_d}
          \bigl|_{m_i} 
\end{array} \right ]}^{T}
  \end{equation}
  where $u_1, u_2, \dots, u_d$ denotes a orthonormal basis for the
  points in $\mathcal{N}(m_i)$. $\mathbf{H}^{(i)}$ is a matrix of size
  $d(d+1)/2 \times K$ and can be found as follows. Consider the
  second-order Taylor series expansion for $f$ around $m_i$
  \begin{gather}
    \label{eq:85}
    f(m_i) + \sum_{r=1}^{d}{b_r u_r} + \sum_{r=1}^{d}\sum_{s=r}^{d}{a_{rs} u_r u_s}
    \shortintertext{where}
    b_r = \frac{\partial{f}}{\partial u_r} \Bigl|_{m_i} , \qquad a_{rs}
    = \frac{1}{2}\frac{\partial^2 f}{\partial u_r \partial u_s} \Bigl|_{m_i}
  \end{gather}
  The truncated polynomial in Eq.~\eqref{eq:85} can be fitted using
  least squares \citep[see][]{kim09:_semi_regres_hessian})
  \begin{equation}
    \label{eq:86}
    \min_{\bm{w} \in \mathbb{R}^{p}} \sum_{j=1}^{K} \Bigl( (f(m_i) -
    f(m_j)) - (\mathbf{X}^{(i)} \bm{w})(j)\Bigr)^2
  \end{equation}
  where $p = 1 + d + d(d+1)/2$ and $\mathbf{X}^{(i)}$ is the $K \times
  p$ matrix with the first $d+1$ columns of $\mathbf{X}^{(i)}$ being
  the columns of $\bm{1}$ and the first $d$ columns of
  $\mathbf{U}^{(i)}$, and the remaining $d(d+1)/2$ columns of
  $\mathbf{X}^{(i)}$ being the element-wise products between pairs of
  the first $d$ columns of $\mathbf{U}^{(i)}$. The solution to
  Eq.~\eqref{eq:86} is given by $\bm{w} =
  (\mathbf{X}^{(i)})^{\dagger}f$. The matrix $\mathbf{H}^{(i)}$ is
  then the transpose of the last $d(d+1)/2$ columns of
  $(\mathbf{X}^{(i)}  )^{\dagger}$. If we want $\mathbf{H}^{(i)}$ to
  have orthnormal columns, then $\mathbf{H}^{(i)}$ coincides with the
  transpose of the last $d(d+1)/2$ columns of the matrix $\mathbf{Q}$
  in the QR decomposition of $\mathbf{X}^{(i)}$.
\item Build a symmetric matrix $\mathbf{H} =
  \sum_{i=1}^{N}{(\mathbf{H}^{(i)})^{T}\mathbf{H}^{(i)}}$. If
  $\bm{\nu} = (f(m_j))_{j=1}^{N}$ is a sample vector of $f$, then
  $\bm{\nu}^{T} \mathbf{H} \bm{\nu}$ is an approximation of
  $\mathcal{H}(f)$. 
\item Perform an eigendecomposition of $\mathbf{H}$ and identify the
  $d$ eigenvectors of $\mathbf{H}$ corresponding to the second
  smallest eigenvalues up to the $d+1$ smallest eigenvalues. Let
  $\mathbf{V}$ be the matrix with these eigenvectors as columns. The
  embedding coordinates of point $m_i$ are given by the $i$-th row of
  $\mathbf{V}$.
\end{enumerate}

\subsection{Laplacian eigenmaps}
\label{sec:laplacian-eigenmaps}
Laplacian eigenmaps \citet{belkin03:_laplac} is another embedding
techniques for high dimensional data that embeds using a system of
generalized eigenvectors of the graph Laplacian. Given a set of $N$
data points $\mathcal{X} = (x_1, x_2, \dots, x_N)$ in some high
dimensional space $\Omega$ with norm $\| \cdot \|$, the Laplacian
eigenmaps algorithm proceeds as follow.
\begin{enumerate}
\item Construct a graph $G = (V,E,\omega)$ over $\mathcal{X}$. Like in
  the case of Isomap in \S \ref{sec:isomap}, there are various ways of
  constructing a $G$, such as $\epsilon$-ball or $K$-NN. We assume
  that the resulting graph $G$ is connected. 
\item Assign a similarity measure $\omega \colon E \mapsto
  \mathbb{R}^{\geq 0}$. Once again, there are various different
  methods. One widely used method is to use the Gaussian
  similarity/heat kernel as given by
  \begin{equation}
    \label{eq:88}
    \omega(x_i,x_j) = e^{-\frac{\|x_i - x_j\|^2}{\delta^2}}
  \end{equation}
  Another method is to make $\omega$ unweighted, i.e. $\omega(x_i,x_j)
  \equiv 1$.
\item Let $\mathbf{L}$ be the combinatorial Laplacian of $G$ with
  similarity measure $\omega$ (see \S \ref{sec:graph-laplacians}. Let
  $\mathbf{D}$ be the diagonal matrix with entries $\mathbf{D}(v,v) =
  \deg(v)$. Compute the eigenvalues and eigenvectors of the following
  generalized eigenvalue problem
  \begin{equation}
    \label{eq:91}
    \mathbf{Lf} = \lambda \mathbf{Df}
  \end{equation}
\item Let $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_N$ be the
  eigenvalues of Eq. (\ref{eq:91}), and $\mathbf{f}_1, \mathbf{f}_2,
  \dots$ be the corresponding eigenvectors. The embedding into
  $\mathbb{R}^{m}$ is given by
  \begin{equation}
    \label{eq:92}
    x_i \rightarrow (\mathbf{f}_2(i), \mathbf{f}_3(i), \dots,
    \mathbf{f}_{m+1}(i))
  \end{equation}
  where $\mathbf{f}_1$ is constant, and thus ignored.  
\end{enumerate}
Part of the motivation of Laplacian eigenmaps is the relationship
between the Laplace-Beltrami operator $\Delta$ on a manifold and the
normalized Laplacian of a graph. Let $\mathcal{M}$ be a compact,
smooth $d$-dimensional Riemannian manifold. Suppose that the data
points $\mathcal{X}$ are sampled uniformly at random from
$\mathcal{M}$. Roughly speaking, if one construct a complete graph $G$
over the data points $\mathcal{X}$ with similarity measure
$\omega(x_i,x_j) = K/N \exp{(\tfrac{\|x_i - x_j\|^2}{4t})}$ where $K$
is a constant depending on the dimension $d$ and the width $t$, then
as $N \rightarrow \infty$, the eigenvalues and eigenvectors of the
Laplacian matrix $L$ converges to the corresponding eigenvalues and
eigenfunctions of the Laplace-Beltrami operator $\Delta$ on
$\mathcal{M}$, see \citet{belkin08:_towar_theor_found_laplac}, and
\citet{belkin06:_conver_laplac_eigen}. \\ \\
%
%
\noindent The embeddings found by Laplacian eigenmaps can also be
viewed as the solutions to an optimization problem. Let
$\mathbf{Z}$ be a $N \times d$ matrix where $z^{(i)}$, the $i$-th
row of $\mathbf{Z}$, is the embedding coordinates of $x_i \in
\mathcal{X}$. Consider the following optimization problem
\begin{equation}
  \label{eq:89}
  \argmin_{\mathbf{Z}^{T}\mathbf{D} \mathbf{Z} = \mathbf{I}_m}{\,\,
    \mathrm{trace}(\mathbf{Z}^{T} \mathbf{L} \mathbf{Z})}
  = \argmin_{\mathbf{Z}^{T}\mathbf{D} \mathbf{Z} = \mathbf{I}_m}{\,\,
    \sum_{i,j}{ \|z^{(i)} - z^{(j)}\|^{2} \omega_{ij}}}
\end{equation}
where $\mathbf{I}_m$ is the $m \times m$ identity matrix. The
objective function in Eq.~\eqref{eq:89} penalizes mappings where $x_i$
and $x_j$ with high similarity (large $\omega(x_i,x_j)$) are mapped to
$z^{(i)}$ and $z^{(j)}$ that are far apart. The constraint in
Eq.~\eqref{eq:89} is to prevent degenerate solutions. The solution of
the above optimization problem can be found as follows
 \begin{equation}
   \label{eq:90}
   \begin{split}
     \min_{\mathbf{Z}^{T}\mathbf{D} \mathbf{Z} = \mathbf{I}_m}{\,\,
       \mathrm{trace}(\mathbf{Z}^{T} \mathbf{L} \mathbf{Z})} &=
     \min_{\mathbf{Q}^{T}\mathbf{Q} = \mathbf{I}_m}{\,\,
       \mathrm{trace}(\mathbf{Q}^{T} \mathbf{D}^{-1/2} \mathbf{L}
       \mathbf{D}^{-1/2} \mathbf{Q})} \\
     &= \min_{\mathbf{Q}^{T}\mathbf{Q} = \mathbf{I}_m}{\, \,
       \mathrm{trace}(\mathbf{Q}^{T} \mathbf{U}^{T} \bm{\Sigma}
       \mathbf{U} \mathbf{Q})} \\
     &= \min_{\mathbf{V}^{T}\mathbf{V} = \mathbf{I}_m}{\, \,
       \mathrm{trace}(\mathbf{V}^{T} \bm{\Sigma}
       \mathbf{V})} \\
     &= \sum_{k=1}^{m}{\lambda^{\uparrow}_m}
   \end{split}
 \end{equation}
 where $\mathbf{U}^{T} \bm{\Sigma} \mathbf{U}$ is the
 eigendecomposition of $\mathbf{D}^{-1/2} \mathbf{L}
 \mathbf{D}^{-1/2}$ and $\lambda^{\uparrow}_m$ are the diagonal
 entries of $\bm{\Sigma}$, arranged in non-decreasing order. Now
 $\mathbf{D}^{-1/2} \mathbf{L} \mathbf{D}^{-1/2} = \mathbf{D}^{1/2}
 \mathbf{D}^{-1} \mathbf{L} \mathbf{D}^{-1/2}$, and so the eigenvalues
 of $\mathbf{D}^{-1} \mathbf{L}$ coincides with the eigenvalues of
 $\mathbf{D}^{-1/2} \mathbf{L} \mathbf{D}^{-1/2}$. The minimum in
 Eq.~\eqref{eq:90} is thus achieved when the columns of $\mathbf{Z}$
 are the $m$ eigenvectors corresponding to the eigenvalues
 $\lambda_{1}^{\uparrow}, \lambda_{2}^{\uparrow}, \dots,
 \lambda_{m}^{\uparrow}$ of $\mathbf{D}^{-1} \mathbf{L}$. This is
 equivalent to Eq.~\eqref{eq:91}. The embeddings found by Laplacian
 eigenmaps is therefore the solution of the optimization problem in
 Eq.~\eqref{eq:89}.
%
%
\subsection{Diffusion maps}
\label{sec:diffusion-maps}
Diffusion maps \citet{coifman06:_diffus_maps} is an embedding
framework based upon diffusion processes. The technique is based on
the eigenvalues and eigenvectors of transition matrices of Markov
chains on graphs, and is closely related to Laplacian eigenmaps as
discussed in \S \ref{sec:laplacian-eigenmaps}. Associated with
diffusion maps is a notion of a family of distances named diffusion
distances. \citet{coifman06:_diffus_maps} viewed the family of
diffusion distances as defining multiscale geometries on the data
set. \\ \\
\noindent
Given a set of $N$ data points $\mathcal{X} = \{x_1,x_2,\dots,x_N\}$,
diffusion maps proceeds as follows (c.f. Isomap \S \ref{sec:isomap}
and Laplacian eigenmaps \S \ref{sec:laplacian-eigenmaps})
\begin{enumerate}
\item Construct a graph $G = (V,E,\omega)$ over $\mathcal{X}$.
\item Generate the transition matrix $\mathbf{P}$ of the random walk
  on $G$, see \S \ref{sec:random-walks-graphs}.
\item Let $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_N$ be the
  eigenvalues of $\mathbf{P}$ and $\mathbf{f}_1, \mathbf{f}_2, \dots,
  \mathbf{f}_N$ be the corresponding eigenvectors. Choose an integer
  $t \geq 0$. $t$ represent the time scale. The embedding 
  $\vartheta_{t,m}$ of $\mathcal{X}$ into $\mathbb{R}^{m}$ is given by
  \begin{equation}
    \label{eq:93}
    \vartheta_{t,m}(x_i) \mapsto \Bigl(\lambda_2^{t} \mathbf{f}_2(i), \lambda_3^{t}
    \mathbf{f}_3(i), \dots, \lambda_{m+1}^{t} \mathbf{f}_{m+1}(i) \Bigr)
 \end{equation}
 where $\mathbf{f}_1$ is constant and thus ignored.
\end{enumerate}
The construction of the graph $G$ is an important part of the
working of diffusion maps. \citet{coifman06:_diffus_maps} argues for
the following approach to constructing $G$. Let $\mathcal{M}$ be a
smooth, compact submanifold of $\mathbb{R}^{n}$ and suppose that
the distribution of points on $\mathcal{M}$ is given by a density
function $q$. Let $h$ be a radial function. Fix $\alpha \in
\mathbb{R}$. The family of anisotropic diffusion kernels is given by
the following procedure
\begin{enumerate}
\item Choose $\epsilon > 0$ and define $k_{\epsilon}(x,y)$, $x,y
  \in \mathcal{M}$ by $k_{\epsilon}(x,y) = h(\| x - y \|^2/\epsilon)$.
\item Let $q_{\epsilon}(x) = \int_{\mathcal{M}}{k_{\epsilon}(x,y) q(y)
    \,d y}$ and
  form a new kernel $k_{\epsilon}^{\alpha}(x,y)$ as
  \begin{equation}
    \label{eq:95}
    k_{\epsilon}^{(\alpha)}(x,y) =
    \frac{k_{\epsilon}(x,y)}{(q_{\epsilon}(x) q_{\epsilon}(y))^{\alpha}}
  \end{equation}
\item Normalize $k_{\epsilon}^{\alpha}$ by setting
  $d_{\epsilon}^{(\alpha)}(x) = \int_{\mathcal{M}}{k_{\epsilon}^{(\alpha)}(x,y) q(y)
    \, d y}$ and define the transition probability
    $p_{\epsilon,\alpha}(x,y)$ by
    \begin{equation}
      \label{eq:96}
      p_{\epsilon,\alpha}(x,y) = \frac{k_{\epsilon}^{\alpha}(x,y)}{d_{\epsilon}^{\alpha}(x,y)}
    \end{equation}
\end{enumerate}
\citet{coifman06:_diffus_maps} then showed that as $\epsilon
\rightarrow 0$, the integral operator on $L^{2}(\mathcal{M},q)$ with
kernel $p_{\epsilon,\alpha}$ converges to different operators for
different choices of $\alpha$. Special cases includes $\alpha = 0$,
$\alpha = 1/2$ and $\alpha = 1$, with the resulting operator being the
Laplace-Beltrami operator, the Fokker-Planck operator, and the heat
kernel, respectively. See also
\citet{nadler05:_diffus_eigen_fokker_planc_operat} and
\citet{nadler06:_diffus}.

\subsection{Some thoughts on Isomap, Laplacian eigenmaps and diffusion maps}
\label{sec:some-thoughts-isomap}
The introductory remarks to this section mentioned that Isomap,
Laplacian eigenmaps, and diffusion maps can all be understood as
examples of the following recipe
\begin{enumerate}
\item Represent the data as a weighted, or possibly unweighted,
  graph. The vertices correspond to feature vectors and the edges
  convey information about how pairs of vertices are related. The edge
  weights represent pairwise proximities in the ambient feature space,
  either similarities or dissimilarities. Isomap includes an edge
  between vertices $i$ and $j$ if the data points $x_i$ and $x_j$ are
  sufficiently close in the Euclidean feature space and weighs the
  edges by the Euclidean distance between the data points. Laplacian
  eigenmaps and diffusion maps weighs the edges by the Gaussian
  kernel with some bandwidth parameter.
\item Compute new proximities between the vertices. Isomap compute
  shortest path distance, diffusion maps compute diffusion
  distances. A variation of Laplacian eigenmaps compute expected
  commute time.
\item Embed the graph in an Euclidean space using the new
  proximities. Isomap embeds using classical MDS while Laplacian
  eigenmaps and diffusion maps embed using the eigenvalues and
  eigenvectors of the normalized Laplacian. 
\end{enumerate}
Common to all three algorithms is the construction of a graph $G$ over
the data points. However, the question of how to construct a graph $G$
is still unresolved. We have discussed two well known techniques,
namely $\epsilon$-ball and $K$ nearest neighbours. There exist results
that indicates that different graph construction leads to vastly
different results, see, for example,
\citet{maier08:_influen} and \citet{maier09:_optim_k}. \\ \\
%
%
\noindent
What distinguishes these algorithms is then the second and third
step. However, since the embeddings are constructed to be good
representations of the proximities, it's the choice of the proximities
in the second step that's the more important difference among these
algorithms. \S \ref{cha:dist-undir-graphs} and \S
\ref{cha:dist-direct-graphs} are devoted to discussing various notions
of distances on graphs, including diffusion distances and expected
commute time.
%
%

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "dissertation"
%%% End: 
