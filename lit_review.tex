\chapter{Introduction}
\label{cha:introduction}

\section{Dimensionality reduction algorithms}
\label{sec:dimens-reduct-algor}



\subsection{Classical multidimensional scaling}
\label{sec:classical-mds}
Classical multidimensional scaling \citep{torgesen52:_multid}
constructs an embedding $\mathbf{X}$ of a dissimilarity matrix
$\Delta$ into a $d$ dimensional space by finding a coordinate matrix
$\mathbf{X}$ such that $\mathbf{X}\mathbf{X}^{T}$ is the best rank $d$
approximation to the matrix $\tau(\Delta \circ \Delta)$. The procedure
for classical multidimensional scaling can be summarized by the
following steps.
\begin{enumerate}
\item Compute the matrix of squared dissimilarities $\Delta^{(2)} =
  \Delta \circ \Delta$.
\item Compute $\mathbf{B}(\Delta) = \tau(\Delta^{(2)})$ where the
  $\tau$ transform is as defined in \S
  \ref{sec:distance-geometry}. Note that if $\Delta$ is EDM-1, then
  $\mathbf{B}(\Delta)$ is positive semidefinite. 
\item Compute the eigendecomposition of $\mathbf{B}(\Delta)$ as
  $\mathbf{B}(\Delta) = \mathbf{Q} \bm{\Lambda} \mathbf{Q}^{T}$ where
  $\bm{\Lambda} = \mathrm{diag}(\lambda_1, \lambda_2, \dots,
  \lambda_n)$ and $\lambda_1 \geq \lambda_2 \geq \dots \geq
  \lambda_n$.  
\item Set $\mathbf{Q}_d$ to be the matrix formed by the first $d$
  columns of $\mathbf{Q}$. Set $\bm{\Lambda_{d}} =
  \mathrm{diag}(\bar{\lambda}_1,\bar{\lambda}_2, \dots,
  \bar{\lambda}_d)$ where $\bar{\lambda}_i = \max(\lambda_i, 0)$. The
  matrix $\mathbf{X}_d$ of embedding coordinates is given by
  $\mathbf{Q}_d \mathbf{\Lambda}_{d}^{1/2}$.
\end{enumerate}
By a result in \citet{eckart36:_approx}, $\mathbf{X}_d$ minimizes the
following loss function
\begin{equation}
  \label{eq:87}
 L(\mathbf{Z}) = \| \mathbf{Z} \mathbf{Z}^{T} - \mathbf{B}(\Delta)
 \|_F^{2} 
\end{equation}
subjected to the rank constraint $\mathbf{rank}(\mathbf{Z}) \leq
d$. The solution $\mathbf{X}_d$ is unique up to a rotation, i.e., if
$\mathbf{Y}$ is another minimizer of Eq. \eqref{eq:87} with
$\mathrm{rank}(X_d) = \mathrm{rank}(Y)$, then $\mathbf{Y} =
\mathbf{U}\mathbf{X}_d$ for some unitary matrix $\mathbf{U}$. A nice
property of classical multidimensional scaling is that the dimensions
are nested, i.e., the first $k$ dimensions of an optimal $d$
dimensions embedding, with $k < d$, is also an optimal $k$ dimensions
embedding. For more on classical multidimensional scaling, see
\citet{borg05:_moder} and \citet{gower66:_some}.
%
%
\subsection{Isomap}
\label{sec:isomap}
Isomap \citep{tenebaum00:_global_geomet_framew_nonlin_dimen_reduc} was
introduced as a framework for non-linear dimensionality reduction. One
way of interpreting Isomap is as the application of classical MDS to
the matrix $\bm{\Delta}$ where each entry of $\bm{\Delta}$ corresponds
to the shortest path distance between nodes on some underlying
graph. More specifically, given a set of $N$ data points $\mathcal{X}
= \{x_1, x_2, \dots, x_N\}$ in some high dimensional space $\Omega$,
the Isomap algorithm proceeds as follows
\begin{enumerate}
\item Construct a graph $G$ over $\mathcal{X}$. There are many ways
  for constructing a graph $G$. For example, $G$ can be a
  $\epsilon$-ball graph, i.e., $x_i$ and $x_j$ is connected if $\|x_i
  - x_j\| < \epsilon$ where $\| \cdot \|$ is the Euclidean norm, or it
  can be a $K$-NN graph for some $K$. The dissimilarity associated
  with each edge between $x_i$ and $x_j$ is then $\|x_i - x_j \|$.
\item Compute the matrix $\Delta$ of shortest path distances between
  any pairs $x_i, x_j \in \mathcal{X}$
\item Construct a $m$-dimensional embedding of $\mathcal{X}$ by
  classical multidimensional scaling on $\Delta$.  
\end{enumerate}
Let $\mathcal{M}$ be a compact $d$-dimensional manifold. Suppose that
the points in $\mathcal{X}$ are sampled from $\mathcal{M}$. Isomap is
based on the assumption that, under appropriate constructions of the
graph $G$, the shortest path distance between $x_i$ and $x_j$ on $G$
provides a good approximation to the geodesic distance between $x_i$
and $x_j$ on $\mathcal{M}$, as the number of sampled points increases to
infinity. \citet{bernstein00:_graph} discussed some of the theoretical
properties of Isomap including a proof of the asymptotic convergence
of shortest path distances to geodesic distances.
%
%
\subsection{Locally Linear Embedding}
\label{sec:locally-line-embedd}
Locally linear embedding \citep{roweis00:_nonlin} is a dimensionality
reduction algorithm that attempts to compute low dimensional,
neighbourhood preserving embeddings through linear reconstructions of a
point from its neighbours. Let $\mathcal{X} = \{x_1,x_2, \dots, x_N\}$
be $N$ data points with each $x_i \in \mathbb{R}^{D}$. Let
$\mathbf{X}$ be the $N \times D$ matrix with each row $i$ being the
coordinates of $x_i$. The locally linear embedding algorithm with
$\mathbf{X}$ as input proceeds as follows
\begin{enumerate}
\item Choose a parameter $K$ representing the size of the
  neighbourhoods of the data points. For each $x_i \in \mathcal{X}$,
  find its $K$ nearest neighbours and denote the resulting set of data
  points as $\mathcal{N}_K(x_i)$.
\item Let $\mathbf{W} = (w_{ij})$ be the $N \times N$ matrix that is the solution
  of the following optimization problem
  \begin{align*}
    \min_{\mathbf{W}} & \quad \| \mathbf{X} - \mathbf{W}\mathbf{X} \|_F^{2} \\
    \text{subject to:} & \quad w_{ij} = 0 \,\, \text{if $x_j \not \in
      \mathcal{N}_K(x_i)$} \\
    & \quad \sum_{j=1}^{N}{w_{ij}} = 1
  \end{align*}
  $\mathbf{W}$ is the matrix of reconstruction weights with $w_{ij}$
  for $w_{ij} \not = 0$ summarizing the contribution of $x_j \in
  \mathcal{N}_K(x_i)$ to the reconstruction of $x_i$.
\item Let $\mathbf{Y}$ be the $N \times d$ matrix of embedding
  coordinates. $\mathbf{Y}$ is then the solution of the following
  optimization problem
  \begin{equation}
    \label{eq:94}
    \min_{\mathbf{Y}^{T}\mathbf{Y} =
      \mathbf{I}}\|(\mathbf{I} - \mathbf{W})\mathbf{Y}\|_{F}^{2}
  \end{equation}
\end{enumerate}
Let $\mathbf{M} = (\mathbf{I} - \mathbf{W})^{T}(\mathbf{I} -
\mathbf{W})$. If $\lambda_1 \leq \lambda_2 \leq \dots \leq \lambda_N$
are the eigenvalues of $\mathbf{M}$, then the solution of the
optimization problem in Eq.~\eqref{eq:94} is the matrix $\mathbf{Y}$
that's composed of the $d$ eigenvectors corresponding to the
eigenvalues $\lambda_2$ through $\lambda_{d+1}$. This lead
\citet{ham04} to view locally linear embedding as a version of kernel
PCA \citep{scholkopf97:_lectur_notes_comput_scien} with the kernel
given by the matrix $\mathbf{M}$.
%
%
\subsection{Hessian eigenmaps}
\label{sec:hessian-eigenmaps}
Hessian eigenmaps \citep{donoho03:_hesian} was described as a locally
linear embedding technique for high-dimensionality data,
c.f. \citep{roweis00:_nonlin} and \S
\ref{sec:locally-line-embedd}. Hessian eigenmaps is motivated by the
assumption that the data points lies on a $d$-dimensional Riemannian
manifold $\mathcal{M}$. For $f \colon \mathcal{M} \mapsto \mathbb{R}$,
define a quadratic form $\mathcal{H}(f)$ associated with $f$ by
$\mathcal{H}(f) = \int_{M}{ \| H_f(m) \|_{F}^{2} dm }$ where $H_f(m)$
is the Hessian of $f$ at $m$ and $\| \cdot \|_F$ is the Frobenius
norm. $\mathcal{H}(f)$ averages the Frobenius norm of the Hessian of
$f$ over $\mathcal{M}$. The embedding coordinates
corresponds to the basis for the null space of $\mathcal{H}(f)$. \\ \\
\noindent
Let $\{m_i\}_{i=1}^{N}$ be a collection of $N$ points in
$\mathbb{R}^{n}$. Suppose that $K$ is a parameter chosen to be the
size of the nearest neighborhoods and $d$ is a parameter chosen to be
the dimension of the embedding coordinates. The Hessian eigenmaps
procedure proceed as follows \citet{donoho03:_hesian}
\begin{enumerate}
\item Identify neighbors: For each $m_i$, set $\mathcal{N}(m_i)$ to be
  the set of the $K$ nearest neighbours of $m_i$. For each
  neighbourhood $\mathcal{N}(m_i)$, set $\mathbf{M}^{(i)}$ to be the
  $K \times n$ matrix whose rows are the points $m_j \in
  \mathcal{N}(m_i)$. Define $\bar{\mathbf{M}}^{(i)}$ to be
  $(\mathbf{I} - \mathbf{J}/K)\mathbf{M}^{(i)}$,
  i.e. $\bar{\mathbf{M}}^{(i)}$ is the row centering of
  $\mathbf{M}^{(i)}$.
\item Do principal component analysis (PCA) on
  $\bar{\mathbf{M}}^{(i)}$. This is equivalent to finding the singular
  value decomposition (SVD) of $\bar{\mathbf{M}}^{(i)} = \mathbf{U}^{(i)}
  \bm{\Lambda}^{(i)} \mathbf{V}^{(i)}$ and using the first $d$ columns of
  $\mathbf{U}^{(i)}$ as the tangent coordinates of points in
  $\mathcal{N}(m_i)$. 
\item Let $f \colon \mathcal{M} \mapsto \mathbb{R}$ be a smooth
  function. Denote by $\bm{\nu}^{(i)} \in \mathbb{R}^{K}$ the vector
  consisting of $f(m_j)$ for $m_j \in \mathcal{N}(m_i)$. The Hessian
  estimator for $\mathcal{N}(m_i)$ is a matrix $\mathbf{H}^{(i)}$ such
  that $\mathbf{H}^{(i)} \bm{\nu}^{(i)}$ approximates the entries of
  the Hessian matrix of $f$ at $m_i$, i.e.,
  \begin{equation}
    \label{eq:84}
    \mathbf{H}^{(i)} \bm{\nu}^{(i)} \approx {\left[ \begin{array}{ccccc}
          \frac{\partial^2 f}{\partial u_1 \partial u_1} \bigl |_{m_i} &
          \frac{\partial^2 f}{\partial u_1 \partial u_2} \bigl |_{m_i} &
          & \dots & \frac{\partial^2 f}{\partial u_{d} \partial u_d} \bigl|_{m_i} \end{array} \right ]}^{T}
  \end{equation}
  where $u_1, u_2, \dots, u_d$ denotes a orthonormal basis for the
  points in $\mathcal{N}(m_i)$. $\mathbf{H}^{(i)}$ is a matrix of size
  $d(d+1)/2 \times K$ and can be found as follows. Consider the
  second-order Taylor series expansion for $f$ around $m_i$
  \begin{gather}
    \label{eq:85}
    f(m_i) + \sum_{r=1}^{d}{b_r u_r} + \sum_{r=1}^{d}\sum_{s=r}^{d}{a_{rs} u_r u_s}
    \shortintertext{where}
    b_r = \frac{\partial{f}}{\partial u_r} \Bigl|_{m_i} , \qquad a_{rs}
    = \frac{1}{2}\frac{\partial^2 f}{\partial u_r \partial u_s} \Bigl|_{m_i}
  \end{gather}
  The truncated polynomial in Eq.~\eqref{eq:85} can be fitted using
  least squares \citep[see][]{kim09:_semi_regres_hessian})
  \begin{equation}
    \label{eq:86}
    \min_{\bm{w} \in \mathbb{R}^{p}} \sum_{j=1}^{K} \Bigl( (f(m_i) -
    f(m_j)) - (\mathbf{X}^{(i)} \bm{w})(j)\Bigr)^2
  \end{equation}
  where $p = 1 + d + d(d+1)/2$ and $\mathbf{X}^{(i)}$ is the $K \times
  p$ matrix with the first $d+1$ columns of $\mathbf{X}^{(i)}$ being
  the columns of $\bm{1}$ and the first $d$ columns of
  $\mathbf{U}^{(i)}$, and the remaining $d(d+1)/2$ columns of
  $\mathbf{X}^{(i)}$ being the element-wise products between pairs of
  the first $d$ columns of $\mathbf{U}^{(i)}$. The solution to
  Eq.~\eqref{eq:86} is given by $\bm{w} =
  (\mathbf{X}^{(i)})^{\dagger}f$. The matrix $\mathbf{H}^{(i)}$ is
  then the transpose of the last $d(d+1)/2$ columns of
  $(\mathbf{X}^{(i)}  )^{\dagger}$. If we want $\mathbf{H}^{(i)}$ to
  have orthnormal columns, then $\mathbf{H}^{(i)}$ coincides with the
  transpose of the last $d(d+1)/2$ columns of the matrix $\mathbf{Q}$
  in the QR decomposition of $\mathbf{X}^{(i)}$.
\item Build a symmetric matrix $\mathbf{H} =
  \sum_{i=1}^{N}{(\mathbf{H}^{(i)})^{T}\mathbf{H}^{(i)}}$. If
  $\bm{\nu} = (f(m_j))_{j=1}^{N}$ is a sample vector of $f$, then
  $\bm{\nu}^{T} \mathbf{H} \bm{\nu}$ is an approximation of
  $\mathcal{H}(f)$. 
\item Perform an eigendecomposition of $\mathbf{H}$ and identify the
  $d$ eigenvectors of $\mathbf{H}$ corresponding to the second
  smallest eigenvalues up to the $d+1$ smallest eigenvalues. Let
  $\mathbf{V}$ be the matrix with these eigenvectors as columns. The
  embedding coordinates of point $m_i$ are given by the $i$-th row of
  $\mathbf{V}$.
\end{enumerate}

\subsection{Laplacian eigenmaps}
\label{sec:laplacian-eigenmaps}
Laplacian eigenmaps \citet{belkin03:_laplac} is another embedding
techniques for high dimensional data that embeds using a system of
generalized eigenvectors of the graph Laplacian. Given a set of $N$
data points $\mathcal{X} = (x_1, x_2, \dots, x_N)$ in some high
dimensional space $\Omega$ with norm $\| \cdot \|$, the Laplacian
eigenmaps algorithm proceeds as follow.
\begin{enumerate}
\item Construct a graph $G = (V,E,\omega)$ over $\mathcal{X}$. Like in
  the case of Isomap in \S \ref{sec:isomap}, there are various ways of
  constructing a $G$, such as $\epsilon$-ball or $K$-NN. We assume
  that the resulting graph $G$ is connected. 
\item Assign a similarity measure $\omega \colon E \mapsto
  \mathbb{R}^{\geq 0}$. Once again, there are various different
  methods. One widely used method is to use the Gaussian
  similarity/heat kernel as given by
  \begin{equation}
    \label{eq:88}
    \omega(x_i,x_j) = e^{-\frac{\|x_i - x_j\|^2}{\delta^2}}
  \end{equation}
  Another method is to make $\omega$ unweighted, i.e. $\omega(x_i,x_j)
  \equiv 1$.
\item Let $\mathbf{L}$ be the combinatorial Laplacian of $G$ with
  similarity measure $\omega$ (see \S \ref{sec:graph-laplacians}. Let
  $\mathbf{D}$ be the diagonal matrix with entries $\mathbf{D}(v,v) =
  \deg(v)$. Compute the eigenvalues and eigenvectors of the following
  generalized eigenvalue problem
  \begin{equation}
    \label{eq:91}
    \mathbf{Lf} = \lambda \mathbf{Df}
  \end{equation}
\item Let $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_N$ be the
  eigenvalues of Eq. (\ref{eq:91}), and $\mathbf{f}_1, \mathbf{f}_2,
  \dots$ be the corresponding eigenvectors. The embedding into
  $\mathbb{R}^{m}$ is given by
  \begin{equation}
    \label{eq:92}
    x_i \rightarrow (\mathbf{f}_2(i), \mathbf{f}_3(i), \dots,
    \mathbf{f}_{m+1}(i))
  \end{equation}
  where $\mathbf{f}_1$ is constant, and thus ignored.  
\end{enumerate}
Part of the motivation of Laplacian eigenmaps is the relationship
between the Laplace-Beltrami operator $\Delta$ on a manifold and the
normalized Laplacian of a graph. Let $\mathcal{M}$ be a compact,
smooth $d$-dimensional Riemannian manifold. Suppose that the data
points $\mathcal{X}$ are sampled uniformly at random from
$\mathcal{M}$. Roughly speaking, if one construct a complete graph $G$
over the data points $\mathcal{X}$ with similarity measure
$\omega(x_i,x_j) = K/N \exp{(\tfrac{\|x_i - x_j\|^2}{4t})}$ where $K$
is a constant depending on the dimension $d$ and the width $t$, then
as $N \rightarrow \infty$, the eigenvalues and eigenvectors of the
Laplacian matrix $L$ converges to the corresponding eigenvalues and
eigenfunctions of the Laplace-Beltrami operator $\Delta$ on
$\mathcal{M}$, see \citet{belkin08:_towar_theor_found_laplac}, and
\citet{belkin06:_conver_laplac_eigen}. \\ \\
%
\noindent The embeddings found by Laplacian eigenmaps can also be
viewed as the solutions to an optimization problem. Let
$\mathbf{Z}$ be a $N \times d$ matrix where $z^{(i)}$, the $i$-th
row of $\mathbf{Z}$, is the embedding coordinates of $x_i \in
\mathcal{X}$. Consider the following optimization problem
\begin{equation}
  \label{eq:89}
  \argmin_{\mathbf{Z}^{T}\mathbf{D} \mathbf{Z} = \mathbf{I}_m}{\,\,
    \mathrm{trace}(\mathbf{Z}^{T} \mathbf{L} \mathbf{Z})}
  = \argmin_{\mathbf{Z}^{T}\mathbf{D} \mathbf{Z} = \mathbf{I}_m}{\,\,
    \sum_{i,j}{ \|z^{(i)} - z^{(j)}\|^{2} \omega_{ij}}}
\end{equation}
where $\mathbf{I}_m$ is the $m \times m$ identity matrix. The
objective function in Eq.~\eqref{eq:89} penalizes mappings where $x_i$
and $x_j$ with high similarity (large $\omega(x_i,x_j)$) are mapped to
$z^{(i)}$ and $z^{(j)}$ that are far apart. The constraint in
Eq.~\eqref{eq:89} is to prevent degenerate solutions. The solution of
the above optimization problem can be found as follows
 \begin{equation}
   \label{eq:90}
   \begin{split}
     \min_{\mathbf{Z}^{T}\mathbf{D} \mathbf{Z} = \mathbf{I}_m}{\,\,
       \mathrm{trace}(\mathbf{Z}^{T} \mathbf{L} \mathbf{Z})} &=
     \min_{\mathbf{Q}^{T}\mathbf{Q} = \mathbf{I}_m}{\,\,
       \mathrm{trace}(\mathbf{Q}^{T} \mathbf{D}^{-1/2} \mathbf{L}
       \mathbf{D}^{-1/2} \mathbf{Q})} \\
     &= \min_{\mathbf{Q}^{T}\mathbf{Q} = \mathbf{I}_m}{\, \,
       \mathrm{trace}(\mathbf{Q}^{T} \mathbf{U}^{T} \bm{\Sigma}
       \mathbf{U} \mathbf{Q})} \\
     &= \min_{\mathbf{V}^{T}\mathbf{V} = \mathbf{I}_m}{\, \,
       \mathrm{trace}(\mathbf{V}^{T} \bm{\Sigma}
       \mathbf{V})} \\
     &= \sum_{k=1}^{m}{\lambda^{\uparrow}_m}
   \end{split}
 \end{equation}
 where $\mathbf{U}^{T} \bm{\Sigma} \mathbf{U}$ is the
 eigendecomposition of $\mathbf{D}^{-1/2} \mathbf{L}
 \mathbf{D}^{-1/2}$ and $\lambda^{\uparrow}_m$ are the diagonal
 entries of $\bm{\Sigma}$, arranged in non-decreasing order. Now
 $\mathbf{D}^{-1/2} \mathbf{L} \mathbf{D}^{-1/2} = \mathbf{D}^{1/2}
 \mathbf{D}^{-1} \mathbf{L} \mathbf{D}^{-1/2}$, and so the eigenvalues
 of $\mathbf{D}^{-1} \mathbf{L}$ coincides with the eigenvalues of
 $\mathbf{D}^{-1/2} \mathbf{L} \mathbf{D}^{-1/2}$. The minimum in
 Eq.~\eqref{eq:90} is thus achieved when the columns of $\mathbf{Z}$
 are the $m$ eigenvectors corresponding to the eigenvalues
 $\lambda_{1}^{\uparrow}, \lambda_{2}^{\uparrow}, \dots,
 \lambda_{m}^{\uparrow}$ of $\mathbf{D}^{-1} \mathbf{L}$. This is
 equivalent to Eq.~\eqref{eq:91}. The embeddings found by Laplacian
 eigenmaps is therefore the solution of the optimization problem in
 Eq.~\eqref{eq:89}.

\subsection{Diffusion maps}
\label{sec:diffusion-maps}
Diffusion maps \citet{coifman06:_diffus_maps} is an embedding
framework based upon diffusion processes. The technique is based on
the eigenvalues and eigenvectors of transition matrices of Markov
chains on graphs, and is closely related to Laplacian eigenmaps as
discussed in \S \ref{sec:laplacian-eigenmaps}. Associated with
diffusion maps is a notion of a family of distances named diffusion
distances. \citet{coifman06:_diffus_maps} viewed the family of
diffusion distances as defining multiscale geometries on the data
set. \\ \\
\noindent
Given a set of $N$ data points $\mathcal{X} = \{x_1,x_2,\dots,x_N\}$,
diffusion maps proceeds as follows (c.f. Isomap \S \ref{sec:isomap}
and Laplacian eigenmaps \S \ref{sec:laplacian-eigenmaps})
\begin{enumerate}
\item Construct a graph $G = (V,E,\omega)$ over $\mathcal{X}$.
\item Generate the transition matrix $\mathbf{P}$ of the random walk
  on $G$, see \S \ref{sec:random-walks-graphs}.
\item Let $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_N$ be the
  eigenvalues of $\mathbf{P}$ and $\mathbf{f}_1, \mathbf{f}_2, \dots,
  \mathbf{f}_N$ be the corresponding eigenvectors. Choose an integer
  $t \geq 0$. $t$ represent the time scale. The embedding 
  $\vartheta_{t,m}$ of $\mathcal{X}$ into $\mathbb{R}^{m}$ is given by
  \begin{equation}
    \label{eq:93}
    \vartheta_{t,m}(x_i) \mapsto \Bigl(\lambda_2^{t} \mathbf{f}_2(i), \lambda_3^{t}
    \mathbf{f}_3(i), \dots, \lambda_{m+1}^{t} \mathbf{f}_{m+1}(i) \Bigr)
 \end{equation}
 where $\mathbf{f}_1$ is constant and thus ignored.
\end{enumerate}
The construction of the graph $G$ is an important part of the
working of diffusion maps. \citet{coifman06:_diffus_maps} argues for
the following approach to constructing $G$. Let $\mathcal{M}$ be a
smooth, compact submanifold of $\mathbb{R}^{n}$ and suppose that
the distribution of points on $\mathcal{M}$ is given by a density
function $q$. Let $h$ be a radial function. Fix $\alpha \in
\mathbb{R}$. The family of anisotropic diffusion kernels is given by
the following procedure
\begin{enumerate}
\item Choose $\epsilon > 0$ and define $k_{\epsilon}(x,y)$, $x,y
  \in \mathcal{M}$ by $k_{\epsilon}(x,y) = h(\| x - y \|^2/\epsilon)$.
\item Let $q_{\epsilon}(x) = \int_{\mathcal{M}}{k_{\epsilon}(x,y) q(y)
    \,d y}$ and
  form a new kernel $k_{\epsilon}^{\alpha}(x,y)$ as
  \begin{equation}
    \label{eq:95}
    k_{\epsilon}^{(\alpha)}(x,y) =
    \frac{k_{\epsilon}(x,y)}{(q_{\epsilon}(x) q_{\epsilon}(y))^{\alpha}}
  \end{equation}
\item Normalize $k_{\epsilon}^{\alpha}$ by setting
  $d_{\epsilon}^{(\alpha)}(x) = \int_{\mathcal{M}}{k_{\epsilon}^{(\alpha)}(x,y) q(y)
    \, d y}$ and define the transition probability
    $p_{\epsilon,\alpha}(x,y)$ by
    \begin{equation}
      \label{eq:96}
      p_{\epsilon,\alpha}(x,y) = \frac{k_{\epsilon}^{\alpha}(x,y)}{d_{\epsilon}^{\alpha}(x,y)}
    \end{equation}
\end{enumerate}
\citet{coifman06:_diffus_maps} then showed that as $\epsilon
\rightarrow 0$, the integral operator on $L^{2}(\mathcal{M},q)$ with
kernel $p_{\epsilon,\alpha}$ converges to different operators for
different choices of $\alpha$. Special cases includes $\alpha = 0$,
$\alpha = 1/2$ and $\alpha = 1$, with the resulting operator being the
Laplace-Beltrami operator, the Fokker-Planck operator, and the heat
kernel, respectively. See also
\citet{nadler05:_diffus_eigen_fokker_planc_operat} and
\citet{nadler06:_diffus}.

\subsection{Some thoughts on Isomap, Laplacian eigenmaps and diffusion maps}
\label{sec:some-thoughts-isomap}
We have seen that common to Isomap, Laplacian eigenmaps, and diffusion
map is the construction of a graph $G$ over the data points.  However,
the question of how to construct a graph $G$ is still unresolved. We
have discussed two well known techniques, namely $\epsilon$-ball and
$K$ nearest neighbours. There exist results that indicates that
different graph construction leads to vastly different results, see,
for example, \citet{maier08:_influen} and \citet{maier09:_optim_k}.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "dissertation"
%%% End: 
