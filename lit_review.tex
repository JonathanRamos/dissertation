\chapter{Introduction}
\label{cha:introduction}

\section{Representation of data}
\label{sec:representation-data}
Machine learning algorithms in general start out with a representation
of the data. We discuss briefly in this section some important
representations of data, namely feature vectors, proximities matrices,
and graphs. We also mention the connections between these
representations. Of particular interest to our work is the
representation of data as graphs. \\ \\
%
%
\noindent An intuitive way of respresenting data is as a matrix where
each row is associated with an object and each column is associated
with a feature. The entries in row $i$-th then comprise a vector of
features that represent the $i$-th object. This representation is
referred to as the \emph{feature vector} representation of the
data. The feature space $\Omega$ is then the set of all possible
feature vectors. If the observed features are all numerical, then the
feature vectors can be considered as points in $\mathbb{R}^{p}$ and
$\Omega$ can be considered as a subset of $\mathbb{R}^{p}$. We assume
in this dissertation that the feature vector representation of data is
synonymous with the representation of data as points in
$\mathbb{R}^{p}$ for some $p$. \\ \\
%
%
\noindent 
Let $\Omega$ be a feature space for data from some arbitrary
domain. Given $x, y \in \Omega$, we might want to talk of the
proximity between $x$ and $y$. A notion of proximity is the
dissimilarity measure. Consider a function $\delta$ on $\Omega
\times \Omega$ satifying the following conditions
\begin{enumerate}[(i)]
\item $\delta(x,y) \geq 0$ for all $x, y \in \Omega$.
\item $\delta(x,x) = 0$ for all $x \in \Omega$.
\item $\delta(x,y) = \delta(y,x)$ for all $x, y \in \Omega$.
\item $\delta(x,y) \leq \delta(x,z) + \delta(z,y)$ for all $x,y,z \in \Omega$.
\item $\delta(x,y) = 0$ if and only if $x = y$. 
\end{enumerate}
$\delta$ is then said to be a dissimilarity measure if conditions (i)
through (iii) are satisfied. $\delta$ is a semi-metric if, in addition
to being a dissimilarity measure, $\delta$ also satisfies condition
(iv). $\delta$ is a metric or distance if $\delta$ satisfies all five
conditions. If $\Omega \subset \mathbb{R}^{p}$, then examples of
distances on $\Omega$ include the $p$-norm distance (Minskowski
distance) for all $p \geq 1$. In any case, given a set of data points
$\mathcal{X} \subset \Omega$ and a dissimilarity measure $\delta$ on
$\Omega \times \Omega$, we can construct the matrix $\Delta =
(\delta(x_i,x_j))_{x_i,x_j \in \mathcal{X}}$. $\Delta$ is then said to
be a \emph{proximity matrix} or, more specifically, a \emph{dissimilarity
  matrix}. \\ \\
%
%
\noindent
Another notion of proximity is the notion of similarity. Consider a
function $\gamma$ on $\Omega \times \Omega$ satisfying the
following conditions
\begin{enumerate}[(i)]
\item $\gamma(x,y) \geq 0$ for all $x,y \in \Omega$.
\item $\gamma(x,x) \geq \gamma(x,y)$ for all $x,y \in \Omega$.
\item $\gamma(x,y) = \gamma(y,x)$ for all $x,y \in \Omega$.
\end{enumerate}
$\gamma$ is then said to be a similarity measure. $\gamma$ is said to
be a \emph{proper} similarity measure if the inequality in (ii) is strict for
all $x,y \in \Omega$, $x \not = y$. $\gamma$ is said to be
\emph{normed} if $\gamma(x,x) = 1$ for all $x \in \Omega$. Given a set
of data points $\mathcal{X} \subset \Omega$, the matrix $\Gamma =
(\gamma(x_i,x_j))_{x_i,x_j \in \mathcal{X}}$ is then said to be a
\emph{similarity matrix}. \\ \\
%
%
\noindent One widely used similarity measure in machine learning is the Gaussian
similarity or heat kernel. Assume that $\Omega \subset \mathbb{R}^{p}$
for some $p$. The Gaussian similarity between $x,y \in \Omega$ is
given by
\begin{equation}
  \label{eq:100}
  \gamma(x,y) = \exp{\Bigl(-\frac{\|x - y\|^{2}}{\sigma^2}\Bigr)}
\end{equation}
for some $\sigma^2 > 0$. The Gaussian similarity is a normed and
proper similarity measure. With an appropriate choice of $\sigma^2$,
it also decreases rapidly as $\|x - y\|^2$ increases. One interesting
feature of the Gaussian similarity is that the resulting similarity
matrix $\Gamma$ is positive semidefinite. This follows from a result
stating that positive definite kernels can be constructed from
element-wise exponentials of conditionally positive definite kernels
\citep{schoenberg38:_metric_spaces_compl_monot_funct,kondor02:_diffus}. \\
\\
%
%
\noindent
Given a set of data points $\mathcal{X}$ from some feature space
$\Omega$, the proximity representation of $\mathcal{X}$ is then a $n
\times n$ similarity matrix $\Gamma$ or dissimilarity matrix $\Delta$
where $n = | \mathcal{X}|$. Obviously, given a feature space
representation for $\mathcal{X}$ and a similarity measure $\gamma$ or
dissimilarity measure $\delta$, we can construct a proximity
representation for $\mathcal{X}$. The proximity representations might
therefore appear superflous. However, there are many application
domains where it's much easier to give a rough estimate of the
proximities between instances instead of the measurements of the
variables of the instances themselves. Another advantage of the
proximity representations is the amount of space needed when the
number of variables far exceeds the number of instances, which is what
happened in application domains that suffer from the curse of
dimensionlality. The space consumption $O(n^2)$ of the proximity
representation is much less than the $O(np)$ of the feature vector
representation when $p \gg n$. Finally, the proximity representations
allow us to leave part of a proximity matrix unspecified if the
proximity measure for those entries are unavailable. For example,
\citet{park07:_hippoc} analyzed hippocampus images of people belonging
to three different groups, namely the clinically depressed, the high
risk and the control group. The proximities in their study is very
computational intensive. It's therefore possible that it's not
desirable or feasible to compute the full proximity matrix for a
larger group of hippocampus images. The above
reasons indicate that the proximity representation is a valid and
useful representation in its own right.
\section{Dimension reduction algorithms}
\label{sec:dimens-reduct-algor}
This section survey some of the existing literature on dimension 
reduction and manifold learning algorithms. 
Dimensionality reduction, to be intentionally vague, is the reduction
of the number of variables describing the data points. Classic
examples of dimensional reduction techniques are principal component
analysis (PCA) \citep{pearson01:_on,hotelling33:_analy} and
multidimensional scaling
\citep{torgesen52:_multid,gower66:_some}. These techniques are viewed
as linear dimensional reduction because the reduced variables are
obtained by projections/linear transformations of the original set of
variables. If we believe that such linear transformations are not
suitable for the task at hand, then new techniques or generalizations
of existing techniques are called for. Some of the proposed techniques
are grouped the umbrella of \emph{non-linear} dimension reduction. For
example, Sammon's mapping \cite{j.69:_nonlin} is an early example of
a non-linear dimension reduction technique. \\ \\
%
\noindent
Some of the early approaches to nonlinear
dimension reduction was through the extension of PCA. In the
1980's, nonlinear extension of PCA, namely principal curves and
surfaces, was proposed by Hastie
\citet{hastie84:_princ,hastie89:_princ}. Meanwhile extension of PCA
using neural networks was also done in the neural networks community,
e.g., \citet{rubner89,oja92:_princ}.  See also the survey article of
\citet{oja02:_unsup} for more information on work in this
area. Recently, other extensions of PCA were also proposed, such as
regularized principal manifold \citet{smola01:_regul_princ_manif} ,
probabilistic principal component analysis \citet{tipping99:_mixtur}
and generative topographic mapping
\citet{bishop98:_gtm}. \\ \\
%
% 
\noindent Since the introduction of Isomap
\citep{tenebaum00:_global_geomet_framew_nonlin_dimen_reduc} and
Locally Linear Embedding \citep{roweis00:_nonlin}, there has been an
explosion of interest in techniques for manifold learning. Manifold
learning is a class of nonlinear dimension reduction algorithms that
are motivated by the assumption that data lies in a high-dimensional
space but is of low intrinsic dimension. Examples of data sets where
this assumption is reasonable includes image data
\citep{tenebaum00:_global_geomet_framew_nonlin_dimen_reduc,bregler95:_nonlin,bregler94:_surfac}
and microarray data \citep{wouters03:_graph}. In image data, the
dimension of an image can be considered to be the number of pixels in
its representation. This makes a small $100$ pixel by $100$ pixel
image to lies in a space of dimension $10^4$. However, in a collection
of images, the subject of the images might be fixed and the only
variable between the images are the directions and placement of the
camera. The images are thus assumed to have a much lower intrinsic
dimension, say $3$, the same as the degrees of freedom of the
camera. In microarray DNA data, the number of observed genes for a
particular test subject could be on the order of $10^4$. On the other
hand, the number of test subjects is much less, often of the order of
$10^2$.
\\ \\
%
%
\noindent
There are many reasons for the interest in manifold learning
algorithm. The first reason is the intuitive geometric motivation of
data points lying on some low-dimensional manifold in high-dimensional
space. Other reasons for the popularity of manifold learning
algorithms might stems from the popularity and success of concepts
such as graph Laplacians, random
walks on graphs, and spectral decomposition of matrices.  \\ \\
%
\noindent The concept of the combinatorial Laplacian of a graph had
been known since the matrix-tree theorem
\citep{kirchhoff47:_uber_aufl_gleic_str}, which stated that the number
of spanning tree of a graph $G$ is equal to any cofactor of the
Laplacian $\mathbf{L}(G)$ of $G$. \citet{fiedler73:_algeb} then noted
that the second smallest eigenvalue of $\mathbf{L}(G)$ is related to a
notion of connectivity on $G$. This lead to interests in
understanding the spectral properties of graph Laplacian. See for
example the book \citet{cvetkovic80:_spect_graph_theor_applic} and the
survey articles \citet{merris94:_laplac,mohar91:_graph}. The
normalized Laplacian $\bm{\mathcal{L}}(G)$ rose in popularity through
the work done by Chung and her coauthors. The monograph
\citet{chung97:_spect_graph_theor} contains an overview of the
normalized Laplacian and its connection with graph theory, harmonic
analysis, and other diverse disciplines. Laplacians matrices of graphs
had also appeared in the context of spectral partitioning and
clustering, e.g.,
\citet{scott90:_featur,shi00:_normal,ng02,weiss9:_segmen}. Laplacian matrices
also appeared in manifold learning, where it was observed that they
share connections with the Laplace-Beltrami operator on Riemannian
manifolds. This lead to the study of convergence of discrete
Laplacians
\citep{hein05:_from,luxburg08:_consis,belkin08:_towar_theor_found_laplac,
  hein07:_conver_laplac,coifman06:_diffus_maps}. The graph Laplacians
also appeared in the context of semi-supervised learning where they
were used as regularization terms
\citep{zhu03:_semi_super_learn_using_gauss,belkin06:_manif_regul,zhu05:_semi_super}. \\ \\
%
\noindent
Random walks had been a topic of interest to the scientific community
for the last hundred plus years. Karl Pearson asked in
1905, in an issue of
Nature, for help in finding references or solution to the following
problem
\begin{verbatim}
A man starts from a point O and walks l yards in a
straight line; he then turns through any angle whatever 
and walks another l yards in a second straight line. He 
repeats this process n times. I require the probability 
that after these n stretches he is at a distance between
r and r + dr from his starting point, O.
\end{verbatim}
After hearing from Lord Rayleigh that his problem is the same as that
of the composition of $n$ iso-periodic vibrations of unit amplitudes
and random phases, where the probability that the composition deviates
from $0$ is exponentially small, Karl Pearson quipped that ``in open
country the most probable place to find a drunken man who is at all
capable of keeping on his feet is somewhere near his starting point''
\footnote{\url{http://www.nature.com/physics/looking-back/pearson/index.html}}.
Random walks on graphs had appeared in connection with electrical
networks
\citep{doyle84:_random_walks_elect_networ,lyons:_probab_trees_networ}. Random
walks on graphs are examples of Markov chains and the study of mixing
time through such notions as cover time of graphs was also performed
\citep{lovasz96:_random_graph,levin09:_markov_chain_mixin_times}. Recently,
random walks on graphs had been in widespread use in machine
learning. For example, random walks are used in clustering
\citep{saerens04,yen07:_graph,qui07:_clust}, semi-supervised learning
\citep{szummer01:_partial_markov,zhou04:_learn,zhou04:_learn_label_unlab,zhu03:_semi_super_learn_using_gauss},
and graphs sparsification
\citep{spielmand08:_graph}. \\ \\
%
\noindent
Spectral decomposition is at the heart of many techniques in machine
learning. As we have alluded to in our discussion of graph Laplacians,
the area of spectral partitioning and clustering is based on the idea
that the eigenvectors of various matrix representation of data points,
e.g., the combinatorial Laplacian, provide a meaningful way to
partition or cluster the data points. Analogously, a significant portion
of manifold learning algorithm embed data points by using eigenvalues
and eigenvectors of some matrix representation of the data. With the
transition to very large data sets, a lot of work had been done in
approximate spectral decomposition. \footnote{Need reference/citation}
\\ \\
% We
% think that the terminilogy of ``non-linear dimensionality reduction''
% is unfortunate since it's not entirely clear what non-linear means in
% this context. A significant portion of non-linear dimensionality
% reduction algorithm obtained the embedding coordinates by performing
% spectral decomposition of some matrix and then projecting onto the
% first $d$ columns of the orthogonal matrix in the resulting
% decomposition, which is a linear transformation. We think that it's
% more accurate to refer to these techniques as \emph{non-Euclidean}
% dimensionality reduction. The non-Euclidean refers to the aspect
% that, for a lot of these techniques, Euclidean distance or inner
% product in Euclidean space are not appropriate measure of proximities
% between the data points. For example, kernel PCA
% \citep{scholkopf97:_lectur_notes_comput_scien} extends PCA by
% replacing the covariance matrix, which is an Euclidean inner product
% matrix, with other notions of inner product matrices. In the case of
% Isomap \citep{tenebaum00:_global_geomet_framew_nonlin_dimen_reduc},
% the Euclidean distance between points are replaced by shortest path
% distances on some underlying graphs, in the hope that these shortest
% path distances approximate the geodesic distance on some manifold.
%\\ \\
%
%
The structure for this section is as follow. We discuss the classic
dimension reduction technique, namely PCA and classical MDS in
 \S~\ref{sec:princ-comp-analys} and \S~\ref{sec:classical-mds}. 
 We then review several well known manifold learning
algorithms, namely Isomap, LLE, Hessian eigenmaps, Laplacian
eigenmaps, and diffusion maps, in \S \ref{sec:isomap} through \S
\ref{sec:diffusion-maps}. Some of the techniques, such as Isomap,
Laplacian eigenmaps \citep{belkin03:_laplac}, and diffusion maps
\citep{coifman06:_diffus_maps} can be understood as examples of the
following recipe: (i) Represent the data as a weighted (or possibly
unweighted) graph with vertices corresponding to feature vectors and
edge weights representing pairwise proximities in the ambient feature
space; (ii) Compute new proximities between the vertices. (iii) Embed
the graph into a Euclidean space using the new proximities. The
embedding is typically into a space of low dimension. Other
techniques, such as Locally Linear Embedding, Hessian eigenmaps
\citep{donoho03:_hesian}, LTSA
\citep{zhang03:_intel_data_engin_autom_learn} and GNA
\citep{brand05:_from} strive to obtain a global coordinate system that
preserves the neighbourhood structure of the data points. This is done
by computing, for each data point $p$, an approximation of the tangent
space $T_p$ and then aligning the tangent spaces so as to minimize a
loss function. The tangent spaces represent the neighbourhood
structure at each point and their alignment give a global coordinate
system to embed the data points. For additional survey of manifold
learning and dimension reduction algorithms, including some that
are not discussed here, refer to the survey articles of
\citet{burges05:_data} and
\citet{saul06:_semis}. 

% \\
% \\
% %
% %
% \noindent
% The outline of this section is then as follows. We discuss classical
% MDS and PCA in \S \ref{sec:classical-mds}. \S \ref{sec:isomap} through
% \S \ref{sec:diffusion-maps} summarized some of the key ideas of
% well-known manifold learning algorithms, namely Isomap, LLE, Hessian
% eigenmaps, Laplacian eigenmaps, and diffusion maps.
\subsection{Principal Component Analysis}
\label{sec:princ-comp-analys}
Principal component analysis (PCA) is arguably one of the most popular
dimension reduction technique and it's used to analyze data in
many scientific disciplines. PCA can be viewed as a technique
for projection pursuits, i.e., the search for interesting projections
of the data. A significant number of nonlinear dimension 
reduction techniques are either extensions of PCA or employed PCA in
the construction of their embeddings and it is therefore important that
we are acquainted with the ideas behind PCA. This subsection summarize
the PCA procedure. For a more comprehensive overview of PCA, see the
text of \citet{jolliffe02:_princ_compon_analy}. \\ \\
%
Let $\mathbf{X}$ be an $n \times p$ matrix where each row of
$\mathbf{X}$ is a data point in $\mathbb{R}^{p}$.\footnote{Sometimes
  PCA is described in the literature using the matrix $\mathbf{Y} =
  \mathbf{X}^{T}$ instead of $\mathbf{X}$. The covariance matrix is
  then $1/n\tilde{\mathbf{Y}}\tilde{\mathbf{Y}}^{T}$.} The procedure
for PCA can be summarized by the following steps.
\begin{enumerate}
\item Compute the centered data matrix $\tilde{\mathbf{X}}$ as
  \begin{equation}
    \label{eq:97}
    \tilde{\mathbf{X}} = \mathbf{X}\Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr)
  \end{equation}
  Each column of $\tilde{\mathbf{X}}$ sums to $0$. Each row of
  $\tilde{\mathbf{X}}$ can be interpreted as the difference between
  a data point and the data points' centroid. 
\item Compute the covariance matrix $\mathbf{S} =
  \frac{1}{n}\tilde{\mathbf{X}}^{T}\tilde{\mathbf{X}}$ \footnote{The
    scale factor $1/n$ in $\mathbf{S}$ can also be replaced by
    $1/(n-1)$, in which case the resulting matrix might be refered to
    in the literature as the sample covariance matrix. The 
    principal components are identical in both cases since they are
    normalized to have unit length.}.
\item Find the spectral decomposition of $\mathbf{S}$ as $\mathbf{S} =
  \mathbf{V} \bm{\Lambda} \mathbf{V}^{T}$. The matrix $\mathbf{V}$ is
  a new basis for representing the centered data points $\tilde{\mathbf{X}}$,
  i.e., we project $\tilde{\mathbf{X}}$ onto the space spanned by
  $\mathbf{V}$.  If we arrange the eigenvalues of $\mathbf{S}$ as
  $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_n$ and denote by
  $\mathbf{f}_1, \mathbf{f}_2, \dots, \mathbf{f}_n$ the corresponding
  eigenvectors, then the $i$-th principal component is given by
  $\tilde{\mathbf{X}}\mathbf{f}_i$, normalized to have unit length.
\end{enumerate}
To get a $d$ dimensional representation of $\mathbf{X}$, PCA uses the
vectors $\tilde{\mathbf{X}}\mathbf{f}_1$ through
$\tilde{\mathbf{X}}\mathbf{f}_d$. The \emph{total variation} of the
resulting embedding is given by
$(\sum_{k=1}^{d}{\lambda_k})/\mathrm{trace}(\Lambda)$. The total
variation attempts to quantify how much of the variance of the data
points are described by
the $d$ dimensional representation. \\ \\
%
%
\noindent
We can also describe PCA using the singular value decomposition (SVD)
of $\tilde{\mathbf{X}}$. If $\tilde{\mathbf{X}} = \mathbf{U}
\bm{\Sigma} \mathbf{V}^{T}$ then
$\tilde{\mathbf{X}}^{T}\tilde{\mathbf{X}} = \mathbf{V} \bm{\Sigma}^{2}
\mathbf{V}$ and $\tilde{\mathbf{X}}\tilde{\mathbf{X}}^{T} =
\mathbf{U}\bm{\Sigma}^{2} \mathbf{U}^{T}$. Therefore,
$\tilde{\mathbf{X}}\mathbf{V} = \mathbf{U}\mathbf{\Sigma}$ and the
principal components are the eigenvectors of
$\tilde{\mathbf{X}}\mathbf{X}^{T}$. We can thus approach PCA using
either the covariance matrix
$\tilde{\mathbf{X}}^{T}\tilde{\mathbf{X}}$ or the inner product matrix
$\tilde{\mathbf{X}}\tilde{\mathbf{X}}^{T}$. Kernel PCA
\citep{scholkopf97:_lectur_notes_comput_scien} extends PCA by
replacing the inner product matrix
$\tilde{\mathbf{X}}\tilde{\mathbf{X}}^{T}$ with an arbitrary inner
product matrix $\mathbf{B}$.
\subsection{Classical multidimensional scaling}
\label{sec:classical-mds}
Classical multidimensional scaling \citep{torgesen52:_multid,gower66:_some}
constructs an embedding $\mathbf{X}$ of a dissimilarity matrix
$\Delta$ in a $d$-dimensional Euclidean space by finding a coordinate matrix
$\mathbf{X}$ such that $\mathbf{X}\mathbf{X}^{T}$ is the best rank $d$
approximation to the matrix $\tau(\Delta \circ \Delta)$. The
procedure for classical multidimensional scaling is summarized by
the following steps.
\begin{enumerate}
\item Compute the matrix of squared dissimilarities $\Delta^{(2)} =
  \Delta \circ \Delta$.
\item Compute $\mathbf{B}(\Delta) = \tau(\Delta^{(2)})$ where the
  $\tau$ transform is as defined in \S
  \ref{sec:distance-geometry}. Note that if $\Delta$ is EDM-1, then
  $\mathbf{B}(\Delta)$ is positive semidefinite. 
\item Compute the eigendecomposition of $\mathbf{B}(\Delta)$ as
  $\mathbf{B}(\Delta) = \mathbf{Q} \bm{\Lambda} \mathbf{Q}^{T}$ where
  $\bm{\Lambda} = \mathrm{diag}(\lambda_1, \lambda_2, \dots,
  \lambda_n)$ and $\lambda_1 \geq \lambda_2 \geq \dots \geq
  \lambda_n$.  
\item Set $\mathbf{Q}_d$ to be the matrix formed by the first $d$
  columns of $\mathbf{Q}$. Set $\bm{\Lambda_{d}} =
  \mathrm{diag}(\bar{\lambda}_1,\bar{\lambda}_2, \dots,
  \bar{\lambda}_d)$ where $\bar{\lambda}_i = \max(\lambda_i, 0)$. The
  matrix $\mathbf{X}_d$ of embedding coordinates is given by
  $\mathbf{Q}_d \mathbf{\Lambda}_{d}^{1/2}$.
\end{enumerate}
 By a result in \citet{eckart36:_approx}, $\mathbf{X}_d$ minimizes the
following loss function
\begin{equation}
  \label{eq:87}
 L(\mathbf{Z}) = \| \mathbf{Z} \mathbf{Z}^{T} - \mathbf{B}(\Delta)
 \|_F^{2} 
\end{equation}
subjected to the rank constraint $\mathrm{rank}(\mathbf{Z}) \leq
d$. The solution $\mathbf{X}_d$ is unique up to a rotation, i.e., if
$\mathbf{Y}$ also minimizes Eq. \eqref{eq:87} with
$\mathrm{rank}(\mathbf{X}_d) = \mathrm{rank}(Y)$, then $\mathbf{Y} =
\mathbf{U}\mathbf{X}_d$ for some unitary matrix $\mathbf{U}$. A nice
property of classical multidimensional scaling is that the dimensions
are nested, i.e., the first $k$ dimensions of an optimal $d$-dimension
embedding, with $k < d$, is also an optimal $k$-dimension
embedding. For more on classical multidimensional scaling, see
\citet{borg05:_moder}.
%
%


\subsection{Isomap}
\label{sec:isomap}
Isomap \citep{tenebaum00:_global_geomet_framew_nonlin_dimen_reduc} was
introduced as a framework for non-linear dimension reduction. One
way of interpreting Isomap is as the application of classical MDS to
the matrix $\bm{\Delta}$ where each entry of $\bm{\Delta}$ corresponds
to the shortest path distance between nodes on some underlying
graph. More specifically, given a set of $N$ data points $\mathcal{X}
= \{x_1, x_2, \dots, x_N\}$ in some feature space $\Omega$,
the Isomap algorithm proceeds as follows
\begin{enumerate}
\item Construct a graph $G$ over $\mathcal{X}$. There are many ways
  for constructing a graph $G$. For example, $G$ can be a
  $\epsilon$-ball graph, i.e., $x_i$ and $x_j$ is connected if $\|x_i
  - x_j\| < \epsilon$ where $\| \cdot \|$ is the Euclidean norm, or it
  can be a $k$-NN graph for some $k$. The dissimilarity associated
  with an edge between $x_i$ and $x_j$ is then $\|x_i - x_j \|$.
\item Compute the matrix $\Delta$ of shortest path distances between
  all pairs $x_i, x_j \in \mathcal{X}$
\item Construct a $m$-dimensional embedding of $\mathcal{X}$ by
  applying classical multidimensional scaling to $\Delta$.  
\end{enumerate}
Let $\mathcal{M}$ be a compact $d$-dimensional manifold. Suppose that
the points in $\mathcal{X}$ are sampled from $\mathcal{M}$. Isomap is
based on the assumption that, under appropriate constructions of the
graph $G$, the shortest path distance between $x_i$ and $x_j$ on $G$
converges to the geodesic distance between $x_i$ and $x_j$ on
$\mathcal{M}$, as the number of sampled points increases to
infinity. \citet{bernstein00:_graph} discussed some of the theoretical
properties of Isomap including a proof of the asymptotic convergence
to geodesic distances. \citet{silva02:_global} discussed improving the
running time of Isomap by embedding the matrix of shortest path
distances using landmark MDS. As had been observed by
\citet{brand05:_chart}, among others, Isomap and other dimension
reduction algorithms discussed in this section are embedding methods,
i.e., they don't learn a mapping function from the domain of the data
points to the embedding space. This can be circumvented somewhat by
the use of out-of-sample extensions, which are methods for embedding
the data points that are not in the sample. Out-of-sample extensions for
Isomap, LLE, MDS, and Laplacian eigenmaps exist and can be found in
\citet{bengio04:_out_lle_isomap_mds_eigen,trosset08}.
%
%
\subsection{Locally Linear Embedding}
\label{sec:locally-line-embedd}
Locally linear embedding \citep{roweis00:_nonlin} is a dimension 
reduction algorithm that attempts to compute low dimensional,
neighbourhood preserving embeddings through linear reconstructions of a
point from its neighbours. Let $\mathcal{X} = \{x_1,x_2, \dots, x_N\}$
be $N$ data points with each $x_i \in \mathbb{R}^{D}$. Let
$\mathbf{X}$ be the $N \times D$ matrix with each row $i$ being the
coordinates of $x_i$. The LLE procedure is summarized by the following steps.
\begin{enumerate}
\item Choose a parameter $k$ representing the size of the
  neighbourhoods of the data points. For each $x_i \in \mathcal{X}$,
  find its $K$ nearest neighbours and denote the resulting set of data
  points as $\mathcal{N}_k(x_i)$.
\item Let $\mathbf{W} = (w_{ij})$ be the $N \times N$ matrix that is
  the solution of the following optimization problem
  \begin{align*}
    \min_{\mathbf{W}} & \quad \| \mathbf{X} - \mathbf{W}\mathbf{X} \|_F^{2} \\
    \text{subject to:} & \quad w_{ij} = 0 \,\, \text{if $x_j \not \in
      \mathcal{N}_K(x_i)$} \\
    & \quad \sum_{j=1}^{N}{w_{ij}} = 1
  \end{align*}
  $\mathbf{W}$ is the matrix of reconstruction weights with $w_{ij}$
  for $w_{ij} \not = 0$ summarizing the contribution of $x_j \in
  \mathcal{N}_K(x_i)$ to the reconstruction of $x_i$.
\item Let $\mathbf{Y}$ be the $N \times d$ matrix of embedding
  coordinates. $\mathbf{Y}$ is then the solution of the following
  optimization problem
  \begin{equation}
    \label{eq:94}
    \min_{\mathbf{Y}^{T}\mathbf{Y} =
      \mathbf{I}}\|(\mathbf{I} - \mathbf{W})\mathbf{Y}\|_{F}^{2}
  \end{equation}
\end{enumerate}
Let $\mathbf{M} = (\mathbf{I} - \mathbf{W})^{T}(\mathbf{I} -
\mathbf{W})$. If $\lambda_1 \leq \lambda_2 \leq \dots \leq \lambda_N$
are the eigenvalues of $\mathbf{M}$, then the solution of the
optimization problem in Eq.~\eqref{eq:94} is the matrix $\mathbf{Y}$
that's composed of the $d$ eigenvectors corresponding to the
eigenvalues $\lambda_1$ through $\lambda_{d}$. This lead \citet{ham04}
to view locally linear embedding as a version of kernel PCA
\citep{scholkopf97:_lectur_notes_comput_scien} with the kernel given
by the matrix $\mathbf{M}$.
%
%
\subsection{Hessian eigenmaps}
\label{sec:hessian-eigenmaps}
Hessian eigenmaps \citep{donoho03:_hesian} was described as a locally
linear embedding technique for high-dimensional data,
c.f. \citep{roweis00:_nonlin} and \S
\ref{sec:locally-line-embedd}. Hessian eigenmaps is motivated by the
assumption that the data points lies on a $d$-dimensional Riemannian
manifold $\mathcal{M}$. For $f \colon \mathcal{M} \mapsto \mathbb{R}$,
define a quadratic form $\mathcal{H}(f)$ associated with $f$ by
$\mathcal{H}(f) = \int_{M}{ \| H_f(m) \|_{F}^{2} dm }$ where $H_f(m)$
is the Hessian of $f$ at $m$ and $\| \cdot \|_F$ is the Frobenius
norm. $\mathcal{H}(f)$ averages the Frobenius norm of the Hessian of
$f$ over $\mathcal{M}$. The embedding coordinates
corresponds to the basis for the null space of $\mathcal{H}(f)$. \\ \\
\noindent
Let $\{m_i\}_{i=1}^{N}$ be a collection of $N$ points in
$\mathbb{R}^{n}$. Suppose that $K$ is a parameter chosen to be the
size of the nearest neighborhoods and $d$ is a parameter chosen to be
the dimension of the embedding coordinates. The Hessian eigenmaps
procedure proceed as follows \citet{donoho03:_hesian}
\begin{enumerate}
\item Identify neighbors. For each $m_i$, set $\mathcal{N}(m_i)$ to be
  the set of the $K$ nearest neighbours of $m_i$. For each
  neighbourhood $\mathcal{N}(m_i)$, set $\mathbf{M}^{(i)}$ to be the
  $K \times n$ matrix whose rows are the points $m_j \in
  \mathcal{N}(m_i)$. Define $\bar{\mathbf{M}}^{(i)}$ to be
  $(\mathbf{I} - \mathbf{J}/K)\mathbf{M}^{(i)}$,
  i.e. $\bar{\mathbf{M}}^{(i)}$ is the row centering of
  $\mathbf{M}^{(i)}$.
\item Do principal component analysis (PCA) on
  $\bar{\mathbf{M}}^{(i)}$. This is equivalent to finding the singular
  value decomposition (SVD) of $\bar{\mathbf{M}}^{(i)} = \mathbf{U}^{(i)}
  \bm{\Lambda}^{(i)} \mathbf{V}^{(i)}$ and using the first $d$ columns of
  $\mathbf{U}^{(i)}$ as the tangent coordinates of points in
  $\mathcal{N}(m_i)$. 
\item Let $f \colon \mathcal{M} \mapsto \mathbb{R}$ be a smooth
  function. Denote by $\bm{\nu}^{(i)} \in \mathbb{R}^{K}$ the vector
  consisting of $f(m_j)$ for $m_j \in \mathcal{N}(m_i)$. The Hessian
  estimator for $\mathcal{N}(m_i)$ is a matrix $\mathbf{H}^{(i)}$ such
  that $\mathbf{H}^{(i)} \bm{\nu}^{(i)}$ approximates the entries of
  the Hessian matrix of $f$ at $m_i$, i.e.,
  \begin{equation}
    \label{eq:84}
    \mathbf{H}^{(i)} \bm{\nu}^{(i)} \approx {\left[ \begin{array}{ccccc}
          \frac{\partial^2 f}{\partial u_1 \partial u_1} \bigl |_{m_i} &
          \frac{\partial^2 f}{\partial u_1 \partial u_2} \bigl |_{m_i} &
          & \dots & \frac{\partial^2 f}{\partial u_{d} \partial u_d}
          \bigl|_{m_i} 
\end{array} \right ]}^{T}
  \end{equation}
  where $u_1, u_2, \dots, u_d$ denotes a orthonormal basis for the
  points in $\mathcal{N}(m_i)$. $\mathbf{H}^{(i)}$ is a matrix of size
  $d(d+1)/2 \times K$ and can be found as follows. Consider the
  truncated second-order Taylor series expansion for $f$ around $m_i$
  \begin{gather}
    \label{eq:85}
    f(m_i) + \sum_{r=1}^{d}{b_r u_r} + \sum_{r=1}^{d}\sum_{s=r}^{d}{a_{rs} u_r u_s}
    \intertext{where}
    b_r = \frac{\partial{f}}{\partial u_r} \Bigl|_{m_i} , \qquad a_{rs}
    = \frac{1}{2}\frac{\partial^2 f}{\partial u_r \partial u_s} \Bigl|_{m_i}
  \end{gather}
  The truncated polynomial in Eq.~\eqref{eq:85} can be fitted using
  least squares \citep{kim09:_semi_regres_hessian})
  \begin{equation}
    \label{eq:86}
    \min_{\bm{w} \in \mathbb{R}^{p}} \sum_{j=1}^{K} \Bigl( (f(m_i) -
    f(m_j)) - (\mathbf{X}^{(i)} \bm{w})(j)\Bigr)^2
  \end{equation}
  where $p = 1 + d + d(d+1)/2$ and $\mathbf{X}^{(i)}$ is the $K \times
  p$ matrix with the first $d+1$ columns of $\mathbf{X}^{(i)}$ being
  the columns of $\bm{1}$ and the first $d$ columns of
  $\mathbf{U}^{(i)}$, and the remaining $d(d+1)/2$ columns of
  $\mathbf{X}^{(i)}$ being the element-wise products between pairs of
  the first $d$ columns of $\mathbf{U}^{(i)}$. The solution to
  Eq.~\eqref{eq:86} is given by $\bm{w} =
  (\mathbf{X}^{(i)})^{\dagger}f$. The matrix $\mathbf{H}^{(i)}$ is
  then the transpose of the last $d(d+1)/2$ columns of
  $(\mathbf{X}^{(i)}  )^{\dagger}$. If we want $\mathbf{H}^{(i)}$ to
  have orthnormal columns, then $\mathbf{H}^{(i)}$ coincides with the
  transpose of the last $d(d+1)/2$ columns of the matrix $\mathbf{Q}$
  in the QR decomposition of $\mathbf{X}^{(i)}$.
\item Build a symmetric matrix $\mathbf{H} =
  \sum_{i=1}^{N}{(\mathbf{H}^{(i)})^{T}\mathbf{H}^{(i)}}$. If
  $\bm{\nu} = (f(m_j))_{j=1}^{N}$ is a sample vector of $f$, then
  $\bm{\nu}^{T} \mathbf{H} \bm{\nu}$ is an approximation of
  $\mathcal{H}(f)$. 
\item Perform an eigendecomposition of $\mathbf{H}$ and identify the
  $d$ eigenvectors of $\mathbf{H}$ corresponding to the second
  smallest eigenvalues up to the $d+1$ smallest eigenvalues. Let
  $\mathbf{V}$ be the matrix with these eigenvectors as columns. The
  embedding coordinates of point $m_i$ are given by the $i$-th row of
  $\mathbf{V}$.
\end{enumerate}

\subsection{Laplacian eigenmaps}
\label{sec:laplacian-eigenmaps}
Laplacian eigenmaps \citep{belkin03:_laplac} is another embedding
techniques for high dimensional data that embeds using a system of
generalized eigenvectors of the graph Laplacian. \citet{shi97:_normal}
also suggested using the same system of generalized eigenvectors for
spectral clustering since the resulting clustering solution is an
approximate solution of NCut, an NP-hard optimization problem. Given a
set of $N$ data points $\mathcal{X} = (x_1, x_2, \dots, x_N)$ in some
high dimensional space $\Omega$ with norm $\| \cdot \|$, the Laplacian
eigenmaps algorithm proceeds as follow.
\begin{enumerate}
\item Construct a graph $G = (V,E,\omega)$ over $\mathcal{X}$. Like in
  the case of Isomap in \S \ref{sec:isomap}, there are various ways of
  constructing a $G$, such as $\epsilon$-ball or $K$-NN. We assume
  that the resulting graph $G$ is connected. 
\item Assign a similarity measure $\omega \colon E \mapsto
  \mathbb{R}^{\geq 0}$. Once again, there are various different
  methods. One widely used method is to use the Gaussian
  similarity/heat kernel as given by
  \begin{equation}
    \label{eq:88}
    \omega(x_i,x_j) = e^{-\frac{\|x_i - x_j\|^2}{\delta^2}}
  \end{equation}
  Another method is to make $\omega$ unweighted, i.e. $\omega(x_i,x_j)
  \equiv 1$.
\item Let $\mathbf{L}$ be the combinatorial Laplacian of $G$ with
  similarity measure $\omega$ (see \S \ref{sec:graph-laplacians}. Let
  $\mathbf{D}$ be the diagonal matrix with entries $\mathbf{D}(v,v) =
  \deg(v)$. Compute the eigenvalues and eigenvectors of the following
  generalized eigenvalue problem
  \begin{equation}
    \label{eq:91}
    \mathbf{Lf} = \lambda \mathbf{Df}
  \end{equation}
\item Let $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_N$ be the
  eigenvalues of Eq. (\ref{eq:91}), and $\mathbf{f}_1, \mathbf{f}_2,
  \dots$ be the corresponding eigenvectors. The embedding into
  $\mathbb{R}^{m}$ is given by
  \begin{equation}
    \label{eq:92}
    x_i \rightarrow (\mathbf{f}_2(i), \mathbf{f}_3(i), \dots,
    \mathbf{f}_{m+1}(i))
  \end{equation}
  where $\mathbf{f}_1$ is constant, and thus ignored.  
\end{enumerate}
Part of the motivation of Laplacian eigenmaps is the relationship
between the Laplace-Beltrami operator $\Delta$ on a manifold and the
normalized Laplacian of a graph. Let $\mathcal{M}$ be a compact,
smooth $d$-dimensional Riemannian manifold. Suppose that the data
points $\mathcal{X}$ are sampled uniformly at random from
$\mathcal{M}$. Roughly speaking, if one construct a complete graph $G$
over the data points $\mathcal{X}$ with similarity measure
$\omega(x_i,x_j) = K/N \exp{(\tfrac{\|x_i - x_j\|^2}{4t})}$ where $K$
is a constant depending on the dimension $d$ and the width $t$, then
as $N \rightarrow \infty$, the eigenvalues and eigenvectors of the
Laplacian matrix $L$ converges to the corresponding eigenvalues and
eigenfunctions of the Laplace-Beltrami operator $\Delta$ on
$\mathcal{M}$, see \citet{belkin08:_towar_theor_found_laplac}, and
\citet{belkin06:_conver_laplac_eigen}. \\ \\
%
%
\noindent The embeddings found by Laplacian eigenmaps can also be
viewed as the solutions to an optimization problem. Let
$\mathbf{Z}$ be a $N \times d$ matrix where $z^{(i)}$, the $i$-th
row of $\mathbf{Z}$, is the embedding coordinates of $x_i \in
\mathcal{X}$. Consider the following optimization problem
\begin{equation}
  \label{eq:89}
  \argmin_{\mathbf{Z}^{T}\mathbf{D} \mathbf{Z} = \mathbf{I}_m}{\,\,
    \mathrm{trace}(\mathbf{Z}^{T} \mathbf{L} \mathbf{Z})}
  = \argmin_{\mathbf{Z}^{T}\mathbf{D} \mathbf{Z} = \mathbf{I}_m}{\,\,
    \sum_{i,j}{ \|z^{(i)} - z^{(j)}\|^{2} \omega_{ij}}}
\end{equation}
where $\mathbf{I}_m$ is the $m \times m$ identity matrix. The
objective function in Eq.~\eqref{eq:89} penalizes mappings where $x_i$
and $x_j$ with high similarity (large $\omega(x_i,x_j)$) are mapped to
$z^{(i)}$ and $z^{(j)}$ that are far apart. The constraint in
Eq.~\eqref{eq:89} is to prevent degenerate solutions. The solution of
the above optimization problem can be found as follows
 \begin{equation}
   \label{eq:90}
   \begin{split}
     \min_{\mathbf{Z}^{T}\mathbf{D} \mathbf{Z} = \mathbf{I}_m}{\,\,
       \mathrm{trace}(\mathbf{Z}^{T} \mathbf{L} \mathbf{Z})} &=
     \min_{\mathbf{Q}^{T}\mathbf{Q} = \mathbf{I}_m}{\,\,
       \mathrm{trace}(\mathbf{Q}^{T} \mathbf{D}^{-1/2} \mathbf{L}
       \mathbf{D}^{-1/2} \mathbf{Q})} \\
     &= \min_{\mathbf{Q}^{T}\mathbf{Q} = \mathbf{I}_m}{\, \,
       \mathrm{trace}(\mathbf{Q}^{T} \mathbf{U}^{T} \bm{\Sigma}
       \mathbf{U} \mathbf{Q})} \\
     &= \min_{\mathbf{V}^{T}\mathbf{V} = \mathbf{I}_m}{\, \,
       \mathrm{trace}(\mathbf{V}^{T} \bm{\Sigma}
       \mathbf{V})} \\
     &= \sum_{k=1}^{m}{\lambda^{\uparrow}_m}
   \end{split}
 \end{equation}
 where $\mathbf{U}^{T} \bm{\Sigma} \mathbf{U}$ is the
 eigendecomposition of $\mathbf{D}^{-1/2} \mathbf{L}
 \mathbf{D}^{-1/2}$ and $\lambda^{\uparrow}_m$ are the diagonal
 entries of $\bm{\Sigma}$, arranged in non-decreasing order. Now
 $\mathbf{D}^{-1/2} \mathbf{L} \mathbf{D}^{-1/2} = \mathbf{D}^{1/2}
 \mathbf{D}^{-1} \mathbf{L} \mathbf{D}^{-1/2}$, and so the eigenvalues
 of $\mathbf{D}^{-1} \mathbf{L}$ coincides with the eigenvalues of
 $\mathbf{D}^{-1/2} \mathbf{L} \mathbf{D}^{-1/2}$. The minimum in
 Eq.~\eqref{eq:90} is thus achieved when the columns of $\mathbf{Z}$
 are the $m$ eigenvectors corresponding to the eigenvalues
 $\lambda_{1}^{\uparrow}, \lambda_{2}^{\uparrow}, \dots,
 \lambda_{m}^{\uparrow}$ of $\mathbf{D}^{-1} \mathbf{L}$. This is
 equivalent to Eq.~\eqref{eq:91}. The embeddings found by Laplacian
 eigenmaps is therefore the solution of the optimization problem in
 Eq.~\eqref{eq:89}.
%
%
\subsection{Diffusion maps}
\label{sec:diffusion-maps}
Diffusion maps \citet{coifman06:_diffus_maps} is an embedding
framework based upon diffusion processes. The technique is based on
the eigenvalues and eigenvectors of transition matrices of Markov
chains on graphs, and is closely related to Laplacian eigenmaps as
discussed in \S \ref{sec:laplacian-eigenmaps}. Associated with
diffusion maps is a notion of a family of distances named diffusion
distances. \citet{coifman06:_diffus_maps} viewed the family of
diffusion distances as defining multiscale geometries on the data
set. \\ \\
\noindent
Given a set of $N$ data points $\mathcal{X} = \{x_1,x_2,\dots,x_N\}$,
diffusion maps proceeds as follows (c.f. Isomap \S \ref{sec:isomap}
and Laplacian eigenmaps \S \ref{sec:laplacian-eigenmaps})
\begin{enumerate}
\item Construct a graph $G = (V,E,\omega)$ over $\mathcal{X}$.
\item Generate the transition matrix $\mathbf{P}$ of the random walk
  on $G$, see \S \ref{sec:random-walks-graphs}.
\item Let $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_N$ be the
  eigenvalues of $\mathbf{P}$ and $\mathbf{f}_1, \mathbf{f}_2, \dots,
  \mathbf{f}_N$ be the corresponding eigenvectors. Choose an integer
  $t \geq 0$. $t$ represent the time scale. The embedding 
  $\vartheta_{t,m}$ of $\mathcal{X}$ into $\mathbb{R}^{m}$ is given by
  \begin{equation}
    \label{eq:93}
    \vartheta_{t,m}(x_i) \mapsto \Bigl(\lambda_2^{t} \mathbf{f}_2(i), \lambda_3^{t}
    \mathbf{f}_3(i), \dots, \lambda_{m+1}^{t} \mathbf{f}_{m+1}(i) \Bigr)
 \end{equation}
 where $\mathbf{f}_1$ is constant and thus ignored.
\end{enumerate}
The construction of the graph $G$ is an important part of the
working of diffusion maps. \citet{coifman06:_diffus_maps} argues for
the following approach to constructing $G$. Let $\mathcal{M}$ be a
smooth, compact submanifold of $\mathbb{R}^{n}$ and suppose that
the distribution of points on $\mathcal{M}$ is given by a density
function $q$. Let $h$ be a radial function. Fix $\alpha \in
\mathbb{R}$. The family of anisotropic diffusion kernels is given by
the following procedure
\begin{enumerate}
\item Choose $\epsilon > 0$ and define $k_{\epsilon}(x,y)$, $x,y
  \in \mathcal{M}$ by $k_{\epsilon}(x,y) = h(\| x - y \|^2/\epsilon)$.
\item Let $q_{\epsilon}(x) = \int_{\mathcal{M}}{k_{\epsilon}(x,y) q(y)
    \,d y}$ and
  form a new kernel $k_{\epsilon}^{\alpha}(x,y)$ as
  \begin{equation}
    \label{eq:95}
    k_{\epsilon}^{(\alpha)}(x,y) =
    \frac{k_{\epsilon}(x,y)}{(q_{\epsilon}(x) q_{\epsilon}(y))^{\alpha}}
  \end{equation}
\item Normalize $k_{\epsilon}^{\alpha}$ by setting
  $d_{\epsilon}^{(\alpha)}(x) = \int_{\mathcal{M}}{k_{\epsilon}^{(\alpha)}(x,y) q(y)
    \, d y}$ and define the transition probability
    $p_{\epsilon,\alpha}(x,y)$ by
    \begin{equation}
      \label{eq:96}
      p_{\epsilon,\alpha}(x,y) = \frac{k_{\epsilon}^{\alpha}(x,y)}{d_{\epsilon}^{\alpha}(x,y)}
    \end{equation}
\end{enumerate}
\citet{coifman06:_diffus_maps} then showed that as $\epsilon
\rightarrow 0$, the integral operator on $L^{2}(\mathcal{M},q)$ with
kernel $p_{\epsilon,\alpha}$ converges to different operators for
different choices of $\alpha$. Special cases includes $\alpha = 0$,
$\alpha = 1/2$ and $\alpha = 1$, with the resulting operator being the
Laplace-Beltrami operator, the Fokker-Planck operator, and the heat
kernel, respectively. See also
\citet{nadler05:_diffus_eigen_fokker_planc_operat} and
\citet{nadler06:_diffus}.

\subsection{Some thoughts on dimension reduction algorithms}
\label{sec:some-thoughts-isomap}
The introductory remarks to this section mentioned that Isomap,
Laplacian eigenmaps, and diffusion maps can all be understood as
examples of the following recipe
\begin{enumerate}
\item Represent the data as a weighted, or possibly unweighted,
  graph. The vertices correspond to feature vectors and the edges
  convey information about how pairs of vertices are related. The edge
  weights represent pairwise proximities in the ambient feature space,
  either similarities or dissimilarities. Isomap includes an edge
  between vertices $i$ and $j$ if the data points $x_i$ and $x_j$ are
  sufficiently close in the Euclidean feature space and weighs the
  edges by the Euclidean distance between the data points. Laplacian
  eigenmaps and diffusion maps weighs the edges by the Gaussian
  kernel with some bandwidth parameter.
\item Compute new proximities between the vertices. Isomap compute
  shortest path distance, diffusion maps compute diffusion
  distances. A variation of Laplacian eigenmaps compute expected
  commute time.
\item Embed the graph in an Euclidean space using the new
  proximities. Isomap embeds using classical MDS while Laplacian
  eigenmaps and diffusion maps embed using the eigenvalues and
  eigenvectors of the normalized Laplacian. 
\end{enumerate}
Common to all three algorithms is the construction of a graph $G$ over
the data points. However, the question of how to construct a graph $G$
is still unresolved. We have discussed two well known techniques,
namely $\epsilon$-ball and $K$ nearest neighbours. There exist results
that indicates that different graph construction leads to vastly
different results, see, for example,
\citet{maier08:_influen} and \citet{hein07:_conver_laplac} \\ \\
%
%
\noindent
What distinguishes these algorithms is then the second and third
step. The later sections of this paper are concerned with the
interplay between the second and third steps of the above
recipe. \S~\ref{cha:dist-undir-graphs} and
\S~\ref{cha:dist-direct-graphs} are devoted to discussing various
notions of distances on graphs, including diffusion distances and
expected commute time. We discussed techniques for embedding different
notions of graph metrics in \S~\ref{cha:graph-metr-dimens}. We will
see in \S~\ref{cha:graph-metr-dimens} that the embedding suggested by
Laplacian eigenmaps and diffusion maps don't extend easily to directed
graphs. This lead us to propose the view that, for the class of
dimension reduction algorithms that can be described by the above
recipe, the choice of graph metrics is the more important distinction
between the algorithms. We also want to mention two body of works that
are slightly related to the view of dimension reduction algorithms
expounded in this dissertation. \citet{ham04} showed that several
dimension reduction algorithms such as Isomap and Laplacian eigenmaps
can be viewed as instances of Kernel
PCA. \citet{yan07:_graph_embed_exten} showed that the same dimension
reduction algorithms can also be viewed as embeddings of the form
$\argmin_{\bm{y}^{T} \mathbf{K}\mathbf{B} \mathbf{K} \bm{y} = \bm{1}}
\bm{y}^{T} \mathbf{K} \mathbf{L} \mathbf{K} \bm{y}$ for some
row-centered matrix $\mathbf{L}$ and some positive semidefinite
$\mathbf{K}$.
%  
%
%
%

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "dissertation"
%%% End: 
