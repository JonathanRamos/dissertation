\chapter{Introduction}
\label{cha:introduction}

\section{Laplacian eigenmaps}
\label{sec:laplacian-eigenmaps}



\section{Hessian eigenmaps}
\label{sec:hessian-eigenmaps}
Hessian eigenmaps \citet{donoho03:_hesian} was described as a locally
linear embedding technique for high-dimensionality data. Hessian
eigenmaps assume that the data points lies on a  
Riemannian manifold $\mathcal{M} \subset \mathbb{R}^{n}$ such
that $\mathcal{M}$ is locally isometric to an open connected subset
$\Omega \subset \mathbb{R}^{d}$. For $f \colon \mathcal{M} \mapsto
\mathbb{R}$, define a quadratic form $\mathcal{H}(f)$ associated with
$f$ by $\mathcal{H}(f) = \int_{M}{ \| H_f(m) \|_{F}^{2} dm }$ where
$H_f(m)$ is the Hessian of $f$ at $m$ and $\| \cdot \|_F$ is the
Frobenius norm. $\mathcal{H}(f)$ averages the Frobenius norm of the
Hessian of $f$ over $\mathcal{M}$. The embedding coordinates
corresponds to the basis for the null space of $\mathcal{H}(f)$. The
procedure can be described as follows. 

Let $\{m_i\}_{i=1}^{N}$ be a collection of $N$ points in
$\mathbb{R}^{n}$. Suppose that $K$ is a parameter chosen to be the
size of the nearest neighborhoods and $d$ is a parameter chosen to be
the dimension of the embedding coordinates. The Hessian eigenmaps
procedure proceed as follows \citet{donoho03:_hesian}
\begin{enumerate}
\item Identify neighbors: For each $m_i$, set $\mathcal{N}(m_i)$ to be
  the set of the $K$ nearest neighbours of $m_i$. For each
  neighbourhood $\mathcal{N}(m_i)$, set $\mathbf{M}^{(i)}$ to be the $K \times n$
  matrix whose rows are the points $m_j \in \mathcal{N}(m_i)$. Define
  $\bar{\mathbf{M}}^{(i)}$ to be $(\mathbf{I} -
  \mathbf{J}/K)\mathbf{M}^{(i)}$, i.e. $\bar{\mathbf{M}}^{(i)}$ is
  the row centering of $\mathbf{M}^{(i)}$.
\item Do principal component analysis (PCA) on
  $\bar{\mathbf{M}}^{(i)}$. This is equivalent to finding the singular
  value decomposition (SVD) of $\bar{\mathbf{M}}^{(i)} = \mathbf{U}^{(i)}
  \bm{\Lambda}^{(i)} \mathbf{V}^{(i)}$ and using the first $d$ columns of
  $\mathbf{U}^{(i)}$ as the tangent coordinates of points in
  $\mathcal{N}(m_i)$. 
\item Let $f \colon \mathcal{M} \mapsto \mathbb{R}$ be a smooth
  function. Denote by $\bm{\nu}^{(i)} \in \mathbb{R}^{K}$ the vector
  consisting of $f(m_j)$ for $m_j \in \mathcal{N}(m_i)$. The Hessian
  estimator for $\mathcal{N}(m_i)$ is a matrix $\mathbf{H}^{(i)}$ such
  that $\mathbf{H}^{(i)} \bm{\nu}^{(i)}$ approximates the entries of
  the Hessian matrix of $f$ at $m_i$, i.e.,
  \begin{equation}
    \label{eq:84}
    \mathbf{H}^{(i)} \bm{\nu}^{(i)} \approx {\left[ \begin{array}{ccccc}
        \frac{\partial^2 f}{\partial u_1 \partial u_1} \bigl |_{m_i} &
        \frac{\partial^2 f}{\partial u_1 \partial u_2} \bigl |_{m_i} &
        & \dots & \frac{\partial^2 f}{\partial u_{d} \partial u_d} \bigl|_{m_i} \end{array} \right ]}^{T}
  \end{equation}
  where $u_1, u_2, \dots, u_d$ denotes a orthonormal basis for the
  points in $\mathcal{N}(m_i)$. $\mathbf{H}^{(i)}$ is a matrix of size
  $d(d+1)/2 \times K$ and can be found as follows. Consider the
  second-order Taylor series expansion for $f$ around $m_i$
  \begin{gather}
    \label{eq:85}
    f(m_i) + \sum_{r=1}^{d}{b_r u_r} + \sum_{r=1}^{d}\sum_{s=r}^{d}{a_{rs} u_r u_s}
    \shortintertext{where}
    b_r = \frac{\partial{f}}{\partial u_r} \Bigl|_{m_i} , \qquad a_{rs}
    = \frac{1}{2}\frac{\partial^2 f}{\partial u_r \partial u_s} \Bigl|_{m_i}
  \end{gather}
  The truncated polynomial in Eq.~\eqref{eq:85} can be fitted using
  least squares \citep[see][]{kim09:_semi_regres_hessian})
  \begin{equation}
    \label{eq:86}
    \min_{\bm{w} \in \mathbb{R}^{p}} \sum_{j=1}^{K} \Bigl( (f(m_i) -
    f(m_j)) - (\mathbf{X}^{(i)} \bm{w})(j)\Bigr)^2
  \end{equation}
  where $p = 1 + d + d(d+1)/2$ and $\mathbf{X}^{(i)}$ is the $K \times
  p$ matrix with the first $d+1$ columns of $\mathbf{X}^{(i)}$ being
  the columns of $\bm{1}$ and the first $d$ columns of
  $\mathbf{U}^{(i)}$, and the remaining $d(d+1)/2$ columns of
  $\mathbf{X}^{(i)}$ being the element-wise products between pairs of
  the first $d$ columns of $\mathbf{U}^{(i)}$. The solution to
  Eq.~\eqref{eq:86} is given by $\bm{w} =
  (\mathbf{X}^{(i)})^{\dagger}f$. The matrix $\mathbf{H}^{(i)}$ is
  then the transpose of the last $d(d+1)/2$ columns of
  $(\mathbf{X}^{(i)}  )^{\dagger}$. If we want $\mathbf{H}^{(i)}$ to
  have orthnormal columns, then $\mathbf{H}^{(i)}$ coincides with the
  transpose of the last $d(d+1)/2$ columns of the matrix $\mathbf{Q}$
  in the QR decomposition of $\mathbf{X}^{(i)}$.
\item Build a symmetric matrix $\mathbf{H} =
  \sum_{i=1}^{N}{(\mathbf{H}^{(i)})^{T}\mathbf{H}^{(i)}}$. If
  $\bm{\nu} = (f(m_j))_{j=1}^{N}$ is a sample vector of $f$, then
  $\bm{\nu}^{T} \mathbf{H} \bm{\nu}$ is an approximation of
  $\mathcal{H}(f)$. 
\item Perform an eigendecomposition of $\mathbf{H}$ and identify the
  $d$ eigenvectors of $\mathbf{H}$ corresponding to the second
  smallest eigenvalues up to the $d+1$ smallest eigenvalues. Let
  $\mathbf{V}$ be the matrix with these eigenvectors as columns. The
  embedding coordinates of point $m_i$ are given by the $i$-th row of
  $\mathbf{V}$.
\end{enumerate}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "dissertation"
%%% End: 
