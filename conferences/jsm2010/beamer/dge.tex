\documentclass[professionalfonts,hyperref={pdfpagelabels=false,colorlinks=true,linkcolor=cyan}]{beamer}

\mode<presentation>{
  \usetheme{Boadilla}
  \useinnertheme{rectangles}
  \usecolortheme[cmyk={0,1,0.63,0.29}]{structure}
  \setbeamertemplate{navigation symbols}{}
}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{subfigure}
\usepackage{bm}
\usepackage[all]{xy}
\bibliographystyle{plainnat}
%\usepackage{pgfpages}
%\pgfpagesuselayout{4 on 1}[letterpaper, landscape, border shrink=5mm]

\newtheorem{question}[theorem]{Question}
\newtheorem{openquestion}[theorem]{Open Question}
\setbeamercolor{question title}{bg = red}
\setbeamercolor{block body question}{bg=blue!60}
\AtBeginSection[]{
  \begin{frame}<beamer>
    \frametitle{Outline}
    \tableofcontents[currentsection,currentsubsection]
    \end{frame}
}
%
\begin{document}
\title[Embedding Directed Proximity Data]{Embedding Directed Proximity Data}
\author[Tang \& Trosset]{Minh Tang\inst{1} \and Michael
  Trosset\inst{2}}
\institute[Indiana University]{
  \inst{1} School of Informatics and Computing \\
  Indiana University, Bloomington
  \and \inst{2} Department of Statistics \\ Indiana University,
  Bloomington
}
\date[JSM 2010]{This work was funded by a grant from the Office of Naval Research.}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
  \frametitle{Problem Description}
  We consider the problem of constructing a Euclidean representation
  of directed proximity data. The representation can served as the
  basis for doing exploratory data analysis. 

  \vskip10pt Let $\bm{\Delta}$ be an $N \times N$ directed
  dissimilarity matrix. Let $\mathbf{U} = (I_{i \geq j})_{i,j=1}^{N}$ and
  $\mathbf{L} = (I_{i < j})_{i,j=1}^{N}$ be the upper triangular and
  lower triangular matrices of all ones, respectively. We proposed a projected
  subspaces algorithms that embed $\bm{\Delta}$ either by
 \begin{enumerate}
  \item Embedding $\bm{\Delta}_U = \bm{\Delta} \ast \mathbf{U} +
    \bm{\Delta}' \ast \mathbf{L}$ and $\bm{\Delta}_{L} =
    \bm{\Delta} \ast \mathbf{L} + \bm{\Delta}' \ast \mathbf{U}$
    together. Here $\ast$ is the Hadamard product operator for
    matrices.
  \item Embedding $\bm{\Delta}_S = (\bm{\Delta} + \bm{\Delta}')/2$
    and $\bm{\Delta}_{A} = (\bm{\Delta} - \bm{\Delta}')/2$
    together. $\bm{\Delta}_S$ and $\bm{\Delta}_{A}$ are the symmetric
    and skew-symmetric part of $\bm{\Delta}$.
  \end{enumerate}
  Our work is related to three-ways MDS and asymmetric MDS models.
\end{frame}

\begin{frame}
  \frametitle{Three-way MDS models}
  Given a set of $M$ dissimilarity matrices
  $\{\bm{\Delta}^{(k)}\}_{k=1}^{M}$, three-way MDS algorithms attempt
  to find a common group space $\mathbf{G}$ and transformation
  matrices $\{\mathbf{T}_k\}$ that minimize some loss function
  $L(\mathbf{G},
  \{\mathbf{T}_k\}_{k=1}^{M})$. For example, one could consider
  \begin{equation}
    \label{eq:2}
    L(\mathbf{G}, \{\mathbf{T}_k\}_{k=1}^{M}) = \sum_{k = 1}^{M}\| \mathbf{B}_k -
    \mathbf{G}\mathbf{T}_k \mathbf{T}_k' \mathbf{G}' \|_F^2 
  \end{equation}
 where $\mathbf{B}_k$ is the double centering of $\bm{\Delta}^{(k)}
 \ast \bm{\Delta}^{(k)}$.
 \vskip10pt If one restricted $\{\mathbf{T}_k\}$ to be positive
 diagonal matrices, the resulting model is the INDSCAL
 model \cite{carroll70:_analy_n_eckar_young}. If the
 $\{\mathbf{T}_k\}$ are arbitrary transformation matrices, the
 resulting model is the IDIOSCAL model \cite{carroll74:_contem}. The
 IDIOSCAL model is deemed to be too general by some authors,
 e.g., \cite{carroll70:_analy_n_eckar_young}. 
\end{frame}

\begin{frame}
  \frametitle{Projected subspaces model}
  The projected subspaces model restricted the $\{\mathbf{T}_k\}$ to
  be orthogonal projection matrices. The resulting optimization
  problem can be written as
  \begin{equation}
  \label{eq:5}
	\begin{aligned}
	& \underset{\mathbf{G}, \{\mathbf{T}_k\}}{\text{minimize}}
	& & \sum_{k=1}^{M} \| \mathbf{B}_k - \mathbf{G}
\mathbf{T}_k \mathbf{G}' \|_F^2 \\
	& \text{subject to}
	& & \mathbf{T}_k \succeq 0, \quad \mathbf{T}_k^2 = \mathbf{T}_k \;
    \forall k\\
    & & & \mathrm{rank}(\mathbf{T}_k) = d_k \; \forall k \\
	&&& \mathbf{G}' \mathbf{G} = \mathbf{I}
	\end{aligned}
\end{equation}
The above problem can be solved using alternating
optimization.
\begin{itemize}
\item Let $\mathbf{V} \mathbf{D} \mathbf{V}'$ be the spectral
  decomposition of $\mathbf{G}' \mathbf{B}_k \mathbf{G}$. The update
  for $\mathbf{T}_k$ is $\mathbf{T}_k = \mathbf{V}_{d_k} \mathbf{V}_{d_k}'$.
\item Let $\alpha_k = \| \mathbf{B}_k \|_{\infty}$. The update for
  $\mathbf{G}$ is $\mathbf{G} = \mathbf{V} \mathbf{W}'$
  \cite{kiers90:_major} where $\mathbf{V} \bm{\Sigma} \mathbf{W}'$ is
  the singular value decomposition of
  \begin{equation}
    \label{eq:3}
    \mathbf{G} + \frac{2}{\sum_{k=1}^{M} \alpha_k} \sum_{k=1}^{M}
    \mathbf{B}_k \mathbf{G} \mathbf{T_k}
  \end{equation}
\end{itemize}
\end{frame}
\begin{frame}
 \frametitle{Comparing the models}
  \begin{columns}[t]
  \begin{column}{0.5\textwidth}
    Projected subspaces model
    \begin{enumerate}
    \item The algorithm as given is guaranteed to converge.
    \item Updating $\mathbf{T}_k$ is a single update.
    \item The model is \alert{not} scale invariant.
    \item The model is invariant with respect to rotation.
    \end{enumerate}
  \end{column}
  
  \begin{column}{0.5\textwidth}
    INDSCAL
    \begin{enumerate}
    \item The CANDECOMP algorithm almost always converge but
      convergence is not guaranteed.j
    \item Updating $\mathbf{T}_k$ needs to respect non-negativity
      constraint. Non-negative or alternating least squares can be
      used, but they are iterative procedures. 
    \item The model is scale invariant.
    \item The model is \alert{not} invariant with respect to rotation.
    \end{enumerate}
  \end{column}
\end{columns}
\end{frame}

\begin{frame}
  \frametitle{Toy example}
  We sampled points from a two-dimensional $N(\mathbf{0},
  \mathbf{I})$. We used two distance measure,
  $\delta_{ij}^{(1)} = \| \mathbf{x}_i - \mathbf{x}_j \|$ and
  $\delta_{ij}^{(2)} = \| \mathbf{T}\mathbf{x}_i - \mathbf{T}
  \mathbf{x}_j \|$ where $\mathbf{T} =
  \bigl[\begin{smallmatrix} 1 & 0 \\ 0 & 0 \end{smallmatrix}
  \bigr]$. $\bm{\Delta}$ was constructed as $\bm{\Delta} =
  (\delta_{ij}^{(1)}I_{i \geq j} + \delta_{ij}^{(2)} I_{i <
    j})$. Figure \ref{fig:toy1_projected} gives the embedding of
  $\bm{\Delta}_U$ and $\bm{\Delta}_L$ by the projected subspaces
  algorithm. Figure \ref{fig:toy1_separate} gives the separate
  embeddings of $\bm{\Delta}_U$ and $\bm{\Delta}_L$ by classical MDS. 
  \begin{figure}[htbp]
  \subfiglabelskip=10pt
    \centering
    \subfigure[][]{
      \label{fig:toy1_projected}
      \includegraphics[width=40mm]{dge_toy1_projected.pdf}
    }
    \subfigure[][]{
      \label{fig:toy1_separate}
      \includegraphics[width=40mm]{dge_toy1_separate.pdf}
    }
    \caption{Embedding of a toy example by projected subspaces and
      classical MDS.}
    \label{fig:toy_dge}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Morse codes confusion rates}
  The confusion rates for the Morse codes of the characters A through
  Z along with the digits 0 through 9 was recorded in a directed
  similarity matrix $\mathbf{\Gamma}$. \cite{rothkopf57} asked 598
  subjects to judge whether two Morse code signals that are presented
  one after another were the same. We transformed the asymmetric
  similarity matrix $\bm{\Gamma}$ to an asymmetric dissimilarity
  matrix $\bm{\Delta}$ by $\bm{\Delta}(i,j) = \bm{\Gamma}(i,i) -
  \bm{\Gamma}(i,j)$. Figure \ref{fig:morsecode} presented the
  embeddings of $\bm{\Delta}^{(S)}$ and $\bm{\Delta}^{(A)}$. The
  common group space $\mathbf{G}$ was constrained to be three
  dimensional and the projection matrices $\mathbf{T}_1$ and
  $\mathbf{T}_2$ were constrained to have $\mathrm{rank}(\mathbf{T}_1)
  = \mathrm{rank}(\mathbf{T}_2)$. 
\end{frame}
\begin{frame}
  \frametitle{Morse code confusion rates (cont'd)}
\begin{figure}[htbp]
  \centering
    \includegraphics[width=7cm]{dge_morsecode_bimension12.pdf}
    \caption{Embeddings of the Morse code data set. The
      distances between the blue colored points approximate the
      absolute value of the entries of $\bm{\Delta}^{(A)}$.}
  \label{fig:morsecode}
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Cola brands switching}
  The data were collected from 448 households over a period of 104
  weeks. The original measurements was the number of households that
  switch from one cola soft drink to another cola soft drink. The
  original measurements were then converted into a dissimilarity
  matrix $\bm{\Delta}$ using the gravity model. 
\begin{figure}[htbp]
  \centering
    \includegraphics[width=7cm]{cola_switching.pdf}
    \caption{Embeddings of the cola brands switching data
      set. The distances between the blue colored points approximate
      the absolute value of the entries of $\bm{\Delta}^{(A)}$.}
  \label{fig:colaswitch}
\end{figure}
\end{frame}
\begin{frame}
\bibliography{dissertation}
\end{frame}
\end{document}  

