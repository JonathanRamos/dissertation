\chapter{Distances on undirected graphs}
\section{Expected commute time}
Let $G = (V,E,\omega)$ be an undirected graph, with $\omega$ being the
similarity measure. We assume that the transition matrix
$\mathbf{P}$ on $G$ is irreducible and aperiodic, see
\S \ref{sec:finite-markov-chain}. The {\em expected commute time}
$\delta(u,v)$ between $u \in V$ and $v \in V$ is defined as
\begin{equation}
  \label{eq:25}
  \delta(u,v) = \mathbb{E}_{u}[\tau_v] + \mathbb{E}_{v}[\tau_u]
\end{equation}
Let $\mathbf{M}$ be the matrix of first passage time, i.e.,
$\mathbf{M}(u,v) = \mathbb{E}_{u}[\tau_v]$. The following proposition
shows that $\mathbf{M}$ is the unique solution of a given matrix equation. 
\begin{proposition}
  \label{prop:4}
 $\mathbf{M}$ is the unique solution of the following matrix equation
  \begin{equation}
    \label{eq:3}
   (\mathbf{I} - \mathbf{P})\mathbf{X} = \mathbf{J} - \bm{\Pi}^{-1}
  \end{equation}
  subjected to the condition 
  \begin{equation}
    \label{eq:32}
 \mathbf{M}_{\mathrm{dg}} = \mathbf{0}, \qquad \mathbf{M}(u,v) \geq 0   
  \end{equation}
  where $\mathbf{M}_{\mathrm{dg}}$ is the diagonal matrix formed
    by setting the off-diagonal entries of $\mathbf{M}$ to zero.
    Thus, $\mathbf{M} = \mathbf{X} -
    \mathbf{J}\mathbf{X}_{\mathrm{dg}}$ where $\mathbf{X}$ satisfy
    Eq. \eqref{eq:3}.
\end{proposition}
\begin{proof}
  If $u = v$, then $\mathbb{E}_{u}[\tau_u] = 0$ and thus
  $\mathbf{M}(u,u) = 0$. Otherwise, if $u \not = v$, then
  $\mathbf{M}(u,v) = \mathbb{E}_{u}[\tau_v]$ can be expanded as
  \begin{equation}
    \label{eq:4}
    \mathbb{E}_{u}[\tau_v] = 1 + \sum_{w \in V}{\mathbf{P}(u,w)
      \mathbb{E}_{w}[\tau_v]} = 1 + (\mathbf{PM})(u,v)
  \end{equation}
  Thus, $\mathbf{F} = \mathbf{J} + (\mathbf{P} -
  \mathbf{I})\mathbf{M}$ is a diagonal matrix. Futhermore, if $\pi$ is
  the vector of stationary distribution, then
  \begin{equation}
    \label{eq:26}
    \pi^{T} \mathbf{F} = \pi^{T} \mathbf{J} + \pi^{T} (\mathbf{P} -
    \mathbf{I})\mathbf{M} = \mathbf{1}   
  \end{equation}
  Therefore, $\mathbf{F}(u,u) = 1/\pi(u)$ and thus $\mathbf{F} =
  \bm{\Pi}^{-1}$. We thus have $\bm{\Pi}^{-1} = \mathbf{J} +
  (\mathbf{P} - \mathbf{I})\mathbf{M}$. $\mathbf{M}$ is then a 
  solution of the matrix equation as given by Eq.~\eqref{eq:3}.

  We now show that $\mathbf{M}$ is the unique solution of
  Eq.~\eqref{eq:3} subjected to the condition in
  Eq.~\eqref{eq:32}. Let $\mathbf{M}'$ be another solution of
  Eq.~\eqref{eq:3} subjected to the condition in
  Eq.~\eqref{eq:32}. Then $\mathbf{Y} = \mathbf{M} - \mathbf{M}'$
  satisfy
  \begin{equation}
    \label{eq:19}
    (\mathbf{I} - \mathbf{P})\mathbf{Y} = \mathbf{0}
  \end{equation}
  By Lemma \ref{lem:1}, each column of $\mathbf{Y}$ is constant. Since
  $\mathbf{M}_{\mathrm{dg}} = \mathbf{M'}_{\mathrm{dg}} = \mathbf{0}$, each
  column of $\mathbf{Y}$ must be identically $0$. Thus $\mathbf{M} = \mathbf{M'}$,
  proving the uniqueness of $\mathbf{M}$. If $\mathbf{X}$ satisfy
  Eq. \eqref{eq:3}, then $\mathbf{X} - \mathbf{J}\mathbf{X}_{\mathrm{dg}}$
  satisfy the condition in Eq.~\eqref{eq:32}.
\end{proof}
%
\begin{proposition}
  \label{prop:5}
  Let $\mathbf{Q} = \mathbf{1}^{T}\mathbf{\pi}$ be the matrix with each row being
  the stationary distribution $\pi$. The matrix $\mathbf{M}$ of expected
  first passage time is given by
  \begin{equation}
    \label{eq:21}
    \mathbf{M} = \mathbf{J}(\mathbf{Z} \bm{\Pi}^{-1})_{\mathrm{dg}} - \mathbf{Z}
    \bm{\Pi}^{-1}
  \end{equation}
  where $\mathbf{Z} = (\mathbf{I} - \mathbf{P} + \mathbf{Q})^{-1}$. 
\end{proposition}
\begin{proof}
  From Proposition \ref{prop:7}, we know that $\mathbf{Z} =
  (\mathbf{I} - \mathbf{P} + \mathbf{Q})^{-1}$ is well defined. 
  We first show that $\mathbf{X} = (\mathbf{I} - \mathbf{P} + \mathbf{Q})^{-1}(\mathbf{J}
  - \bm{\Pi}^{-1})$ satisfy Eq. \eqref{eq:3}. We have from Proposition
  \ref{prop:8} that $(\mathbf{I} - \mathbf{P})\mathbf{X} = (\mathbf{I} -
  \mathbf{P})\mathbf{Z}(\mathbf{J} - \bm{\Pi}^{-1}) = (\mathbf{I} - \mathbf{Q})(\mathbf{J} -
  \bm{\Pi}^{-1})$. Since
  $\mathbf{Q}\mathbf{J} = \mathbf{J} = \mathbf{Q}\bm{\Pi}^{-1}$, one has
  \begin{equation}
    \label{eq:27}
    (\mathbf{I} - \mathbf{P})\mathbf{X} =(\mathbf{I} - \mathbf{Q})(\mathbf{J} -
  \bm{\Pi}^{-1}) = \mathbf{J} - \bm{\Pi}^{-1}
  \end{equation}
  and thus $\mathbf{X}$ satisfy Eq. \eqref{eq:3}. Also, from Proposition
  \ref{prop:8}, we have
  \begin{equation}
    \label{eq:31}
    \mathbf{X} = \mathbf{Z}(\mathbf{J} - \bm{\Pi}^{-1}) = \mathbf{J} -
    \mathbf{Z}\bm{\Pi}^{-1}
  \end{equation}
  and thus $\mathbf{X} - \mathbf{J}\mathbf{X}_{\mathrm{dg}} =
  \mathbf{J}(\mathbf{Z}\bm{\Pi}^{-1})_{\mathrm{dg}} - \mathbf{Z}\bm{\Pi}^{-1}$. 
\end{proof}
The expected commute time $\delta(u,v)$ is then just
$\delta(u,v) = \mathbf{M}(u,v)
+ \mathbf{M}^{T}(u,v)$. Thus, if we let $\Delta_{\delta}$ be the
matrix of expected commute time between the vertices, then
\begin{equation}
  \label{eq:52}
  \Delta_\delta = \mathbf{M} + \mathbf{M}^{T} = \mathbf{J}(\mathbf{Z}\bm{\Pi}^{-1})_{\mathrm{dg}} - \mathbf{Z}\bm{\Pi}^{-1} -
  \bm{\Pi}^{-1}\mathbf{Z}^{T} + (\bm{\Pi}^{-1}\mathbf{Z})_{\mathrm{dg}}\mathbf{J}
\end{equation}
We now show that $\Delta_\delta$ is an EDM-2 matrix whenever $G$ is an
undirected graph. 
\begin{proposition}
  \label{prop:10}
  If $G$ is an undirected graph, then $\mathbf{Z}\bm{\Pi}^{-1} =
  \bm{\Pi}^{-1}\mathbf{Z}^{T}$. The matrix $\Delta_{\delta}$ of
  expected commute time is then given by 
  \begin{equation}
    \label{eq:33}
 \Delta_{\delta}  
  = \mathbf{J}(\mathbf{Z}\bm{\Pi}^{-1})_{\mathrm{dg}} - \mathbf{Z}\bm{\Pi}^{-1} -
  \bm{\Pi}^{-1}\mathbf{Z}^{T} + (\bm{\Pi}^{-1}\mathbf{Z})_{\mathrm{dg}} \mathbf{J} =
   \kappa(\mathbf{Z}\bm{\Pi}^{-1})
  \end{equation}
  The matrix $\mathbf{Z}\bm{\Pi}^{-1}$
  is positive definite, and $\Delta_{\delta}$ is an EDM-2 matrix.
\end{proposition}
\begin{proof}
  From Proposition \ref{prop:15}, we have $\mathbf{P}\bm{\Pi}^{-1} =
  \bm{\Pi}^{-1}\mathbf{P}^{T}$. Thus, $\mathbf{P}^{k}\bm{\Pi}^{-1} =
  \bm{\Pi}^{-1}(\mathbf{P}^{T})^{k}$. Furthermore, $\mathbf{Q}\bm{\Pi}^{-1} =
  \mathbf{J}$ and so
  \begin{equation}
    \label{eq:53}
    \mathbf{Z}\bm{\Pi}^{-1} = \biggl[\mathbf{I} +
    \sum_{k=1}^{\infty}(\mathbf{P}^{k} -
    \mathbf{Q})\biggr]\bm{\Pi}^{-1} =
    \bm{\Pi}^{-1}\biggl[\mathbf{I} +
    \sum_{k=1}^{\infty}((\mathbf{P}^{T})^{k} - \mathbf{Q}^{T})\biggr]
    = \bm{\Pi}^{-1}\mathbf{Z}^{T}
  \end{equation}
  Now, $\mathbf{Z}\bm{\Pi}^{-1}$ is positive definite if and only if
  $\bm{\Pi}\mathbf{Z}^{-1} = \bm{\Pi}(\mathbf{I} - \mathbf{P} +
  \mathbf{Q}) \succ 0$. Since $\bm{\Pi}(\mathbf{I} - \mathbf{P} +
  \mathbf{Q}) = \bm{\Pi}(\mathbf{I} - \mathbf{P}) + \pi\pi^{T}$, we
  see that $\bm{\Pi}\mathbf{Z}^{-1} \succ 0$ if $\bm{\Pi}(\mathbf{I} -
  \mathbf{P}) \succeq 0$. We know that $\bm{\Pi}(\mathbf{I} -
  \mathbf{P})$ is symmetric and diagonally dominant (see Definition
  \ref{def:4}). By Ger\u{s}gorin circle theorem, the eigenvalues of
  $\bm{\Pi}(\mathbf{I} - \mathbf{P})$ are non-negative. Thus,
  $\bm{\Pi}(\mathbf{I} - \mathbf{P}) \succeq 0$ and
  the claim that $\mathbf{Z}\bm{\Pi}^{-1}$ is positive definite
  follows. $\Delta_{\delta} = \kappa(\mathbf{Z}\bm{\Pi}^{-1})$ is then
  an EDM-2 matrix.
\end{proof}
%
There exists in the literatures a notion of distances known as
resistance distance
\cite{bapat99:_resis_distan_in_graph,klein93:_resis_distan}. Let $G =
(V,E,\omega)$ be an undirected graph with similarity measure
$\omega$. Let $\mathbf{L}$ be the combinatorial Laplacian of $G$. The
resistance distance $r(u,v)$ between $u, v \in V$ is defined as
\begin{equation}
  \label{eq:35}
  r(u,v) = \tfrac{1}{2}(\mathbf{L}^{\dagger}(u,u) - \mathbf{L}^{\dagger}(u,v) -
  \mathbf{L}^{\dagger}(v,u) + \mathbf{L}^{\dagger}(v,v))
\end{equation}
where $\mathbf{L}^{\dagger}$ is the {\em Moore-Penrose} pseudo-inverse of
$\mathbf{L}$. It's widely known that for undirected graphs, resistance
distance is proportional to expected commute
$\delta(u,v)$. Specifically,
\begin{equation}
  \label{eq:36}
  r(u,v) = \frac{2 \delta(u,v)}{\mathrm{Vol}(G)}
\end{equation}
Eq.~\eqref{eq:36} is an easy corollary of the following result.
\begin{proposition}
  \label{prop:11}
  Let $G = (V,E,\omega)$ be an undirected graph with $|V| = n$. The
  Moore-Penrose pseudo-inverse $\mathbf{L}^{\dagger}$ of $\mathbf{L}$ is given
  by
  \begin{equation}
    \label{eq:37}
    \mathbf{L}^{\dagger} = c \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr) \mathbf{Z}
    \bm{\Pi}^{-1} \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr)
  \end{equation}
  where $c = 1/\mathrm{Vol}(G)$ is a constant. 
\end{proposition}
\begin{proof}
  We will show that $\mathbf{L}^{\dagger}$ as defined by Eq.~\eqref{eq:37}
  satisfies the following conditions for a Moore-Penrose pseudo-inverse
  \begin{gather*}
    \mathbf{L}\mathbf{L}^{\dagger} = \mathbf{L}^{\dagger}\mathbf{L} \tag{(i)} \\
    \mathbf{L}\mathbf{L}^{\dagger}\mathbf{L} = \mathbf{L} \tag{(ii)} \\
    \mathbf{L}^{\dagger}\mathbf{L} \mathbf{L}^{\dagger} = \mathbf{L}^{\dagger}
    \tag{(iii)}
  \end{gather*}
  If $G = (V,E,\omega)$ is an undirected graph, then $\pi(u) =
  \deg(u)/\mathrm{Vol}(G)$ and thus $\mathbf{D} = \mathrm{Vol}(G)
  \bm{\Pi}$. Therefore $\mathbf{L} = \mathbf{D}(\mathbf{I} - \mathbf{P}) =
  \mathrm{Vol}(G) \bm{\Pi}(\mathbf{I} - \mathbf{P})$. We also have
  \begin{equation}
    \label{eq:38}
    \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr)\mathbf{L} = \mathbf{L}
  \end{equation}
  and thus
  \begin{equation}
    \label{eq:39}
    \begin{split}
      \mathbf{L}^{\dagger}\mathbf{L} &= \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr) \mathbf{Z}
       \bm{\Pi}^{-1} \bm{\Pi}(\mathbf{I} - \mathbf{P}) \\
       &= \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr) (\mathbf{I} - \mathbf{P} + \mathbf{Q})^{-1}
       (\mathbf{I} - \mathbf{P}) \\
       &= \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr)(\mathbf{I} - \mathbf{Q}) \\
       &= \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr)
   \end{split}
  \end{equation}
  Similarly,
  \begin{equation}
    \label{eq:40}
    \begin{split}
      \mathbf{L}\mathbf{L}^{\dagger} &= \bm{\Pi}(\mathbf{I} - \mathbf{P}) \mathbf{Z}
      \bm{\Pi}^{-1} \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr) \\
      &= \bm{\Pi}(\mathbf{I} - \mathbf{P}) (\mathbf{I} - \mathbf{P} + \mathbf{Q})^{-1}
      \bm{\Pi}^{-1} \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr) \\
      &= (\mathbf{I} - \mathbf{Q}^{T})\Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr) \\
      &= \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr)
   \end{split}
  \end{equation}
  Thus, condition (i) is satisfied. Furthermore, we also have, from
  Eq.~\eqref{eq:39} and Eq.~\eqref{eq:40}, that
  \begin{gather*}
   \mathbf{L}\mathbf{L}^{\dagger}\mathbf{L} = \Bigl(\mathbf{I} -
   \frac{\mathbf{J}}{n}\Bigr) \mathbf{L}
   = \mathbf{L} \\
    \mathbf{L}^{\dagger}\mathbf{L}\mathbf{L}^{\dagger} = c \Bigl(\mathbf{I} -
    \frac{\mathbf{J}}{n}\Bigr) \mathbf{Z}
     \bm{\Pi}^{-1} \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr) \Bigl(\mathbf{I} -
     \frac{\mathbf{J}}{n}\Bigr) = c \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr) \mathbf{Z}
     \bm{\Pi}^{-1} \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr) = \mathbf{L}^{\dagger}
  \end{gather*}
  Thus, $\mathbf{L}^{\dagger}$ as defined by Eq.~\eqref{eq:37} is the
  Moore-Penrose inverse of $\mathbf{L}$. 
\end{proof}
Proposition \ref{prop:11} is equivalent to saying that
$\mathbf{L}^{\dagger}$ is a constant times the $\tau$ transform of
$\mathbf{Z}\bm{\Pi}^{-1}$. Since $\kappa(\mathbf{X}) =
\kappa(\tau(\mathbf{X}))$, we have 
\begin{equation}
  \label{eq:42}
 \Delta_{\delta} = \kappa(\mathbf{Z}\bm{\Pi}^{-1}) = \mathrm{Vol}(G)
\kappa(\mathbf{L}^{\dagger}) 
\end{equation}
Eq.~\eqref{eq:36} thus follows from Proposition \ref{prop:11}, as
claimed. 
%
\section{Diffusion distances}
\label{sec:diffusion-distances}
Let $G = (V,E,\omega)$ be an undirected graph with
$\omega$ being a similarity measure between vertices of $V$. Denote by
$\mathbf{P}$ the probability transition matrix of $G$. The diffusion
distances at time $t$, $\rho_{t}(u,v)$, between $u,v \in V$ is defined as
\cite{coifman06:_diffus_maps}
\begin{equation}
  \label{eq:43}
  \rho^{2}_{t}(u,v) = \sum_{w \in V}{\Bigl(\mathbf{P}^{t}(u,w) -
      \mathbf{P}^{t}(v,w)\Bigr)^2 \frac{1}{\pi(w)}}
\end{equation}
\begin{proposition}
  \label{prop:12}
  Diffusion distances as defined by Eq.~\eqref{eq:43} can also be
  written as
  \begin{equation}
    \label{eq:44}
    \begin{split}
      \rho_{t}^{2}(u,v) &= \frac{(\mathbf{P}^{2t}(u,u) -
        (\mathbf{P}^{2t})(v,u)}{\pi(u)} +
      \frac{(\mathbf{P}^{2t})(v,v) -
        (\mathbf{P}^{2t}(u,v)}{\pi(v)}  \\
      &= (\mathbf{P}^{2t}\bm{\Pi}^{-1})(u,u) -
      (\mathbf{P}^{2t}\bm{\Pi}^{-1})(v,u) \\
      &+ (\mathbf{P}^{2t}\bm{\Pi}^{-1})(v,v) -
      (\mathbf{P}^{2t}\bm{\Pi}^{-1})(u,v)
    \end{split}
  \end{equation}
The matrix of squared diffusion distances $\Delta_{\rho_t^2}$ is then
$\Delta_{\rho_t^2} = \kappa(\mathbf{P}^{2t}\bm{\Pi}^{-1})$. 
\end{proposition}
\begin{proof}
  Since $G$ is undirected, $\mathbf{P}$ is time-reversible and hence
  \begin{equation}
    \label{eq:45}
    \pi(u) \mathbf{P}(u,v) = \pi(v) \mathbf{P}(v,u) 
  \end{equation}
  By
  expanding the square of $(\mathbf{P}^{t}(u,w) - \mathbf{P}^{t}(v,w))^{2}$ in
  Eq.~\eqref{eq:43} and using Eq.~\eqref{eq:45}, one has
  \begin{equation}
    \label{eq:46}
    \begin{split}
      \rho_{t}^{2}(u,v) &= \sum_{w \in V}{\Bigl(\mathbf{P}^{t}(u,w) -
        \mathbf{P}^{t}(v,w)\Bigr)^2 \frac{1}{\pi(w)}} \\
      &= \sum_{w \in V}{\frac{\mathbf{P}^{t}(u,w)\mathbf{P}^{t}(u,w) -
          \mathbf{P}^{t}(u,w)\mathbf{P}^{t}(v,w)}{\pi(w)}} \\
      &+\sum_{w \in V}{\frac{\mathbf{P}^{t}(v,w)\mathbf{P}^{t}(v,w) -
          \mathbf{P}^{t}(v,w)\mathbf{P}^{t}(u,w)}{\pi(w)}} \\
      &= \sum_{w \in
        V}{\frac{\mathbf{P}^{t}(u,w)\mathbf{P}^{t}(w,u) -
          \mathbf{P}^{t}(v,w)\mathbf{P}^{t}(w,u)}{\pi(u)}} \\ &+
      \sum_{w \in V}{\frac{\mathbf{P}^{t}(v,w)\mathbf{P}^{t}(w,v)
          -
          \mathbf{P}^{t}(u,w)\mathbf{P}^{t}(w,v)}{\pi(v)}} \\
      &= \frac{(\mathbf{P}^{2t})(u,u) -
        (\mathbf{P}^{2t})(v,u)}{\pi(u)} +
      \frac{(\mathbf{P}^{2t})(v,v) -
        (\mathbf{P}^{2t})(u,v)}{\pi(v)} 
    \end{split} 
  \end{equation}
  which is exactly Eq.~\eqref{eq:44}. 
\end{proof} 
Since $\mathbf{P}^{2t}\bm{\Pi}^{-1} =
\mathbf{P}^{t}\mathbf{P}^{t}\bm{\Pi}^{-1} =
\mathbf{P}^{t}\bm{\Pi}^{-1}\mathbf{P}^{T} \succeq 0$,
$\Delta_{\rho_{t}^2} = \kappa(\mathbf{P}^{2t}\bm{\Pi}^{-1})$ is an
EDM-2 matrix. We state the above observation as the following
proposition
\begin{proposition} 
\label{prop:14} 
Let $G$ be an undirected graph and $\mathbf{P}$ be its transition
matrix. The matrix $\Delta_{\rho_{t}^2}$ of squared diffusion distances is 
an EDM-2 matrix for any $t$. $\Delta_{\rho_t}$ is then an EDM-1 matrix.
\end{proposition}
%
We would now like to make an observation regarding diffusion distances
on undirected graphs. From Eq.~\eqref{eq:46}, we observed that
$\rho_{t}^{2}(u,v)$ only depends on the probability between nodes
connected by paths of length $2t$. Thus, diffusion distances between
any two nodes $u$ and $v$ of $G$ for any time scale $t$ only keeps
tracks of paths of even length in $G$. Diffusion distances might
be unintuitive in some scenarios. For a contrived example,
consider the case where $G$ is a cycle. Then, there might be pairs of nodes that are
adjacent to each other and that have diffusion distances larger than
the nodes that are on two different segments of the cycle. We will
take a closer look at this phenomenon in a later chapter of the
thesis. \\ \\
%
\noindent
We now note the connection between expected commute time and diffusion
distances for when $G$ is an undirected graph. From Proposition
\ref{prop:16}, we have
\begin{equation}
  \label{eq:49}
\kappa(\mathbf{P}^{2t}\bm{\Pi^{-1}}) =
\kappa(\mathbf{P}^{2t}\bm{\Pi^{-1}} - \mathbf{J}) =
\kappa((\mathbf{P}^{2t} - \mathbf{Q}) \bm{\Pi^{-1}})
\end{equation}
Let $\mathbf{T}_{m} = \Bigl(\mathbf{I} + \sum_{k =
  1}^{m}{(\mathbf{P}^{k} - \mathbf{Q})}\Bigr)\bm{\Pi}^{-1}$ for $m
\geq 0$. Then $\| \mathbf{T}_m - \mathbf{Z}\bm{\Pi}^{-1} \| \rightarrow 0$ as
$m \rightarrow \infty$. This can be seen as follow. 
\begin{equation}
  \label{eq:47}
  \begin{split}
  \| \mathbf{T}_m - \mathbf{Z}\bm{\Pi}^{-1} \| &=
  \|\sum_{k=m+1}^{\infty}(\mathbf{P} - \mathbf{Q})^{k}\bm{\Pi}^{-1}
    \| \\
   &\leq \| \sum_{k=m+1}^{\infty}(\mathbf{P} - \mathbf{Q})^{k} \|
   \|\bm{\Pi}^{-1} \| \\
   &\leq \sum_{k=m+1}^{\infty} \|(\mathbf{P} - \mathbf{Q})^{k} \|
   \|\bm{\Pi}^{-1} \| \\
   &\leq \sum_{k=m+1}^{\infty} p^{k} \| \bm{\Pi}^{-1} \| \\
   &\leq C \frac{p^{m+1}}{1 - p} 
  \end{split}
\end{equation}
where $C = \| \bm{\Pi}^{-1} \|$ and $p < 1$ is the spectral radius of
$\mathbf{P} - \mathbf{Q}$. The last term in Eq.~\eqref{eq:47}
tends to $0$ as $m \rightarrow \infty$, and so $\| \mathbf{T}_m -
\mathbf{Z}\bm{\Pi}^{-1} \| \rightarrow 0$. Now, for any $n$, $\kappa$
is a bounded linear operator from the vector space of $n \times n$
square matrices to the space of $n \times n$ square matrices. Thus, we
have
\begin{equation}
  \label{eq:50}
  \lim_{m \rightarrow \infty}\kappa(\mathbf{T}_m) = \lim_{m \rightarrow \infty}
  \sum_{k=0}^{m}{\kappa((\mathbf{P}^{m} - \mathbf{Q})\bm{\Pi}^{-1})} =
    \kappa(\mathbf{Z}\bm{\Pi}^{-1})
\end{equation}
Thus, if we let $\mathbf{P}_{2} = \mathbf{P}^{2}$ be the transition matrix
of the two-step random walk on $G$, then $\mathbf{P}^{2t} =
\mathbf{P}_{2}^{t}$ and also that $\mathbf{Q}_{2} = \mathbf{Q}$ and thus
\begin{equation}
  \label{eq:51}
  \sum_{t = 0}^{\infty} \Delta_{\rho_{t}^{2}} = \sum_{t = 0}^{\infty}
  \kappa((\mathbf{P}_{2}^{t} - \mathbf{Q})\bm{\Pi}^{-1}) =
  \kappa(\mathbf{Z}_{2} \bm{\Pi}^{-1})
\end{equation}
where $\mathbf{Z}_{2}$ is the fundamental matrix for
$\mathbf{P}_{2}$. Thus, the expected commute time with respect to $\mathbf{P}_2$ is the
sum of the diffusion distances with respect to $\mathbf{P}$ at all
time-scale $t$. We note this fact in the following proposition.
\begin{proposition}
  \label{prop:17}
  Let $G = (V,E,\omega)$ be an undirected graph and $\mathbf{P}$ be
  the transition matrix of the random walk on $G$. $\mathbf{P}^{2}$ is
  then the transition matrix of the two-step random
  walk on $G$. We denote by $\rho_{t}^{2}$ the squared diffusion
  distances between vertices of $G$ with respect to the transition
  matrix $\mathbf{P}$. We also denote by $\delta_P^{2}$ the expected commute time
  between vertices of $G$ with respect to the two-step random walk as
  given by $\mathbf{P}^{2}$. We then have
  \begin{equation}
    \label{eq:34}
    \delta_{P^{2}}(u,v) = \sum_{t = 0}^{\infty}{\rho_{t}^{2}(u,v)}
  \end{equation}
  The sum in Eq.~\eqref{eq:34} is convergent by Eq.~\eqref{eq:51}.
\end{proposition}
The above proposition was stated incorrectly in Qui and Hancock
\cite{qui07:_clust}. Qui and Hancock's reasoning in
\cite{qui07:_clust} leads to the replacement of the term
$\delta_{P^2}(u,v)$ by the term $\delta_{P}(u,v)$ on the left hand
side of Eq.~\eqref{eq:34} .
\section{Forest metrics}
Let $G = (V,E,\omega)$ be an undirected graph with $\omega$ being the
similarity measure between vertices of $G$. Denote by $\mathbf{L}$ the
combinatorial Laplacian of $G$. Let $\alpha \geq 0$ be a fixed
constant and defined the matrix $\mathbf{Q}_{\alpha}$ by
\begin{equation}
  \label{eq:30}
  \mathbf{Q}_{\alpha} = (\mathbf{I} + \alpha \mathbf{L})^{-1}
\end{equation}
Chebotarev and Shamis
\cite{chebotarev02:_fores_metric_for_graph_vertic} defined a notion of
distance $\eta_\alpha(u,v)$ between vertices of $G$ by
\begin{equation}
  \label{eq:41}
  \eta_\alpha(u,v) = \mathbf{Q}_\alpha(u,u) - \mathbf{Q}_\alpha(u,v) -
  \mathbf{Q}_\alpha(v,u) + \mathbf{Q}_\alpha(v,v)
\end{equation}
The $\eta_{\alpha}$ is called a family of {\em forest metrics} on $G$
by Chebotarev and Shamis. Some properties of forest metrics are given
below, see \cite{chebotarev02:_fores_metric_for_graph_vertic}.
\begin{theorem}
  \label{thm:4}
  For any $\alpha \geq 0$, $\mathbf{Q}_{\alpha}$ is positive
  definite. Furthermore, $\mathbf{Q}_{\alpha}$ is a doubly stochastic
  matrix. The matrix $\Delta_{\eta_{\alpha}}$ of forest metrics
  between vertices of $G$ is thus an EDM-2 matrix for all $\alpha \geq
  0$.
\end{theorem}
Chebotarev and Shamis also provided an interpretation for the entries
of $\mathbf{Q}_{\alpha}$ in
\cite{chebotarev02:_fores_metric_for_graph_vertic}. A forest on $G =
(V,E,\omega)$ is an {\em acyclic} subgraph $G' = (V,E',\omega')$ of
$G$ where $E' \subset E$, and $\omega'$ is $\omega$ restricted to
$E'$. A tree is thus a forest with a single connected
component. Define the weight $\varepsilon(F)$ of a forest $F =
(V,E',\omega')$ to be
\begin{equation}
  \varepsilon(F) = \prod_{e \in E'}{\omega'(e)} 
\end{equation}
If $\mathscr{F} = \{F_1, F_2, \dots, F_m\}$ is a collection of forests
on $G$, we define the weight $\varepsilon(\mathscr{F})$ of
$\mathscr{F}$ by $\varepsilon(\mathscr{F}) = \sum_{F_k \in
  \mathscr{F}}{\varepsilon(F_k)}$. 



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "dissertation"
%%% End: 
