\chapter{Distances on undirected graphs}
\section{Expected commute time}
\label{sec:expect-comm-time}
Let $G = (V,E,\omega)$ be an undirected graph, with $\omega$ being the
similarity measure. We assume that the transition matrix
$\mathbf{P}$ on $G$ is irreducible and aperiodic, see
\S \ref{sec:finite-markov-chain}. The {\em expected commute time}
$\delta(u,v)$ between $u \in V$ and $v \in V$ is defined as
\begin{equation}
  \label{eq:25}
  \delta(u,v) = \mathbb{E}_{u}[\tau_v] + \mathbb{E}_{v}[\tau_u]
\end{equation}
Let $\mathbf{M}$ be the matrix of first passage time, i.e.,
$\mathbf{M}(u,v) = \mathbb{E}_{u}[\tau_v]$. The following proposition
shows that $\mathbf{M}$ is the unique solution of a given matrix equation. 
\begin{proposition}
  \label{prop:4}
 $\mathbf{M}$ is the unique solution of the following matrix equation
  \begin{equation}
    \label{eq:3}
   (\mathbf{I} - \mathbf{P})\mathbf{X} = \mathbf{J} - \bm{\Pi}^{-1}
  \end{equation}
  subjected to the condition 
  \begin{equation}
    \label{eq:32}
 \mathbf{M}_{\mathrm{dg}} = \mathbf{0}, \qquad \mathbf{M}(u,v) \geq 0   
  \end{equation}
  where $\mathbf{M}_{\mathrm{dg}}$ is the diagonal matrix formed
    by setting the off-diagonal entries of $\mathbf{M}$ to zero.
    Thus, $\mathbf{M} = \mathbf{X} -
    \mathbf{J}\mathbf{X}_{\mathrm{dg}}$ where $\mathbf{X}$ satisfy
    Eq. \eqref{eq:3}.
\end{proposition}
\begin{proof}
  If $u = v$, then $\mathbb{E}_{u}[\tau_u] = 0$ and thus
  $\mathbf{M}(u,u) = 0$. Otherwise, if $u \not = v$, then
  $\mathbf{M}(u,v) = \mathbb{E}_{u}[\tau_v]$ can be expanded as
  \begin{equation}
    \label{eq:4}
    \mathbb{E}_{u}[\tau_v] = 1 + \sum_{w \in V}{\mathbf{P}(u,w)
      \mathbb{E}_{w}[\tau_v]} = 1 + (\mathbf{PM})(u,v)
  \end{equation}
  Thus, $\mathbf{F} = \mathbf{J} + (\mathbf{P} -
  \mathbf{I})\mathbf{M}$ is a diagonal matrix. Futhermore, if $\pi$ is
  the vector of stationary distribution, then
  \begin{equation}
    \label{eq:26}
    \pi^{T} \mathbf{F} = \pi^{T} \mathbf{J} + \pi^{T} (\mathbf{P} -
    \mathbf{I})\mathbf{M} = \mathbf{1}   
  \end{equation}
  Therefore, $\mathbf{F}(u,u) = 1/\pi(u)$ and thus $\mathbf{F} =
  \bm{\Pi}^{-1}$. We thus have $\bm{\Pi}^{-1} = \mathbf{J} +
  (\mathbf{P} - \mathbf{I})\mathbf{M}$. $\mathbf{M}$ is then a 
  solution of the matrix equation as given by Eq.~\eqref{eq:3}.

  We now show that $\mathbf{M}$ is the unique solution of
  Eq.~\eqref{eq:3} subjected to the condition in
  Eq.~\eqref{eq:32}. Let $\mathbf{M}'$ be another solution of
  Eq.~\eqref{eq:3} subjected to the condition in
  Eq.~\eqref{eq:32}. Then $\mathbf{Y} = \mathbf{M} - \mathbf{M}'$
  satisfy
  \begin{equation}
    \label{eq:19}
    (\mathbf{I} - \mathbf{P})\mathbf{Y} = \mathbf{0}
  \end{equation}
  By Lemma \ref{lem:1}, each column of $\mathbf{Y}$ is constant. Since
  $\mathbf{M}_{\mathrm{dg}} = \mathbf{M'}_{\mathrm{dg}} = \mathbf{0}$, each
  column of $\mathbf{Y}$ must be identically $0$. Thus $\mathbf{M} = \mathbf{M'}$,
  proving the uniqueness of $\mathbf{M}$. If $\mathbf{X}$ satisfy
  Eq. \eqref{eq:3}, then $\mathbf{X} - \mathbf{J}\mathbf{X}_{\mathrm{dg}}$
  satisfy the condition in Eq.~\eqref{eq:32}.
\end{proof}
%
\begin{proposition}
  \label{prop:5}
  Let $\mathbf{Q} = \mathbf{1}^{T}\mathbf{\pi}$ be the matrix with each row being
  the stationary distribution $\pi$. The matrix $\mathbf{M}$ of expected
  first passage time is given by
  \begin{equation}
    \label{eq:21}
    \mathbf{M} = \mathbf{J}(\mathbf{Z} \bm{\Pi}^{-1})_{\mathrm{dg}} - \mathbf{Z}
    \bm{\Pi}^{-1}
  \end{equation}
  where $\mathbf{Z} = (\mathbf{I} - \mathbf{P} + \mathbf{Q})^{-1}$. 
\end{proposition}
\begin{proof}
  From Proposition \ref{prop:7}, we know that $\mathbf{Z} =
  (\mathbf{I} - \mathbf{P} + \mathbf{Q})^{-1}$ is well defined. 
  We first show that $\mathbf{X} = (\mathbf{I} - \mathbf{P} + \mathbf{Q})^{-1}(\mathbf{J}
  - \bm{\Pi}^{-1})$ satisfy Eq. \eqref{eq:3}. We have from Proposition
  \ref{prop:8} that $(\mathbf{I} - \mathbf{P})\mathbf{X} = (\mathbf{I} -
  \mathbf{P})\mathbf{Z}(\mathbf{J} - \bm{\Pi}^{-1}) = (\mathbf{I} - \mathbf{Q})(\mathbf{J} -
  \bm{\Pi}^{-1})$. Since
  $\mathbf{Q}\mathbf{J} = \mathbf{J} = \mathbf{Q}\bm{\Pi}^{-1}$, one has
  \begin{equation}
    \label{eq:27}
    (\mathbf{I} - \mathbf{P})\mathbf{X} =(\mathbf{I} - \mathbf{Q})(\mathbf{J} -
  \bm{\Pi}^{-1}) = \mathbf{J} - \bm{\Pi}^{-1}
  \end{equation}
  and thus $\mathbf{X}$ satisfy Eq. \eqref{eq:3}. Also, from Proposition
  \ref{prop:8}, we have
  \begin{equation}
    \label{eq:31}
    \mathbf{X} = \mathbf{Z}(\mathbf{J} - \bm{\Pi}^{-1}) = \mathbf{J} -
    \mathbf{Z}\bm{\Pi}^{-1}
  \end{equation}
  and thus $\mathbf{X} - \mathbf{J}\mathbf{X}_{\mathrm{dg}} =
  \mathbf{J}(\mathbf{Z}\bm{\Pi}^{-1})_{\mathrm{dg}} - \mathbf{Z}\bm{\Pi}^{-1}$. 
\end{proof}
The expected commute time $\delta(u,v)$ is then just
$\delta(u,v) = \mathbf{M}(u,v)
+ \mathbf{M}^{T}(u,v)$. Thus, if we let $\Delta_{\delta}$ be the
matrix of expected commute time between the vertices, then
\begin{equation}
  \label{eq:52}
  \Delta_\delta = \mathbf{M} + \mathbf{M}^{T} = \mathbf{J}(\mathbf{Z}\bm{\Pi}^{-1})_{\mathrm{dg}} - \mathbf{Z}\bm{\Pi}^{-1} -
  \bm{\Pi}^{-1}\mathbf{Z}^{T} + (\bm{\Pi}^{-1}\mathbf{Z})_{\mathrm{dg}}\mathbf{J}
\end{equation}
We now show that $\Delta_\delta$ is an EDM-2 matrix whenever $G$ is an
undirected graph. 
\begin{proposition}
  \label{prop:10}
  If $G$ is an undirected graph, then $\mathbf{Z}\bm{\Pi}^{-1} =
  \bm{\Pi}^{-1}\mathbf{Z}^{T}$. The matrix $\Delta_{\delta}$ of
  expected commute time is then given by 
  \begin{equation}
    \label{eq:33}
 \Delta_{\delta}  
  = \mathbf{J}(\mathbf{Z}\bm{\Pi}^{-1})_{\mathrm{dg}} - \mathbf{Z}\bm{\Pi}^{-1} -
  \bm{\Pi}^{-1}\mathbf{Z}^{T} + (\bm{\Pi}^{-1}\mathbf{Z})_{\mathrm{dg}} \mathbf{J} =
   \kappa(\mathbf{Z}\bm{\Pi}^{-1})
  \end{equation}
  The matrix $\mathbf{Z}\bm{\Pi}^{-1}$
  is positive definite, and $\Delta_{\delta}$ is an EDM-2 matrix.
\end{proposition}
\begin{proof}
  From Proposition \ref{prop:15}, we have $\mathbf{P}\bm{\Pi}^{-1} =
  \bm{\Pi}^{-1}\mathbf{P}^{T}$. Thus, $\mathbf{P}^{k}\bm{\Pi}^{-1} =
  \bm{\Pi}^{-1}(\mathbf{P}^{T})^{k}$. Furthermore, $\mathbf{Q}\bm{\Pi}^{-1} =
  \mathbf{J}$ and so
  \begin{equation}
    \label{eq:53}
    \mathbf{Z}\bm{\Pi}^{-1} = \biggl[\mathbf{I} +
    \sum_{k=1}^{\infty}(\mathbf{P}^{k} -
    \mathbf{Q})\biggr]\bm{\Pi}^{-1} =
    \bm{\Pi}^{-1}\biggl[\mathbf{I} +
    \sum_{k=1}^{\infty}((\mathbf{P}^{T})^{k} - \mathbf{Q}^{T})\biggr]
    = \bm{\Pi}^{-1}\mathbf{Z}^{T}
  \end{equation}
  Now, $\mathbf{Z}\bm{\Pi}^{-1}$ is positive definite if and only if
  $\bm{\Pi}\mathbf{Z}^{-1} = \bm{\Pi}(\mathbf{I} - \mathbf{P} +
  \mathbf{Q}) \succ 0$. Since $\bm{\Pi}(\mathbf{I} - \mathbf{P} +
  \mathbf{Q}) = \bm{\Pi}(\mathbf{I} - \mathbf{P}) + \pi\pi^{T}$, we
  see that $\bm{\Pi}\mathbf{Z}^{-1} \succ 0$ if $\bm{\Pi}(\mathbf{I} -
  \mathbf{P}) \succeq 0$. We know that $\bm{\Pi}(\mathbf{I} -
  \mathbf{P})$ is symmetric and diagonally dominant (see Definition
  \ref{def:4}). By Ger\u{s}gorin circle theorem, the eigenvalues of
  $\bm{\Pi}(\mathbf{I} - \mathbf{P})$ are non-negative. Thus,
  $\bm{\Pi}(\mathbf{I} - \mathbf{P}) \succeq 0$ and
  the claim that $\mathbf{Z}\bm{\Pi}^{-1}$ is positive definite
  follows. $\Delta_{\delta} = \kappa(\mathbf{Z}\bm{\Pi}^{-1})$ is then
  an EDM-2 matrix.
\end{proof}
%
There exists in the literatures a notion of distances known as
resistance distance
\cite{bapat99:_resis_distan_in_graph,klein93:_resis_distan}. Let $G =
(V,E,\omega)$ be an undirected graph with similarity measure
$\omega$. Let $\mathbf{L}$ be the combinatorial Laplacian of $G$. The
resistance distance $r(u,v)$ between $u, v \in V$ is defined as
\begin{equation}
  \label{eq:35}
  r(u,v) = \tfrac{1}{2}(\mathbf{L}^{\dagger}(u,u) - \mathbf{L}^{\dagger}(u,v) -
  \mathbf{L}^{\dagger}(v,u) + \mathbf{L}^{\dagger}(v,v))
\end{equation}
where $\mathbf{L}^{\dagger}$ is the {\em Moore-Penrose} pseudo-inverse of
$\mathbf{L}$. It's widely known that for undirected graphs, resistance
distance is proportional to expected commute
$\delta(u,v)$. Specifically,
\begin{equation}
  \label{eq:36}
  r(u,v) = \frac{2 \delta(u,v)}{\mathrm{Vol}(G)}
\end{equation}
Eq.~\eqref{eq:36} is an easy corollary of the following result.
\begin{proposition}
  \label{prop:11}
  Let $G = (V,E,\omega)$ be an undirected graph with $|V| = n$. The
  Moore-Penrose pseudo-inverse $\mathbf{L}^{\dagger}$ of $\mathbf{L}$ is given
  by
  \begin{equation}
    \label{eq:37}
    \mathbf{L}^{\dagger} = c \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr) \mathbf{Z}
    \bm{\Pi}^{-1} \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr)
  \end{equation}
  where $c = 1/\mathrm{Vol}(G)$ is a constant. 
\end{proposition}
\begin{proof}
  We will show that $\mathbf{L}^{\dagger}$ as defined by Eq.~\eqref{eq:37}
  satisfies the following conditions for a Moore-Penrose pseudo-inverse
  \begin{gather*}
    \mathbf{L}\mathbf{L}^{\dagger} = \mathbf{L}^{\dagger}\mathbf{L} \tag{(i)} \\
    \mathbf{L}\mathbf{L}^{\dagger}\mathbf{L} = \mathbf{L} \tag{(ii)} \\
    \mathbf{L}^{\dagger}\mathbf{L} \mathbf{L}^{\dagger} = \mathbf{L}^{\dagger}
    \tag{(iii)}
  \end{gather*}
  If $G = (V,E,\omega)$ is an undirected graph, then $\pi(u) =
  \deg(u)/\mathrm{Vol}(G)$ and thus $\mathbf{D} = \mathrm{Vol}(G)
  \bm{\Pi}$. Therefore $\mathbf{L} = \mathbf{D}(\mathbf{I} - \mathbf{P}) =
  \mathrm{Vol}(G) \bm{\Pi}(\mathbf{I} - \mathbf{P})$. We also have
  \begin{equation}
    \label{eq:38}
    \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr)\mathbf{L} = \mathbf{L}
  \end{equation}
  and thus
  \begin{equation}
    \label{eq:39}
    \begin{split}
      \mathbf{L}^{\dagger}\mathbf{L} &= \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr) \mathbf{Z}
       \bm{\Pi}^{-1} \bm{\Pi}(\mathbf{I} - \mathbf{P}) \\
       &= \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr) (\mathbf{I} - \mathbf{P} + \mathbf{Q})^{-1}
       (\mathbf{I} - \mathbf{P}) \\
       &= \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr)(\mathbf{I} - \mathbf{Q}) \\
       &= \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr)
   \end{split}
  \end{equation}
  Similarly,
  \begin{equation}
    \label{eq:40}
    \begin{split}
      \mathbf{L}\mathbf{L}^{\dagger} &= \bm{\Pi}(\mathbf{I} - \mathbf{P}) \mathbf{Z}
      \bm{\Pi}^{-1} \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr) \\
      &= \bm{\Pi}(\mathbf{I} - \mathbf{P}) (\mathbf{I} - \mathbf{P} + \mathbf{Q})^{-1}
      \bm{\Pi}^{-1} \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr) \\
      &= (\mathbf{I} - \mathbf{Q}^{T})\Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr) \\
      &= \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr)
   \end{split}
  \end{equation}
  Thus, condition (i) is satisfied. Furthermore, we also have, from
  Eq.~\eqref{eq:39} and Eq.~\eqref{eq:40}, that
  \begin{gather*}
   \mathbf{L}\mathbf{L}^{\dagger}\mathbf{L} = \Bigl(\mathbf{I} -
   \frac{\mathbf{J}}{n}\Bigr) \mathbf{L}
   = \mathbf{L} \\
    \mathbf{L}^{\dagger}\mathbf{L}\mathbf{L}^{\dagger} = c \Bigl(\mathbf{I} -
    \frac{\mathbf{J}}{n}\Bigr) \mathbf{Z}
     \bm{\Pi}^{-1} \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr) \Bigl(\mathbf{I} -
     \frac{\mathbf{J}}{n}\Bigr) = c \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr) \mathbf{Z}
     \bm{\Pi}^{-1} \Bigl(\mathbf{I} - \frac{\mathbf{J}}{n}\Bigr) = \mathbf{L}^{\dagger}
  \end{gather*}
  Thus, $\mathbf{L}^{\dagger}$ as defined by Eq.~\eqref{eq:37} is the
  Moore-Penrose inverse of $\mathbf{L}$. 
\end{proof}
Proposition \ref{prop:11} is equivalent to saying that
$\mathbf{L}^{\dagger}$ is a constant times the $\tau$ transform of
$\mathbf{Z}\bm{\Pi}^{-1}$. Since $\kappa(\mathbf{X}) =
\kappa(\tau(\mathbf{X}))$, we have 
\begin{equation}
  \label{eq:42}
 \Delta_{\delta} = \kappa(\mathbf{Z}\bm{\Pi}^{-1}) = \mathrm{Vol}(G)
\kappa(\mathbf{L}^{\dagger}) 
\end{equation}
Eq.~\eqref{eq:36} thus follows from Proposition \ref{prop:11}, as
claimed. \\
\\
\noindent
From Eq.~\eqref{eq:36} we can make an observation about expected
commute time and resistance distances. Expected commute time is scale
invariant with respect to the similarity measure $\omega$, i.e., if we
replace $G = (V,E,\omega)$ with $G' = (V,E,\omega')$ where $\omega' =
\alpha \omega$ and $\alpha > 0$ is a constant, then $\delta_{G}(u,v) =
\delta_{G'}(u,v)$. Resistance distance is however not scale
invariant. In fact, we have $r_{G'}(u,v) = r_{G}(u,v)/\alpha$. This
observations means that even though these two notion of distances
are closely related, they are not equivalent.
%
\section{Diffusion distances}
\label{sec:diffusion-distances}
Let $G = (V,E,\omega)$ be an undirected graph with
$\omega$ being a similarity measure between vertices of $V$. Denote by
$\mathbf{P}$ the probability transition matrix of $G$. The diffusion
distances at time $t$, $\rho_{t}(u,v)$, between $u,v \in V$ is defined as
\cite{coifman06:_diffus_maps}
\begin{equation}
  \label{eq:43}
  \rho^{2}_{t}(u,v) = \sum_{w \in V}{\Bigl(\mathbf{P}^{t}(u,w) -
      \mathbf{P}^{t}(v,w)\Bigr)^2 \frac{1}{\pi(w)}}
\end{equation}
\begin{proposition}
  \label{prop:12}
  Diffusion distances as defined by Eq.~\eqref{eq:43} can also be
  written as
  \begin{equation}
    \label{eq:44}
    \begin{split}
      \rho_{t}^{2}(u,v) &= \frac{(\mathbf{P}^{2t}(u,u) -
        (\mathbf{P}^{2t})(v,u)}{\pi(u)} +
      \frac{(\mathbf{P}^{2t})(v,v) -
        (\mathbf{P}^{2t}(u,v)}{\pi(v)}  \\
      &= (\mathbf{P}^{2t}\bm{\Pi}^{-1})(u,u) -
      (\mathbf{P}^{2t}\bm{\Pi}^{-1})(v,u) \\
      &+ (\mathbf{P}^{2t}\bm{\Pi}^{-1})(v,v) -
      (\mathbf{P}^{2t}\bm{\Pi}^{-1})(u,v)
    \end{split}
  \end{equation}
The matrix of squared diffusion distances $\Delta_{\rho_t^2}$ is then
$\Delta_{\rho_t^2} = \kappa(\mathbf{P}^{2t}\bm{\Pi}^{-1})$. 
\end{proposition}
\begin{proof}
  Since $G$ is undirected, $\mathbf{P}$ is time-reversible and hence
  \begin{equation}
    \label{eq:45}
    \pi(u) \mathbf{P}(u,v) = \pi(v) \mathbf{P}(v,u) 
  \end{equation}
  By
  expanding the square of $(\mathbf{P}^{t}(u,w) - \mathbf{P}^{t}(v,w))^{2}$ in
  Eq.~\eqref{eq:43} and using Eq.~\eqref{eq:45}, one has
  \begin{equation}
    \label{eq:46}
    \begin{split}
      \rho_{t}^{2}(u,v) &= \sum_{w \in V}{\Bigl(\mathbf{P}^{t}(u,w) -
        \mathbf{P}^{t}(v,w)\Bigr)^2 \frac{1}{\pi(w)}} \\
      &= \sum_{w \in V}{\frac{\mathbf{P}^{t}(u,w)\mathbf{P}^{t}(u,w) -
          \mathbf{P}^{t}(u,w)\mathbf{P}^{t}(v,w)}{\pi(w)}} \\
      &+\sum_{w \in V}{\frac{\mathbf{P}^{t}(v,w)\mathbf{P}^{t}(v,w) -
          \mathbf{P}^{t}(v,w)\mathbf{P}^{t}(u,w)}{\pi(w)}} \\
      &= \sum_{w \in
        V}{\frac{\mathbf{P}^{t}(u,w)\mathbf{P}^{t}(w,u) -
          \mathbf{P}^{t}(v,w)\mathbf{P}^{t}(w,u)}{\pi(u)}} \\ &+
      \sum_{w \in V}{\frac{\mathbf{P}^{t}(v,w)\mathbf{P}^{t}(w,v)
          -
          \mathbf{P}^{t}(u,w)\mathbf{P}^{t}(w,v)}{\pi(v)}} \\
      &= \frac{(\mathbf{P}^{2t})(u,u) -
        (\mathbf{P}^{2t})(v,u)}{\pi(u)} +
      \frac{(\mathbf{P}^{2t})(v,v) -
        (\mathbf{P}^{2t})(u,v)}{\pi(v)} 
    \end{split} 
  \end{equation}
  which is exactly Eq.~\eqref{eq:44}. 
\end{proof} 
Since $\mathbf{P}^{2t}\bm{\Pi}^{-1} =
\mathbf{P}^{t}\mathbf{P}^{t}\bm{\Pi}^{-1} =
\mathbf{P}^{t}\bm{\Pi}^{-1}\mathbf{P}^{T} \succeq 0$,
$\Delta_{\rho_{t}^2} = \kappa(\mathbf{P}^{2t}\bm{\Pi}^{-1})$ is an
EDM-2 matrix. We state the above observation as the following
proposition
\begin{proposition} 
\label{prop:14} 
Let $G$ be an undirected graph and $\mathbf{P}$ be its transition
matrix. The matrix $\Delta_{\rho_{t}^2}$ of squared diffusion distances is 
an EDM-2 matrix for any $t$. $\Delta_{\rho_t}$ is then an EDM-1 matrix.
\end{proposition}
%
We would now like to make an observation regarding diffusion distances
on undirected graphs. From Eq.~\eqref{eq:46}, we observed that
$\rho_{t}^{2}(u,v)$ only depends on the probability between nodes
connected by paths of length $2t$. Thus, diffusion distances between
any two nodes $u$ and $v$ of $G$ for any time scale $t$ only keeps
tracks of paths of even length in $G$. Diffusion distances might
be unintuitive in some scenarios. For a contrived example,
consider the case where $G$ is a cycle. Then, there might be pairs of nodes that are
adjacent to each other and that have diffusion distances larger than
the nodes that are on two different segments of the cycle. We will
take a closer look at this phenomenon in a later chapter of the
thesis. \\ \\
%
\noindent
We now note the connection between expected commute time and diffusion
distances for when $G$ is an undirected graph. From Proposition
\ref{prop:16}, we have
\begin{equation}
  \label{eq:49}
\kappa(\mathbf{P}^{2t}\bm{\Pi^{-1}}) =
\kappa(\mathbf{P}^{2t}\bm{\Pi^{-1}} - \mathbf{J}) =
\kappa((\mathbf{P}^{2t} - \mathbf{Q}) \bm{\Pi^{-1}})
\end{equation}
Let $\mathbf{T}_{m} = \Bigl(\mathbf{I} + \sum_{k =
  1}^{m}{(\mathbf{P}^{k} - \mathbf{Q})}\Bigr)\bm{\Pi}^{-1}$ for $m
\geq 0$. Then $\| \mathbf{T}_m - \mathbf{Z}\bm{\Pi}^{-1} \| \rightarrow 0$ as
$m \rightarrow \infty$. This can be seen as follow. 
\begin{equation}
  \label{eq:47}
  \begin{split}
  \| \mathbf{T}_m - \mathbf{Z}\bm{\Pi}^{-1} \| &=
  \|\sum_{k=m+1}^{\infty}(\mathbf{P} - \mathbf{Q})^{k}\bm{\Pi}^{-1}
    \| \\
   &\leq \| \sum_{k=m+1}^{\infty}(\mathbf{P} - \mathbf{Q})^{k} \|
   \|\bm{\Pi}^{-1} \| \\
   &\leq \sum_{k=m+1}^{\infty} \|(\mathbf{P} - \mathbf{Q})^{k} \|
   \|\bm{\Pi}^{-1} \| \\
   &\leq \sum_{k=m+1}^{\infty} p^{k} \| \bm{\Pi}^{-1} \| \\
   &\leq C \frac{p^{m+1}}{1 - p} 
  \end{split}
\end{equation}
where $C = \| \bm{\Pi}^{-1} \|$ and $p < 1$ is the spectral radius of
$\mathbf{P} - \mathbf{Q}$. The last term in Eq.~\eqref{eq:47}
tends to $0$ as $m \rightarrow \infty$, and so $\| \mathbf{T}_m -
\mathbf{Z}\bm{\Pi}^{-1} \| \rightarrow 0$. Now, for any $n$, $\kappa$
is a bounded linear operator from the vector space of $n \times n$
square matrices to the space of $n \times n$ square matrices. Thus, we
have
\begin{equation}
  \label{eq:50}
  \lim_{m \rightarrow \infty}\kappa(\mathbf{T}_m) = \lim_{m \rightarrow \infty}
  \sum_{k=0}^{m}{\kappa((\mathbf{P}^{m} - \mathbf{Q})\bm{\Pi}^{-1})} =
    \kappa(\mathbf{Z}\bm{\Pi}^{-1})
\end{equation}
Thus, if we let $\mathbf{P}_{2} = \mathbf{P}^{2}$ be the transition matrix
of the two-step random walk on $G$, then $\mathbf{P}^{2t} =
\mathbf{P}_{2}^{t}$ and also that $\mathbf{Q}_{2} = \mathbf{Q}$ and thus
\begin{equation}
  \label{eq:51}
  \sum_{t = 0}^{\infty} \Delta_{\rho_{t}^{2}} = \sum_{t = 0}^{\infty}
  \kappa((\mathbf{P}_{2}^{t} - \mathbf{Q})\bm{\Pi}^{-1}) =
  \kappa(\mathbf{Z}_{2} \bm{\Pi}^{-1})
\end{equation}
where $\mathbf{Z}_{2}$ is the fundamental matrix for
$\mathbf{P}_{2}$. Thus, the expected commute time with respect to $\mathbf{P}_2$ is the
sum of the diffusion distances with respect to $\mathbf{P}$ at all
time-scale $t$. We note this fact in the following proposition.
\begin{proposition}
  \label{prop:17}
  Let $G = (V,E,\omega)$ be an undirected graph and $\mathbf{P}$ be
  the transition matrix of the random walk on $G$. $\mathbf{P}^{2}$ is
  then the transition matrix of the two-step random
  walk on $G$. We denote by $\rho_{t}^{2}$ the squared diffusion
  distances between vertices of $G$ with respect to the transition
  matrix $\mathbf{P}$. We also denote by $\delta_P^{2}$ the expected commute time
  between vertices of $G$ with respect to the two-step random walk as
  given by $\mathbf{P}^{2}$. We then have
  \begin{equation}
    \label{eq:34}
    \delta_{P^{2}}(u,v) = \sum_{t = 0}^{\infty}{\rho_{t}^{2}(u,v)}
  \end{equation}
  The sum in Eq.~\eqref{eq:34} is convergent by Eq.~\eqref{eq:51}.
\end{proposition}
The above proposition was stated incorrectly in Qui and Hancock
\cite{qui07:_clust}. Qui and Hancock's reasoning in
\cite{qui07:_clust} leads to the replacement of the term
$\delta_{P^2}(u,v)$ by the term $\delta_{P}(u,v)$ on the left hand
side of Eq.~\eqref{eq:34} .
\section{Forest metrics}
Let $G = (V,E,\omega)$ be an undirected graph with $\omega$ being the
similarity measure between vertices of $G$. Denote by $\mathbf{L}$ the
combinatorial Laplacian of $G$. Let $\alpha \geq 0$ be a fixed
constant and defined the matrix $\mathbf{Q}_{\alpha}$ by
\begin{equation}
  \label{eq:30}
  \mathbf{Q}_{\alpha} = (\mathbf{I} + \alpha \mathbf{L})^{-1}
\end{equation}
Chebotarev and Shamis
\cite{chebotarev02:_fores_metric_for_graph_vertic} defined a notion of
distance $\eta_\alpha(u,v)$ between vertices of $G$ by
\begin{equation}
  \label{eq:41}
  \eta_\alpha(u,v) = \mathbf{Q}_\alpha(u,u) - \mathbf{Q}_\alpha(u,v) -
  \mathbf{Q}_\alpha(v,u) + \mathbf{Q}_\alpha(v,v)
\end{equation}
The $\eta_{\alpha}$ is called a family of {\em forest metrics} on $G$
by Chebotarev and Shamis. Some properties of forest metrics are given
below, see \cite{chebotarev02:_fores_metric_for_graph_vertic}.
\begin{theorem}
  \label{thm:4}
  For any $\alpha \geq 0$, $\mathbf{Q}_{\alpha}$ is positive
  definite. Furthermore, $\mathbf{Q}_{\alpha}$ is a doubly stochastic
  matrix. The matrix $\Delta_{\eta_{\alpha}}$ of forest metrics
  between vertices of $G$ is thus an EDM-2 matrix for all $\alpha \geq
  0$.
\end{theorem}
Chebotarev and Shamis also provided an interpretation for the entries
of $\mathbf{Q}_{\alpha}$ in
\cite{chebotarev02:_fores_metric_for_graph_vertic}. A forest on $G =
(V,E,\omega)$ is an {\em acyclic} subgraph $G' = (V,E',\omega')$ of
$G$ where $E' \subset E$, and $\omega'$ is $\omega$ restricted to
$E'$. A tree is thus a forest with a single connected
component. Define the weight $\varepsilon(F)$ of a forest $F =
(V,E',\omega')$ to be
\begin{equation}
  \varepsilon(F) = \prod_{e \in E'}{\omega'(e)} 
\end{equation}
If $\mathscr{F} = \{F_1, F_2, \dots, F_m\}$ is a collection of forests
on $G$, we define the weight $\varepsilon(\mathscr{F})$ of
$\mathscr{F}$ by $\varepsilon(\mathscr{F}) = \sum_{F_k \in
  \mathscr{F}}{\varepsilon(F_k)}$. Now, let $G = (V,E,\omega)$ be a
graph. Define $G_\alpha$ to be the graph formed by multiplying the
similarity measure $\omega$ on $G$ by $\alpha$, i.e., $G_\alpha =
(V,E,\alpha \omega)$. Denote by $\mathscr{F}_{G_\alpha}$ the
collection of all forests on $G_\alpha$. Furthermore, denote by
$\mathscr{F}_{G_\alpha}^{uv} \subset \mathscr{F}_{G_\alpha}$ the
collection of forests on $G_\alpha$ where $u$ and $v$ belong to the
same tree rooted at $u$. We then have
\begin{equation}
  \label{eq:48}
  \mathbf{Q}_{\alpha}(u,v) = \frac{\varepsilon(\mathscr{F}_{G_{\alpha}}^{uv})}{\varepsilon(\mathscr{F}_{G_{\alpha}})}
\end{equation}
A proof of Eq.~\eqref{eq:48} using the all minors matrix tree theorem
\cite{chaiken82,moon94:_some_deter_expan_and_matrix_tree_theor} is in
\cite{chebotarev02:_fores_laplac}. \\
\\
%
\noindent
Chebotarev and Shamis also note a relationship between forest metrics
and resistance distances in
\cite{chebotarev02:_fores_metric_for_graph_vertic}. Let
$G_{\alpha}^{*} = (V(G_{\alpha}^{*}),E(G_{\alpha}^{*}),\omega')$ be the
graph formed by augmenting to $G_\alpha$ a vertex $v_*$ as follows.
\begin{itemize}
\item $V(G_{\alpha}^{*}) = V(G_{\alpha}) \cup \{v_*\}$, $v_* \not \in
  V(G_{\alpha})$.
\item $E(G_{\alpha}^{*}) = E(G_{\alpha}) \cup \{ \{v, v_*\} \colon
  v \in V(G_{\alpha}) \}$
\item $\omega'(e) = \omega(e)$ if $e \in E(G_{\alpha})$ and $\omega'(e) = 1$ if $e \in \{ \{v,v_*\}
  \colon v \in V(G_{\alpha}) \}$. 
\end{itemize}
The relationship between forest metrics and resistance distances is
then given by the following proposition.
\begin{proposition}
  \label{prop:9}
  Let $G = (V,E,\omega)$ and $G_{\alpha}^{*}$ be as defined
  above. Then
  \begin{equation}
    \label{eq:54}
    \delta_{G_{\alpha}^{*}}(u,v) = \eta_{\alpha}(u,v)
  \end{equation}
  where $u$ and $v$ are vertices of $G$. 
\end{proposition}
The following proof of Proposition \ref{prop:9} is from
\cite{chebotarev02:_fores_metric_for_graph_vertic}.
\begin{proof}
  Let $n = |V(G)|$ be the number of vertices in $G$. Let
  $\mathbf{Q}_{\alpha}^{0}$ be the matrix obtained from
  $\mathbf{Q}_{\alpha}$ by the addition of the zero row and zero
  column corresponding to the vertex $v_*$.
  \begin{equation}
    \label{eq:58}
    \mathbf{Q}_{\alpha}^{0}  =  \left[ \begin{array}{c|c}
        0 & \bm{0} \\ \hline
        \bm{0}^{T} & (\mathbf{I} + \alpha \mathbf{L})^{-1}
        \end{array} \right]
  \end{equation}
  Let $\mathbf{L}_{G_{\alpha}^{*}}$ be the Laplacian matrix of $G_{\alpha}^{*}$, i.e., 
  \begin{equation}
    \label{eq:57}
    \mathbf{L}_{G_{\alpha}^{*}} = \left[ \begin{array}{c|c}
        n & \bm{-1} \\ \hline
        \bm{-1}^{T} & (\mathbf{I} + \alpha \mathbf{L})
        \end{array} \right]
  \end{equation}
  Then $\mathbf{L}_{G_{\alpha}^{*}}\mathbf{Q}_{\alpha}^{0}\mathbf{L}_{G_{\alpha}^{*}} =
  \mathbf{L}_{G_{\alpha}^{*}}$. Thus $\mathbf{Q}_{\alpha}^{0}$ is a generalized
  inverse of $\mathbf{L}_{G_{\alpha}^{*}}$. $\mathbf{Q}_{\alpha}^{0}$ can then be
  written as 
  \begin{equation}
    \label{eq:59}
    \mathbf{Q}_{\alpha}^{0} = \mathbf{L}_{G_{\alpha}^{*}}^{\dagger} +
    \bm{a}\bm{1}^{T} + \bm{1}^{T}\bm{b}
  \end{equation}
  for some vectors $\bm{a}$,$\bm{b}$ and $\mathbf{L}_{G_{\alpha}^{*}}^{\dagger}$, the Moore-Penrose
  pseudo inverse of $\mathbf{L}_{G_{\alpha}^{*}}$. Since $\kappa(\bm{a}\bm{1}^{T}) =
  \mathbf{0} = \kappa(\bm{1}^{T}\bm{b})$, we have
  \begin{equation}
    \label{eq:60}
    \kappa(\mathbf{Q}_{\alpha}^{0}) = \kappa(\mathbf{L}_{G_{\alpha}^{*}}^{\dagger})
  \end{equation}
  Now, $\kappa(\mathbf{Q}_{\alpha}^{0})$ coincides with
  $\kappa(\mathbf{Q}_{\alpha})$ for the set of vertices corresponding
  to $V(G)$, and Eq.~\eqref{eq:54} is thus established.
\end{proof}

\section{$f(\mathbf{P} - \mathbf{Q})$ and graph metrics}
\label{sec:graph-metr-funct}
We recalled from \S \ref{sec:expect-comm-time} that the matrix
$\Delta_{\delta}$ of expected commute time is given by
$\Delta_{\delta} = \kappa((\mathbf{I} - \mathbf{P} -
\mathbf{Q})^{-1}\bm{\Pi}^{-1})$. We also recalled from \S
\ref{sec:diffusion-distances} that the matrix $\Delta_{\rho_{t}^{2}}$
of squared diffusion distance is given by $\Delta_{\rho_{t}^{2}} =
\kappa(\mathbf{P}^{2t}\bm{\Pi}^{-1})$. Proposition \ref{prop:17}
stated that expected commute time with respect to the two-step random
walk is the sum of squared diffusion distances at time scale
$t=0,1,\dots$. \\
\\
\noindent
We know from \S \ref{sec:expect-comm-time} that
\begin{equation}
\label{eq:63}
  (\mathbf{I} - \mathbf{P} + \mathbf{Q})^{-1}\bm{\Pi}^{-1} =
\biggl[\mathbf{I} +
  \sum_{k=1}^{\infty}{(\mathbf{P}^{k} -
    \mathbf{Q})}\biggr]\bm{\Pi}^{-1} = \bm{\Pi}^{-1} +
  \sum_{k=1}^{\infty}{(\mathbf{P}^{k}\bm{\Pi}^{-1} - \mathbf{J})}
\end{equation}
From Proposition \ref{prop:16} we also know that $\kappa(\mathbf{X} -
\mathbf{J}) = \kappa(\mathbf{X})$. Thus
\begin{equation}
  \label{eq:62}
  \kappa((\mathbf{I} - \mathbf{P} + \mathbf{Q})^{-1}\bm{\Pi}^{-1}) = \kappa(\bm{\Pi}^{-1} +
  \sum_{k=1}^{\infty}{(\mathbf{P}^{k}\bm{\Pi}^{-1} - \mathbf{J})}) =
  \kappa(\bm{\Pi}^{-1} + \sum_{k=1}^{\infty}{(\mathbf{P}^{k}\bm{\Pi}^{-1})})
\end{equation}
should hold, except that the sum in the rightmost term in
Eq.~\eqref{eq:62} is not necessarilty convergent since
$\rho(\mathbf{P}) = 1$. Before we worry about this problem, let's
consider the sum in the rightmost term in Eq.~\eqref{eq:62} as is. This sum
say that the expected commute time $\delta(u,v)$ is the
$\kappa$ transform of terms are formed by taking into account the
probability of all the paths between the $u$ and $v$. This
interpretation is easy to understand and confirm that
expected commute time is a sensible notion of distances on graphs. The
interpretation of the sum in Eq. \eqref{eq:63} is harder.  However,
the sum in Eq.~\eqref{eq:63} is always convergent since
$\rho(\mathbf{P} - \mathbf{Q}) < 1$. We have thus arrived at a
situation where a matrix power series in $\mathbf{P} - \mathbf{Q}$ is
convergent and has a simple intepretation. Our aim in this section is
to extend the above observations into a more general result that will
allow us to have a general notion of distances on
graphs that is both well defined and also easily interpretable. \\ \\
\noindent
We first show that any matrix power series of the form
\begin{equation}
  \label{eq:64}
  \sum_{k=0}^{\infty}{c_k (\mathbf{P} - \mathbf{Q})^{k}} = 
c_0\mathbf{I} + c_1(\mathbf{P} - \mathbf{Q}) + c_2(\mathbf{P} -
  \mathbf{Q})^{2} + \cdots
\end{equation}
is convergent, as long as $\limsup_{n \rightarrow \infty} |c_k|^{1/k}
\leq 1$, This is a consequence of the following result \citet[\S
6.2]{horn94:_topic_in_matrix_analy}
\begin{theorem}
  \label{thm:3}
  Let $f(t)$ be a scalar-valued analytic function with a power series
  representation $f(t) = c_0 + c_1t + c_2 t^2 + \cdots$ that has radius
  of convergence $R > 0$. If $\mathbf{A} \in M_n$ is a $n \times n$
  square matrix and $\rho(A) < R$, then the matrix power series
  $f(\mathbf{A}) = c_0 \mathbf{I} + c_1 \mathbf{A} + c_2 \mathbf{A}^2
  + \cdots$ converges with respect to every norm on $M_n$. Furthermore,
  the sum is equal to the primary matrix function $f(\mathbf{A})$
  associated with the stem $f(t)$.
\end{theorem}
The radius of convergence of a power series is given by the {\em
  Cauchy-Hadamard} formula \citet[\S V.3]{gamelin01:_compl_analy}
\begin{equation}
  \label{eq:65}
  R = \frac{1}{\limsup_{k \rightarrow \infty}{|c_k|^{1/k}}}
\end{equation}
The sum in Eq.~\eqref{eq:64} thus converges if $R \geq 1$, i.e.,
if $\limsup_{n \rightarrow \infty} |c_k|^{1/k} \leq 1$. 
\begin{proposition}
  \label{prop:13}
  Let $G = (V,E,\omega)$ be an undirected graph with similarity
  measure $\omega$. Let $\mathbf{P}$ be the transition matrix of
  $G$. Suppose that $\mathbf{P}$ is irreducible and aperiodic.  Let
  $f$ be a scalar-valued analytic function with radius of convergence
  $R \geq 1$. Assume also that $f$ is non-negative on the interval
  $(-1,1)$. Then $f((\mathbf{P} - \mathbf{Q}))\bm{\Pi}^{-1}$ is a well
  defined, positive semidefinite matrix. $\kappa(f(\mathbf{P} -
  \mathbf{Q})\bm{\Pi}^{-1})$ is then an EDM-2 matrix.
\end{proposition}
\begin{proof}
  Since $G$ is a directed graph, $\mathbf{P}$ is time-reversible. By
  Proposition \ref{prop:15}, $\bm{\Pi}\mathbf{P} =
  \mathbf{P}^{T}\bm{\Pi}$. Thus, the matrix
  $\bm{\Pi}^{1/2}\mathbf{P}\bm{\Pi}^{-1/2}$ is
  symmetric. Similarly, the matrix $\mathbf{N} =
  \bm{\Pi}^{1/2}(\mathbf{P} - \mathbf{Q})\bm{\Pi}^{-1/2}$ is
  symmetric. Now, $f(\mathbf{N})$ is well defined and furthermore
  \begin{equation}
    \label{eq:66}
    f(\mathbf{N}) = \bm{\Pi}^{1/2}f(\mathbf{P} - \mathbf{Q})\bm{\Pi}^{-1/2}
  \end{equation}
  Since $\mathbf{N}$ is symmetric, the spectrum 
  $\sigma(f(\mathbf{N}))$ of $f(\mathbf{N})$ is
  \begin{equation}
    \label{eq:67}
    \sigma(f(\mathbf{N})) = \{ f(\lambda) \colon \lambda \in
    \sigma(\mathbf{N}) \}
  \end{equation}
  Now, $f(\mathbf{P} - \mathbf{Q})$ is similar to $f(\mathbf{N})$ and
  thus $\sigma(\mathbf{P} - \mathbf{Q}) =
  \sigma(f(\mathbf{N}))$. However, $(\mathbf{P} - \mathbf{Q})$ is also
  similar to $\mathbf{N}$ and so $\sigma(\mathbf{N}) =
  \sigma(\mathbf{P} - \mathbf{Q}) \subset
  (-1,1)$. Therefore 
  \begin{equation}
    \label{eq:68}
    \sigma(f(\mathbf{P} - \mathbf{Q})) = \{ f(\lambda) \colon \lambda \in
    \sigma(\mathbf{P} - \mathbf{Q})\} \subset \{ f(\lambda) \colon
    \lambda \in (-1,1) \}
  \end{equation}
  Since $f$ is non-negative on $(-1,1)$, $\sigma(f(\mathbf{P} -
  \mathbf{Q})) \subset \mathbb{R}^{\geq 0}$. Thus $f(\mathbf{N})$ is
  positive semidefinite. Now, $f(\mathbf{P} -
  \mathbf{Q})\bm{\Pi}^{-1}$ can be written as
  \begin{equation}
    \label{eq:69}
f(\mathbf{P} - \mathbf{Q})\bm{\Pi}^{-1} = \bm{\Pi}^{-1/2}
f(\mathbf{N}) \bm{\Pi}^{-1/2}
  \end{equation}
and so $f(\mathbf{P} - \mathbf{Q})\bm{\Pi}^{-1} \succeq
0$, as desired. 
\end{proof}
As consequences of Proposition \ref{prop:13}, the following notion of
distances on graphs are all well-defined. Furthermore, the resulting
distance matrix are all EDM-2 matrix.
\begin{enumerate}
\item $\Delta_1 = \kappa(\mathbf{X}_1\bm{\Pi}^{-1})$ where $\mathbf{X}_1 = 
\sum_{k=1}^{\infty}{\tfrac{1}{k}(\mathbf{P}^{2k} -
  \mathbf{Q})}$. $\mathbf{X}_1$ is then the primary matrix function $\log{(\mathbf{I} - \mathbf{P^2} +
  \mathbf{Q})^{-1}}$. The terms of $\mathbf{X}_1$ are formed by taking
  into account all paths between nodes, where each of the paths is
  scaled by its length, with longer paths being discouraged.
\item $\Delta_{2} = \kappa(\mathbf{X}_2\bm{\Pi}^{-1})$ where $\mathbf{X}_2 =
  \sum_{k=1}^{\infty}{k(\mathbf{P}^{2k} - \mathbf{Q})}$. $\mathbf{X}_2$
  is then the primary matrix function $(\mathbf{P}^2 -
  \mathbf{Q})(\mathbf{I} - \mathbf{P}^2 + \mathbf{Q})^{-2}$. The terms
  of $\mathbf{X}_2$ are formed by taking
  into account all even paths between nodes, where each of the paths is
  again scaled by its length, with longer paths being encouraged.
\item $\Delta_{3} = \kappa(\exp(\mathbf{P})\bm{\Pi}^{-1})$. 
\end{enumerate}
Note that for items $1$ and $2$ above, $f(\mathbf{P})$ is not
necessarily defined.
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "dissertation"
%%% End: 
